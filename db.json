{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/archer/source/assets/algolia_logo.svg","path":"assets/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/archer/source/assets/example_qr.png","path":"assets/example_qr.png","modified":0,"renderable":1},{"_id":"themes/archer/source/assets/favicon.ico","path":"assets/favicon.ico","modified":0,"renderable":1},{"_id":"themes/archer/source/assets/loading.svg","path":"assets/loading.svg","modified":0,"renderable":1},{"_id":"themes/archer/source/css/mobile.css","path":"css/mobile.css","modified":0,"renderable":1},{"_id":"themes/archer/source/avatar/Misaka.jpg","path":"avatar/Misaka.jpg","modified":0,"renderable":1},{"_id":"themes/archer/source/css/style.css","path":"css/style.css","modified":0,"renderable":1},{"_id":"themes/archer/source/font/Source Sans Pro.woff","path":"font/Source Sans Pro.woff","modified":0,"renderable":1},{"_id":"themes/archer/source/font/Source Sans Pro.woff2","path":"font/Source Sans Pro.woff2","modified":0,"renderable":1},{"_id":"themes/archer/source/lib/webfontloader.min.js","path":"lib/webfontloader.min.js","modified":0,"renderable":1},{"_id":"themes/archer/source/scripts/main.js","path":"scripts/main.js","modified":0,"renderable":1},{"_id":"themes/archer/source/scripts/search.js","path":"scripts/search.js","modified":0,"renderable":1},{"_id":"themes/archer/source/scripts/share.js","path":"scripts/share.js","modified":0,"renderable":1},{"_id":"themes/archer/source/intro/404-bg.jpg","path":"intro/404-bg.jpg","modified":0,"renderable":1},{"_id":"themes/archer/source/font/Oswald-Regular.ttf","path":"font/Oswald-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/archer/source/font/SourceCodePro-Regular.ttf.woff","path":"font/SourceCodePro-Regular.ttf.woff","modified":0,"renderable":1},{"_id":"themes/archer/source/font/SourceCodePro-Regular.ttf.woff2","path":"font/SourceCodePro-Regular.ttf.woff2","modified":0,"renderable":1},{"_id":"themes/archer/source/lib/jquery.min.js","path":"lib/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/archer/source/intro/about-bg.jpg","path":"intro/about-bg.jpg","modified":0,"renderable":1},{"_id":"themes/archer/source/intro/post-bg.jpg","path":"intro/post-bg.jpg","modified":0,"renderable":1},{"_id":"themes/archer/source/intro/index-bg.jpg","path":"intro/index-bg.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/archer/.eslintrc.json","hash":"5c79a54f29b1d32045e612fcb7379797d79ac380","modified":1539669271894},{"_id":"themes/archer/.gitattributes","hash":"82c1a621642d5b620275ae1ed59845c3f7015a64","modified":1539669271895},{"_id":"themes/archer/.prettierignore","hash":"075b20da2bb3dfa7932394363d63d98cbb0b7042","modified":1539669271902},{"_id":"themes/archer/.gitignore","hash":"d734edbdd316a19eae0ac2421256ac60dd61a1f8","modified":1539669271901},{"_id":"themes/archer/.prettierrc.js","hash":"c77c76428fb4eefd727444d97fa2733a0c10286a","modified":1539669271903},{"_id":"themes/archer/LICENSE","hash":"35f4fb806270f8243459c870a2141e795dfab166","modified":1539669271904},{"_id":"themes/archer/README.md","hash":"a166b2c840fdf215a38d205f33e4dc1a5f2f0769","modified":1539669271905},{"_id":"themes/archer/_config.yml","hash":"4bee8a4402d7434785a2f6e71cb1f37aa54a7a10","modified":1539671335943},{"_id":"themes/archer/gulpfile.js","hash":"ba7c6a88298900460ddd1649300b5672956902f9","modified":1539669271916},{"_id":"themes/archer/package.json","hash":"c07c94772d0e92e77788b0fe5beb3d44eb3c245f","modified":1539669271958},{"_id":"themes/archer/webpack.config.js","hash":"348da2932316c2789a3c0a3cf5efff0dd7cc4ffe","modified":1539669272039},{"_id":"themes/archer/webpack.prod.js","hash":"b6d35f7e9a9718b3e5e3e17c52fcc5e7c38330b0","modified":1539669272040},{"_id":"source/_posts/Dockerfile-syntax.md","hash":"6ab92cd5a7d540f7ba911d4563d8195dc0b6146f","modified":1527041409074},{"_id":"source/_posts/Kubernetes-deploy-Jenkins-slave.md","hash":"1231212bdd4e0b1720ccd9c097e9cd62ab7c3042","modified":1527041478191},{"_id":"source/_posts/docker-install-note.md","hash":"06f8b99914df30423a7ae0bf4c487c4ccd3a8794","modified":1527041503749},{"_id":"source/_posts/elasticsearch-fluent-kibana.md","hash":"bc6927ef8322ebad6fd0f0a668befa60c5f3d0df","modified":1539759151141},{"_id":"source/_posts/docker-traefik-usage.md","hash":"b75403d2bb018d6fa622e2a0404bb624d3e9e3e3","modified":1527041870232},{"_id":"source/_posts/kafka-cluster.md","hash":"5e443b91c47f2aa1820198b6c20d41a26a070ad5","modified":1530002792135},{"_id":"source/_posts/kubernetes-configmap-usage.md","hash":"26757526d91e497152c6761cf4d6edd029bf2e1f","modified":1527041664373},{"_id":"source/_posts/kubernetes-java-client.md","hash":"1c9901c485b5666817297525b1ef4e7ec8fa1520","modified":1527041461361},{"_id":"source/_posts/kubernetes-kubeadmin-init.md","hash":"e770df5fd0c973c41973a1e699a61595f29f34e4","modified":1529380436396},{"_id":"themes/archer/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1539669271859},{"_id":"themes/archer/.git/config","hash":"cdce7d63d0abdeb24d5df5736f4a0062220ae695","modified":1539669271875},{"_id":"themes/archer/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1539669232595},{"_id":"themes/archer/.git/index","hash":"af5a97e5314bdc527217478e070601ea0018c958","modified":1539669272042},{"_id":"themes/archer/.git/packed-refs","hash":"0bf6bc8b65d3f6c5633b771e84058bc82a6fa891","modified":1539669271846},{"_id":"themes/archer/dev/archer.sh","hash":"9474c501c1c55f47f02cccdd9e2039498ebc5e43","modified":1539669271908},{"_id":"themes/archer/docs/README-en.md","hash":"e4fff6fc13f3296c2b168ab220f847192bf1273b","modified":1539669271910},{"_id":"themes/archer/docs/develop-guide-en.md","hash":"c10293eb8ccad5d02412a1369ec1c7e77516b929","modified":1539669271911},{"_id":"themes/archer/docs/develop-guide-zh.md","hash":"522434202e5e810b3c7f9591eb3a4451a4e485f0","modified":1539669271912},{"_id":"themes/archer/languages/default.yml","hash":"c6e36691ded2a5e5939c67bf3c47104664ea56c6","modified":1539669271917},{"_id":"themes/archer/languages/en.yml","hash":"de8fe8ad1967854586fee2214094c4992dd05278","modified":1539669271918},{"_id":"themes/archer/layout/404.ejs","hash":"a054b4ea1147846bed4252dd56182cb8e32d95eb","modified":1539669271920},{"_id":"themes/archer/layout/about.ejs","hash":"990df15653a99453617e72dfc195fa0a75b9a5d1","modified":1539669271949},{"_id":"themes/archer/layout/index.ejs","hash":"c9ae77cd8f7b862d23137a7b4eb5eb01b558ed33","modified":1539669271950},{"_id":"themes/archer/layout/layout.ejs","hash":"fffd1188977f74d19e2f9d267d085dddc9c6778e","modified":1539669271951},{"_id":"themes/archer/layout/post.ejs","hash":"f14b49c920a3afe8d9aa4cb1d843faf43d07c72e","modified":1539669271952},{"_id":"themes/archer/layout/site-meta.ejs","hash":"a9d85607fc7da51bb9becff7fe2f07a8b4fbc915","modified":1539669271954},{"_id":"source/_posts/开发日志/Content-disposition.md","hash":"59aec974f6c160ffbeaa3cbcbecbf869c60cee35","modified":1539660743223},{"_id":"themes/archer/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1539669232611},{"_id":"themes/archer/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1539669232626},{"_id":"themes/archer/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1539669232631},{"_id":"themes/archer/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1539669232639},{"_id":"themes/archer/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1539669232635},{"_id":"themes/archer/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1539669232643},{"_id":"themes/archer/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1539669232648},{"_id":"themes/archer/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1539669232661},{"_id":"themes/archer/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1539669232665},{"_id":"themes/archer/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1539669232670},{"_id":"themes/archer/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1539669232674},{"_id":"themes/archer/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1539669232677},{"_id":"themes/archer/.git/logs/HEAD","hash":"86f5f79c5e352201e92fe68ef2d90135ff727da9","modified":1539669271865},{"_id":"themes/archer/.github/ISSUE_TEMPLATE/-----------bug--help-wanted-or-bug-report-.md","hash":"7d1c5dbbc89b03b9e764e71aedb3f9567bed49bf","modified":1539669271897},{"_id":"themes/archer/.github/ISSUE_TEMPLATE/-----feature-request-.md","hash":"ce7449948855556971a7353d4bfc7e8cd1b49634","modified":1539669271898},{"_id":"themes/archer/.github/ISSUE_TEMPLATE/-----other-issue-.md","hash":"1ee1770c446ffe4d489db8d216981f473da4addc","modified":1539669271900},{"_id":"themes/archer/layout/_partial/algolia.ejs","hash":"21765ec5abc9a65513e6bff57cb021d3b3852d35","modified":1539669271921},{"_id":"themes/archer/layout/_partial/base-background-image.ejs","hash":"a90bd2b089b335b141a6a0d8dee9a9cde60fdc5b","modified":1539669271922},{"_id":"themes/archer/layout/_partial/base-footer.ejs","hash":"179a712ae139d8c5123338d9bd39d2b09c5ed2ce","modified":1539669271923},{"_id":"themes/archer/layout/_partial/base-head.ejs","hash":"b343f5a4c8bba1c71f3229390ed83a8679c2f375","modified":1539669271925},{"_id":"themes/archer/layout/_partial/base-header.ejs","hash":"af76b1a18a63934e83b078bf5f9f886e972a0ceb","modified":1539669271926},{"_id":"themes/archer/layout/_partial/base-preload-polyfill.ejs","hash":"065f8d6c4aae6782e6819815911f7feb6402a4ec","modified":1539669271927},{"_id":"themes/archer/layout/_partial/base-profile.ejs","hash":"5dadba1c15454e25740982b59ef382686f3c6e51","modified":1539669271928},{"_id":"themes/archer/layout/_partial/base-social.ejs","hash":"92ac580acc20bde7b3345bfe132671b9043bfbd6","modified":1539669271929},{"_id":"themes/archer/layout/_partial/base-title-tags.ejs","hash":"e1b4893af2b18f502bad1b552c3f3381ecc3021f","modified":1539669271930},{"_id":"themes/archer/layout/_partial/intro-height.ejs","hash":"fc03729825ac7ffd4045f910bbd936bc5841c65e","modified":1539669271940},{"_id":"themes/archer/source/assets/algolia_logo.svg","hash":"16505f61f19ba65f629dfd033f14ee9abcf18756","modified":1539669271961},{"_id":"themes/archer/source/assets/example_qr.png","hash":"cce20432c34875f4d9c6df927ede0fc0f00bb194","modified":1539669271962},{"_id":"themes/archer/source/assets/favicon.ico","hash":"876bc62fdae04dccebc1367cc24c081077c13ab8","modified":1526983309515},{"_id":"themes/archer/source/assets/loading.svg","hash":"85082b002bae1335114b71550350907884187e38","modified":1539669271964},{"_id":"themes/archer/source/css/mobile.css","hash":"770d2ffb986a814c5e5662d28d2ec38864d8b1a0","modified":1539669271967},{"_id":"themes/archer/source/avatar/Misaka.jpg","hash":"74a0372523f98dfbba992bf80642e160d04dc9b1","modified":1539669271966},{"_id":"themes/archer/source/css/style.css","hash":"49f5d23b1e24e73016efdd8a6b70964a45a601db","modified":1539669271968},{"_id":"themes/archer/source/font/Source Sans Pro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1539669271972},{"_id":"themes/archer/source/font/Source Sans Pro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1539669271973},{"_id":"themes/archer/source/lib/webfontloader.min.js","hash":"6f18a92bbe8bed93113449ed6ff8d148c1e7565a","modified":1539669271988},{"_id":"themes/archer/source/scripts/main.js","hash":"e89a9651131684204118fa74bd19cb781d76a372","modified":1539669271990},{"_id":"themes/archer/source/scripts/search.js","hash":"d5f739e261e8ce74f993c6157b248663bda122bf","modified":1539669271991},{"_id":"themes/archer/source/scripts/share.js","hash":"bb5bb37ce7f47f8c084b232df3e5fe2378d7ca01","modified":1539669271993},{"_id":"themes/archer/src/js/browser.js","hash":"35ea55f62c0f251817c46da0153f70aec323be3d","modified":1539669271995},{"_id":"themes/archer/src/js/fancybox.js","hash":"701720a4c56fb09d78ef3494fd3a88b4e2cde238","modified":1539669271996},{"_id":"themes/archer/src/js/init.js","hash":"0cf1eb883b87590a771b687431cc5180064d13cd","modified":1539669271997},{"_id":"themes/archer/src/js/main.js","hash":"673e19867b5ecf65b0c9c6579d40be587d0e8211","modified":1539669271999},{"_id":"themes/archer/src/js/initSidebar.js","hash":"e27d52b4ccb9502836570a0673f6b656f9ce3661","modified":1539669271998},{"_id":"themes/archer/src/js/mobile.js","hash":"cfc37862242d5b7bd17028269436d3bc42e142e8","modified":1539669272000},{"_id":"themes/archer/src/js/scroll.js","hash":"83f0885f2673234c911d84a56fcaaa020cdcc3e6","modified":1539669272002},{"_id":"themes/archer/src/js/search.js","hash":"ef6533c876831377542a57f447550689144a6b71","modified":1539669272003},{"_id":"themes/archer/src/js/share.js","hash":"83a8ad056039cc673b939f47d69ad56337abc382","modified":1539669272005},{"_id":"themes/archer/src/js/sidebar.js","hash":"f2a5ec38766b3f48e3b043b84029f9da878d1c1a","modified":1539669272006},{"_id":"themes/archer/src/js/tag.js","hash":"a868118bb0daf8902b63c48792797def36b9469a","modified":1539669272008},{"_id":"themes/archer/src/js/toc.js","hash":"5c4db488dc2d80315064c6832eecfb39017a8104","modified":1539669272009},{"_id":"themes/archer/src/js/util.js","hash":"7bec5b1ff8ee91f58d084c47e0ed3908b27adef7","modified":1539669272010},{"_id":"themes/archer/src/scss/_common.scss","hash":"05162d1562d4b141d9bad44c37b0b736371d8b08","modified":1539669272012},{"_id":"themes/archer/src/scss/_mixin.scss","hash":"c6347bf137e80bb0c5b6368488ebd634c299016a","modified":1539669272013},{"_id":"themes/archer/src/scss/_normalize.scss","hash":"fb6a1349bab25b65cf89b47e136d958d10947533","modified":1539669272014},{"_id":"themes/archer/src/scss/_variables.scss","hash":"2264c9b671643b3c64e002f968a75355e4091b61","modified":1539669272035},{"_id":"themes/archer/src/scss/mobile.scss","hash":"0a00e55e8b8d9551eed5966f53d8a6580b816dc5","modified":1539669272037},{"_id":"themes/archer/src/scss/style.scss","hash":"03bca94e57bc44723f8b73bb653f3e002b4b08a8","modified":1539669272038},{"_id":"themes/archer/layout/_partial/comment/custom.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1539669271932},{"_id":"themes/archer/source/intro/404-bg.jpg","hash":"3afb5bb26f4ff0bd0e0a28df955c8aa7d746d3c5","modified":1539669271978},{"_id":"themes/archer/source/font/Oswald-Regular.ttf","hash":"965d729546a43a8490ad4cf33c25ac475682100c","modified":1539669271971},{"_id":"themes/archer/source/font/SourceCodePro-Regular.ttf.woff","hash":"12eef75e1ad3eca9dae42b65505010ce4464a315","modified":1539669271974},{"_id":"themes/archer/source/font/SourceCodePro-Regular.ttf.woff2","hash":"f5991289ec17884cb641da0646d278d36702a190","modified":1539669271976},{"_id":"themes/archer/source/lib/jquery.min.js","hash":"0c3192b500a4fd550e483cf77a49806a5872185b","modified":1539669271987},{"_id":"themes/archer/.git/refs/heads/master","hash":"a348fab820e88d801db0d6877198f89aa953bca0","modified":1539669271864},{"_id":"themes/archer/layout/_partial/comment/changyan.ejs","hash":"244a4c71b862e6385a6de1e83a4975418a8c6fe7","modified":1539669271931},{"_id":"themes/archer/layout/_partial/comment/gitment.ejs","hash":"c043a98e33252b9a628bfd31c4a3b2883dce2b99","modified":1539669271934},{"_id":"themes/archer/layout/_partial/comment/disqus.ejs","hash":"cfdb15116abe89f4b54f7bb18a0b23a597c47a60","modified":1539669271933},{"_id":"themes/archer/layout/_partial/comment/valine.ejs","hash":"f1f785de72e1f7056da8fdb12c85523d20a0b6b0","modified":1539669271936},{"_id":"themes/archer/layout/_partial/comment/livere.ejs","hash":"a6acb5d7778ade98ba5b6932e38a585460513f49","modified":1539669271935},{"_id":"themes/archer/layout/_partial/comment/youyan.ejs","hash":"483c07212879b116b772f428547c9962be96d2ce","modified":1539669271938},{"_id":"themes/archer/layout/_partial/critical-css/critical-style.ejs","hash":"9cd4798cdcd712cc01c9e797adbb4810649310d4","modified":1539669271939},{"_id":"themes/archer/layout/_partial/sidebar/base-sidebar.ejs","hash":"c5ce643a06a2103aa4ac0d8279c024f90886d37f","modified":1539669271944},{"_id":"themes/archer/layout/_partial/sidebar/sidebar-archives.ejs","hash":"e710acbc85e1cc5ae0e7ab5b5899837b9f222b97","modified":1539669271945},{"_id":"themes/archer/layout/_partial/sidebar/sidebar-categories.ejs","hash":"4feb7c3d17a3c8994eb095d43d75fbd0f1ed5b4f","modified":1539669271946},{"_id":"themes/archer/layout/_partial/script/font-loader.ejs","hash":"4281841e5bbb5e1a83c3ebf6506dab057e1fe6b9","modified":1539669271942},{"_id":"themes/archer/layout/_partial/sidebar/sidebar-tags.ejs","hash":"e96750b1aa7113322696857882b1e1fde11c1fc4","modified":1539669271947},{"_id":"themes/archer/package-lock.json","hash":"f3eae5d3878f7761231613b74be88173d3047c0c","modified":1539669271957},{"_id":"themes/archer/src/scss/_partial/_404.scss","hash":"55c33bd49880ccaaa2f5d4dff44470886c3f3633","modified":1539669272016},{"_id":"themes/archer/src/scss/_partial/_algolia.scss","hash":"08594f05f6ed238a7b79d48ebc1ff1e5c9deec46","modified":1539669272017},{"_id":"themes/archer/src/scss/_partial/_index-page.scss","hash":"82a04b6ebb684a87f486cac8f08619ecb52605fd","modified":1539669272019},{"_id":"themes/archer/src/scss/_partial/_post-page.scss","hash":"4d4713ec4b766a60577546af9f98eb721c4a63cd","modified":1539669272028},{"_id":"themes/archer/.git/objects/pack/pack-bf7c0bf5b6b17eeaf30850f103d8522d34e8082c.idx","hash":"152c229993509bf77bfa19985c31f644770726d1","modified":1539669271802},{"_id":"themes/archer/docs/snap.png","hash":"0b2a8bf016f6eed576abfdcdb7dcf8de51c12562","modified":1539669271914},{"_id":"themes/archer/source/intro/about-bg.jpg","hash":"ab388276822417cc4e703312c14e20280ec783b3","modified":1539669271980},{"_id":"themes/archer/source/intro/post-bg.jpg","hash":"525fafb2238c27754d8fa751f143ff1de9b8482d","modified":1539669271985},{"_id":"themes/archer/.git/logs/refs/heads/master","hash":"86f5f79c5e352201e92fe68ef2d90135ff727da9","modified":1539669271867},{"_id":"themes/archer/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1539669271857},{"_id":"themes/archer/src/scss/_partial/_partial/_footer.scss","hash":"46d315718d86e9701573df2e39299895d8ca193c","modified":1539669272020},{"_id":"themes/archer/src/scss/_partial/_partial/_header.scss","hash":"a1163d481627864e9dda441c4c356169cd33a3a1","modified":1539669272022},{"_id":"themes/archer/src/scss/_partial/_partial/_intro.scss","hash":"b3cf7523d8e91fce92cd6a118e308632047cc7b3","modified":1539669272023},{"_id":"themes/archer/src/scss/_partial/_partial/_paginator.scss","hash":"1e4510959f51b5d4d3c5781468c77d25546ce905","modified":1539669272024},{"_id":"themes/archer/src/scss/_partial/_partial/_scrollbar.scss","hash":"c623385d72ce12fa403f5384ded79c3897d0fc9f","modified":1539669272027},{"_id":"themes/archer/src/scss/_partial/_partial/_profile.scss","hash":"b6db1d4a29c34e1b834c54ca076b8084ceac4885","modified":1539669272025},{"_id":"themes/archer/src/scss/_partial/_post/_code.scss","hash":"30a3b9165c97bac6f6c795e598408639f5d9670a","modified":1539669272030},{"_id":"themes/archer/src/scss/_partial/_sidebar/_sidebar-archive.scss","hash":"28b97b11f17d57a258747bb0facae0ad1911f9d5","modified":1539669272031},{"_id":"themes/archer/src/scss/_partial/_sidebar/_sidebar-tags.scss","hash":"0691ca23bf6eabef65ff054c7710ec7432e4022c","modified":1539669272033},{"_id":"themes/archer/src/scss/_partial/_sidebar/_sidebar.scss","hash":"b41904da65b1128510d28a57142298bfd844f7f5","modified":1539669272034},{"_id":"themes/archer/source/intro/index-bg.jpg","hash":"96b52e177b8bc53e64ec6ee1e10b2b6a4e13083b","modified":1539669271983},{"_id":"themes/archer/.git/logs/refs/remotes/origin/HEAD","hash":"86f5f79c5e352201e92fe68ef2d90135ff727da9","modified":1539669271857},{"_id":"themes/archer/.git/objects/pack/pack-bf7c0bf5b6b17eeaf30850f103d8522d34e8082c.pack","hash":"e8184da22b2f5004b239f4a7cf166e773ff6a5b1","modified":1539669271808},{"_id":"public/content.json","hash":"cf757de09f97740f1a42030a9700ced6b88f9c98","modified":1539759163739},{"_id":"public/2018/10/16/开发日志/Content-disposition/index.html","hash":"71da0b81d58ea4910d1781b0934464f81676f5da","modified":1539759163976},{"_id":"public/2018/06/26/kafka-cluster/index.html","hash":"7cbac2e60f64fd6f5dbc0f5a4806c5e90d6d4dd3","modified":1539759163976},{"_id":"public/2018/06/15/kubernetes-kubeadmin-init/index.html","hash":"f3bedc82fe10873e044cbe149f170aca1cc75661","modified":1539759163976},{"_id":"public/2018/05/24/elasticsearch-fluent-kibana/index.html","hash":"dc39e1ecfb7c248239177f3320ef65e98e8cbbe3","modified":1539759163976},{"_id":"public/2018/04/02/docker-traefik-usage/index.html","hash":"085902507d477a136405cce27afe40f1ecdd4cf0","modified":1539759163977},{"_id":"public/2018/03/29/kubernetes-configmap-usage/index.html","hash":"92696b4374589374093f06d45bad835891ae61fa","modified":1539759163977},{"_id":"public/2018/03/29/kubernetes-java-client/index.html","hash":"4584abafe25d50e7b7ad137517579f652621b69b","modified":1539759163977},{"_id":"public/2018/03/11/Kubernetes-deploy-Jenkins-slave/index.html","hash":"85e7f64d60b15db900c58d5b9721a9e1a637ca1c","modified":1539759163977},{"_id":"public/2018/03/01/Dockerfile-syntax/index.html","hash":"de0d0caa80ef9214a6c632228971d068deb3deec","modified":1539759163977},{"_id":"public/2018/01/11/docker-install-note/index.html","hash":"700897816c7081a30867d41f2e87f805fe4aece7","modified":1539759163977},{"_id":"public/archives/index.html","hash":"803c62e4651cfbe2c073718480e991f754e78a6c","modified":1539759163977},{"_id":"public/archives/2018/index.html","hash":"803c62e4651cfbe2c073718480e991f754e78a6c","modified":1539759163977},{"_id":"public/archives/2018/01/index.html","hash":"97058e528164183e0f44e21f5875173e62c855cb","modified":1539759163977},{"_id":"public/archives/2018/03/index.html","hash":"4d1ecbce388c2fd9558e1a4277e1a7a92b55eab7","modified":1539759163977},{"_id":"public/archives/2018/04/index.html","hash":"4d654c6c5c3dd2c39d5d4dbdd2d917cac28acdf8","modified":1539759163978},{"_id":"public/archives/2018/05/index.html","hash":"a57ce80855d851b23c144c59b23d75befa79f520","modified":1539759163978},{"_id":"public/archives/2018/06/index.html","hash":"03f8d0c819736fd58087f240660ad73c51f512e6","modified":1539759163978},{"_id":"public/archives/2018/10/index.html","hash":"30684919c82b6a212c89c6ff7f360019b68c6952","modified":1539759163978},{"_id":"public/tags/docker/index.html","hash":"69f5a5a5d5f64c9e8f8cb12d11157106d8fe8f1d","modified":1539759163978},{"_id":"public/tags/kubernetes/index.html","hash":"1ec9c87f01247984ae4a821795eec845b756799e","modified":1539759163978},{"_id":"public/tags/jenkins/index.html","hash":"e12c27a6c363ba76e7e1db1d3608326cc449647e","modified":1539759163978},{"_id":"public/tags/Elasticsearch/index.html","hash":"a57ce80855d851b23c144c59b23d75befa79f520","modified":1539759163978},{"_id":"public/tags/Docker/index.html","hash":"a57ce80855d851b23c144c59b23d75befa79f520","modified":1539759163978},{"_id":"public/tags/Fluentd/index.html","hash":"a57ce80855d851b23c144c59b23d75befa79f520","modified":1539759163978},{"_id":"public/tags/Kibana/index.html","hash":"a57ce80855d851b23c144c59b23d75befa79f520","modified":1539759163978},{"_id":"public/tags/configmap/index.html","hash":"a7291957648635d53e07177da277e3ac8fcb0553","modified":1539759163978},{"_id":"public/tags/kafka/index.html","hash":"17766bf47838b3b433c53719a600d17cc575d86a","modified":1539759163978},{"_id":"public/tags/开发日志/index.html","hash":"30684919c82b6a212c89c6ff7f360019b68c6952","modified":1539759163978},{"_id":"public/index.html","hash":"07eabf1f045f288043e92a4acf49dd6a6b81d2ad","modified":1539759163978},{"_id":"public/assets/algolia_logo.svg","hash":"16505f61f19ba65f629dfd033f14ee9abcf18756","modified":1539759164005},{"_id":"public/assets/example_qr.png","hash":"cce20432c34875f4d9c6df927ede0fc0f00bb194","modified":1539759164005},{"_id":"public/assets/loading.svg","hash":"85082b002bae1335114b71550350907884187e38","modified":1539759164006},{"_id":"public/assets/favicon.ico","hash":"876bc62fdae04dccebc1367cc24c081077c13ab8","modified":1539759164006},{"_id":"public/avatar/Misaka.jpg","hash":"74a0372523f98dfbba992bf80642e160d04dc9b1","modified":1539759164006},{"_id":"public/font/Source Sans Pro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1539759164006},{"_id":"public/font/Source Sans Pro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1539759164006},{"_id":"public/css/mobile.css","hash":"51d7b5cf26c8f82a3daad4bc55518673fdf281f5","modified":1539759164010},{"_id":"public/intro/404-bg.jpg","hash":"3afb5bb26f4ff0bd0e0a28df955c8aa7d746d3c5","modified":1539759164010},{"_id":"public/font/Oswald-Regular.ttf","hash":"965d729546a43a8490ad4cf33c25ac475682100c","modified":1539759164011},{"_id":"public/font/SourceCodePro-Regular.ttf.woff","hash":"12eef75e1ad3eca9dae42b65505010ce4464a315","modified":1539759164011},{"_id":"public/font/SourceCodePro-Regular.ttf.woff2","hash":"f5991289ec17884cb641da0646d278d36702a190","modified":1539759164011},{"_id":"public/scripts/search.js","hash":"d5f739e261e8ce74f993c6157b248663bda122bf","modified":1539759164014},{"_id":"public/lib/webfontloader.min.js","hash":"bc6ffe9c0d8b3285564619a445c6ca575eb9d0f5","modified":1539759164014},{"_id":"public/css/style.css","hash":"5d5e869db0c62957d59c6865baaecd1428c51804","modified":1539759164014},{"_id":"public/scripts/main.js","hash":"e89a9651131684204118fa74bd19cb781d76a372","modified":1539759164014},{"_id":"public/scripts/share.js","hash":"bb5bb37ce7f47f8c084b232df3e5fe2378d7ca01","modified":1539759164015},{"_id":"public/lib/jquery.min.js","hash":"0dc32db4aa9c5f03f3b38c47d883dbd4fed13aae","modified":1539759164015},{"_id":"public/intro/about-bg.jpg","hash":"ab388276822417cc4e703312c14e20280ec783b3","modified":1539759164325},{"_id":"public/intro/post-bg.jpg","hash":"525fafb2238c27754d8fa751f143ff1de9b8482d","modified":1539759164325},{"_id":"public/intro/index-bg.jpg","hash":"96b52e177b8bc53e64ec6ee1e10b2b6a4e13083b","modified":1539759164371}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Dockerfile语法结构","date":"2018-02-28T16:00:00.000Z","_content":"Dockerfile语法结构\n## Dockerfile 指令\n\nDockerfile 有以下指令选项:\n### FROM  \nFROM指定构建镜像的基础源镜像，如果本地没有指定的镜像，则会自动从 Docker 的\t公共库 pull 镜像下来。\n```\nFROM <image>:<tag> \n```\n\n### MAINTAINER\n指定创建镜像的用户\n```\nMAINTAINER <name>\n```\n\n### RUN\n有两种使用方式\n```\nRUN \"executable\", \"param1\", \"param2\"\n```\n每条RUN指令将在当前镜像基础上执行指定命令，并提交为新的镜像，后续的RUN都\t在之前RUN提交后的镜像为基础，镜像是分层的，可以通过一个镜像的任何一个历史\t提交点来创建，类似源码的版本控制。\n\nexec 方式会被解析为一个 JSON 数组，所以必须使用双引号而不是单引号。exec 方式\t不会调用一个命令 shell，所以也就不会继承相应的变量，如：\n```\nRUN [ \"echo\", \"$HOME\" ]\n```\n\n这种方式是不会达到输出 HOME 变量的，正确的方式应该是这样的\n```\nRUN [ \"sh\", \"-c\", \"echo\", \"$HOME\" ]\n```\n\n### CMD\nCMD有三种使用方式:\n```\nCMD \"executable\",\"param1\",\"param2\"\nCMD \"param1\",\"param2\"\nCMD command param1 param2 (shell form)\n```\n\n### EXPOSE\n告诉 Docker 服务端容器对外映射的本地端口，需要在 docker run 的时候使用-p或\t-P选项生效。\n```\nEXPOSE <port> [<port>...]  \n```\n\n### ENV\n指定一个环节变量，会被后续RUN指令使用，并在容器运行时保留。\n```\nENV <key> <value>       # 只能设置一个变量\n```\n\n### ADD\n```\nADD <src> <dest>\nADD tomcat7.sh /etc/init.d/tomcat7\n```\n所有拷贝到container中的文件和文件夹权限为0755，uid和gid为0；如果是一个目录，\t那么会将该目录下的所有文件添加到container中，不包括目录；如果文件是可识别的\t压缩格式，则docker会帮忙解压缩（注意压缩格式）；如果<src>是文件且<dest>中不使\t用斜杠结束，则会将<dest>视为文件，<src>的内容会写入<dest>；如果<src>是文件且\t<dest>中使用斜杠结束，则会<src>文件拷贝到<dest>目录下。\n\nADD复制本地主机文件、目录或者远程文件 URLS 从 并且添加到容器指定路径中 。\n支持通过 GO 的正则模糊匹配，具体规则可参见 Go filepath.Match\n```\nADD hom* /mydir/        # adds all files starting with \"hom\"\nADD hom?.txt /mydir/    # ? is replaced with any single character\n```\n路径必须是绝对路径，如果 不存在，会自动创建对应目录\n路径必须是 Dockerfile 所在路径的相对路径\n如果是一个目录，只会复制目录下的内容，而目录本身则不会被复制\n\n### COPY\n```\nCOPY <src> <dest>\nCOPY tomcat7.sh /etc/init.d/tomcat7\n```\nCOPY复制新文件或者目录从 并且添加到容器指定路径中 。用法同ADD，唯一的不同\t是不能指定远程文件 URLS。\n\n### ENTRYPOINT\n设置指令，指定容器启动时执行的命令，可以多次设置，但是只有最后一个有效。\n两种格式:\n```\nENTRYPOINT [\"executable\", \"param1\", \"param2\"] #(like an exec, the preferred form)  \nENTRYPOINT command param1 param2 #(as a shell) \n```\n该指令的使用分为两种情况，一种是独自使用，另一种和CMD指令配合使用。\n当独自使用时，如果你还使用了CMD命令且CMD是一个完整的可执行的命令，那么\tCMD指令和ENTRYPOINT会互相覆盖只有最后一个CMD或者ENTRYPOINT有效。\n\n#### CMD指令将不会被执行，只有ENTRYPOINT指令被执行  \n```\nCMD echo “Hello, World!”  \nENTRYPOINT ls -l  \n```\n另一种用法和CMD指令配合使用来指定ENTRYPOINT的默认参数，这时CMD指令不是\t一个完整的可执行命令，仅仅是参数部分；ENTRYPOINT指令只能使用JSON方式指定执\t行命令，而不能指定参数。\n```\nFROM ubuntu  \nCMD [\"-l\"]  \nENTRYPOINT [\"/usr/bin/ls\"]\n```\n\n### VOLUME\n使容器中的一个目录具有持久化存储数据的功能，该目录可以被容器本身使用，也可以\t共享给其他容器使用。我们知道容器使用的是AUFS，这种文件系统不能持久化数据，\t当容器关闭后，所有的更改都会丢失。当容器中的应用有持久化数据的需求时可以在\tDockerfile中使用该指令。\n```\nVOLUME [\"/tmp/data\"]\n```\n运行通过该Dockerfile生成image的容器，/tmp/data目录中的数据在容器关闭后，里面\t的数据还存在。例如另一个容器也有持久化数据的需求，且想使用上面容器共享的\t/tmp/data目录，那么可以运行下面的命令启动一个容器：\n```\ndocker run -t -i -rm -volumes-from container1 image2 bash \n```\ncontainer1为第一个容器的ID，image2为第二个容器运行image的名字。\n\n### USER \n容器运行用户\n```\nUSER root\n```\n\n### WORKDIR\n可以多次切换(相当于cd命令)，对RUN,CMD,ENTRYPOINT生效。\n```\nWORKDIR /path/to/workdir  \n```\n\n## Dockerfile例子\n\n```\n# Pull base image  \nFROM ubuntu:13.10  \n  \nMAINTAINER xx \"email\"  \n  \n# all command in one\nRUN echo \"deb http://archive.ubuntu.com/ubuntu precise main universe\"> /etc/apt/sources.list && \\ \n   apt-get update && \\\n   apt-get -y install curl && \\\n   cd /tmp &&  curl -L 'http://download.oracle.com/otn-pub/java/jdk/7u65-b17/jdk-7u65-linux-x64.tar.gz' -H 'Cookie: oraclelicense=accept-securebackup-cookie; gpw_e24=Dockerfile' | tar -xz  && \\\n   mkdir -p /usr/lib/jvm && \\\n   mv /tmp/jdk1.7.0_65/ /usr/lib/jvm/java-7-oracle/  && \\\n   pdate-alternatives --install /usr/bin/java java /usr/lib/jvm/java-7-oracle/bin/java 300 && \\\n   update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-7-oracle/bin/javac 300 && \\\n   cd /tmp && curl -L 'http://archive.apache.org/dist/tomcat/tomcat-7/v7.0.8/bin/apache-tomcat-7.0.8.tar.gz' | tar -xz && \\\n   mv /tmp/apache-tomcat-7.0.8/ /opt/tomcat7/ && \\\n   chmod 755 /etc/init.d/tomcat7 \n \nENV JAVA_HOME /usr/lib/jvm/java-7-oracle/  \nENV CATALINA_HOME /opt/tomcat7  \nENV PATH $PATH:$CATALINA_HOME/bin  \n\nADD tomcat7.sh /etc/init.d/tomcat7  \n\n# Expose ports.  \nEXPOSE 8080  \n  \n# Define default command.  \nENTRYPOINT service tomcat7 start && tail -f /opt/tomcat7/logs/catalina.out  \n```","source":"_posts/Dockerfile-syntax.md","raw":"---\ntitle: Dockerfile语法结构\ntags: [docker]\ndate: 2018-03-01\n---\nDockerfile语法结构\n## Dockerfile 指令\n\nDockerfile 有以下指令选项:\n### FROM  \nFROM指定构建镜像的基础源镜像，如果本地没有指定的镜像，则会自动从 Docker 的\t公共库 pull 镜像下来。\n```\nFROM <image>:<tag> \n```\n\n### MAINTAINER\n指定创建镜像的用户\n```\nMAINTAINER <name>\n```\n\n### RUN\n有两种使用方式\n```\nRUN \"executable\", \"param1\", \"param2\"\n```\n每条RUN指令将在当前镜像基础上执行指定命令，并提交为新的镜像，后续的RUN都\t在之前RUN提交后的镜像为基础，镜像是分层的，可以通过一个镜像的任何一个历史\t提交点来创建，类似源码的版本控制。\n\nexec 方式会被解析为一个 JSON 数组，所以必须使用双引号而不是单引号。exec 方式\t不会调用一个命令 shell，所以也就不会继承相应的变量，如：\n```\nRUN [ \"echo\", \"$HOME\" ]\n```\n\n这种方式是不会达到输出 HOME 变量的，正确的方式应该是这样的\n```\nRUN [ \"sh\", \"-c\", \"echo\", \"$HOME\" ]\n```\n\n### CMD\nCMD有三种使用方式:\n```\nCMD \"executable\",\"param1\",\"param2\"\nCMD \"param1\",\"param2\"\nCMD command param1 param2 (shell form)\n```\n\n### EXPOSE\n告诉 Docker 服务端容器对外映射的本地端口，需要在 docker run 的时候使用-p或\t-P选项生效。\n```\nEXPOSE <port> [<port>...]  \n```\n\n### ENV\n指定一个环节变量，会被后续RUN指令使用，并在容器运行时保留。\n```\nENV <key> <value>       # 只能设置一个变量\n```\n\n### ADD\n```\nADD <src> <dest>\nADD tomcat7.sh /etc/init.d/tomcat7\n```\n所有拷贝到container中的文件和文件夹权限为0755，uid和gid为0；如果是一个目录，\t那么会将该目录下的所有文件添加到container中，不包括目录；如果文件是可识别的\t压缩格式，则docker会帮忙解压缩（注意压缩格式）；如果<src>是文件且<dest>中不使\t用斜杠结束，则会将<dest>视为文件，<src>的内容会写入<dest>；如果<src>是文件且\t<dest>中使用斜杠结束，则会<src>文件拷贝到<dest>目录下。\n\nADD复制本地主机文件、目录或者远程文件 URLS 从 并且添加到容器指定路径中 。\n支持通过 GO 的正则模糊匹配，具体规则可参见 Go filepath.Match\n```\nADD hom* /mydir/        # adds all files starting with \"hom\"\nADD hom?.txt /mydir/    # ? is replaced with any single character\n```\n路径必须是绝对路径，如果 不存在，会自动创建对应目录\n路径必须是 Dockerfile 所在路径的相对路径\n如果是一个目录，只会复制目录下的内容，而目录本身则不会被复制\n\n### COPY\n```\nCOPY <src> <dest>\nCOPY tomcat7.sh /etc/init.d/tomcat7\n```\nCOPY复制新文件或者目录从 并且添加到容器指定路径中 。用法同ADD，唯一的不同\t是不能指定远程文件 URLS。\n\n### ENTRYPOINT\n设置指令，指定容器启动时执行的命令，可以多次设置，但是只有最后一个有效。\n两种格式:\n```\nENTRYPOINT [\"executable\", \"param1\", \"param2\"] #(like an exec, the preferred form)  \nENTRYPOINT command param1 param2 #(as a shell) \n```\n该指令的使用分为两种情况，一种是独自使用，另一种和CMD指令配合使用。\n当独自使用时，如果你还使用了CMD命令且CMD是一个完整的可执行的命令，那么\tCMD指令和ENTRYPOINT会互相覆盖只有最后一个CMD或者ENTRYPOINT有效。\n\n#### CMD指令将不会被执行，只有ENTRYPOINT指令被执行  \n```\nCMD echo “Hello, World!”  \nENTRYPOINT ls -l  \n```\n另一种用法和CMD指令配合使用来指定ENTRYPOINT的默认参数，这时CMD指令不是\t一个完整的可执行命令，仅仅是参数部分；ENTRYPOINT指令只能使用JSON方式指定执\t行命令，而不能指定参数。\n```\nFROM ubuntu  \nCMD [\"-l\"]  \nENTRYPOINT [\"/usr/bin/ls\"]\n```\n\n### VOLUME\n使容器中的一个目录具有持久化存储数据的功能，该目录可以被容器本身使用，也可以\t共享给其他容器使用。我们知道容器使用的是AUFS，这种文件系统不能持久化数据，\t当容器关闭后，所有的更改都会丢失。当容器中的应用有持久化数据的需求时可以在\tDockerfile中使用该指令。\n```\nVOLUME [\"/tmp/data\"]\n```\n运行通过该Dockerfile生成image的容器，/tmp/data目录中的数据在容器关闭后，里面\t的数据还存在。例如另一个容器也有持久化数据的需求，且想使用上面容器共享的\t/tmp/data目录，那么可以运行下面的命令启动一个容器：\n```\ndocker run -t -i -rm -volumes-from container1 image2 bash \n```\ncontainer1为第一个容器的ID，image2为第二个容器运行image的名字。\n\n### USER \n容器运行用户\n```\nUSER root\n```\n\n### WORKDIR\n可以多次切换(相当于cd命令)，对RUN,CMD,ENTRYPOINT生效。\n```\nWORKDIR /path/to/workdir  \n```\n\n## Dockerfile例子\n\n```\n# Pull base image  \nFROM ubuntu:13.10  \n  \nMAINTAINER xx \"email\"  \n  \n# all command in one\nRUN echo \"deb http://archive.ubuntu.com/ubuntu precise main universe\"> /etc/apt/sources.list && \\ \n   apt-get update && \\\n   apt-get -y install curl && \\\n   cd /tmp &&  curl -L 'http://download.oracle.com/otn-pub/java/jdk/7u65-b17/jdk-7u65-linux-x64.tar.gz' -H 'Cookie: oraclelicense=accept-securebackup-cookie; gpw_e24=Dockerfile' | tar -xz  && \\\n   mkdir -p /usr/lib/jvm && \\\n   mv /tmp/jdk1.7.0_65/ /usr/lib/jvm/java-7-oracle/  && \\\n   pdate-alternatives --install /usr/bin/java java /usr/lib/jvm/java-7-oracle/bin/java 300 && \\\n   update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-7-oracle/bin/javac 300 && \\\n   cd /tmp && curl -L 'http://archive.apache.org/dist/tomcat/tomcat-7/v7.0.8/bin/apache-tomcat-7.0.8.tar.gz' | tar -xz && \\\n   mv /tmp/apache-tomcat-7.0.8/ /opt/tomcat7/ && \\\n   chmod 755 /etc/init.d/tomcat7 \n \nENV JAVA_HOME /usr/lib/jvm/java-7-oracle/  \nENV CATALINA_HOME /opt/tomcat7  \nENV PATH $PATH:$CATALINA_HOME/bin  \n\nADD tomcat7.sh /etc/init.d/tomcat7  \n\n# Expose ports.  \nEXPOSE 8080  \n  \n# Define default command.  \nENTRYPOINT service tomcat7 start && tail -f /opt/tomcat7/logs/catalina.out  \n```","slug":"Dockerfile-syntax","published":1,"updated":"2018-05-23T02:10:09.074Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjncsum4g00007gvr0qxx7m8f","content":"<p>Dockerfile语法结构</p>\n<h2 id=\"Dockerfile-指令\"><a href=\"#Dockerfile-指令\" class=\"headerlink\" title=\"Dockerfile 指令\"></a>Dockerfile 指令</h2><p>Dockerfile 有以下指令选项:</p>\n<h3 id=\"FROM\"><a href=\"#FROM\" class=\"headerlink\" title=\"FROM\"></a>FROM</h3><p>FROM指定构建镜像的基础源镜像，如果本地没有指定的镜像，则会自动从 Docker 的    公共库 pull 镜像下来。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM &lt;image&gt;:&lt;tag&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"MAINTAINER\"><a href=\"#MAINTAINER\" class=\"headerlink\" title=\"MAINTAINER\"></a>MAINTAINER</h3><p>指定创建镜像的用户<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MAINTAINER &lt;name&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"RUN\"><a href=\"#RUN\" class=\"headerlink\" title=\"RUN\"></a>RUN</h3><p>有两种使用方式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RUN &quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;</span><br></pre></td></tr></table></figure></p>\n<p>每条RUN指令将在当前镜像基础上执行指定命令，并提交为新的镜像，后续的RUN都    在之前RUN提交后的镜像为基础，镜像是分层的，可以通过一个镜像的任何一个历史    提交点来创建，类似源码的版本控制。</p>\n<p>exec 方式会被解析为一个 JSON 数组，所以必须使用双引号而不是单引号。exec 方式    不会调用一个命令 shell，所以也就不会继承相应的变量，如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RUN [ &quot;echo&quot;, &quot;$HOME&quot; ]</span><br></pre></td></tr></table></figure></p>\n<p>这种方式是不会达到输出 HOME 变量的，正确的方式应该是这样的<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RUN [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo&quot;, &quot;$HOME&quot; ]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"CMD\"><a href=\"#CMD\" class=\"headerlink\" title=\"CMD\"></a>CMD</h3><p>CMD有三种使用方式:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CMD &quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;</span><br><span class=\"line\">CMD &quot;param1&quot;,&quot;param2&quot;</span><br><span class=\"line\">CMD command param1 param2 (shell form)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"EXPOSE\"><a href=\"#EXPOSE\" class=\"headerlink\" title=\"EXPOSE\"></a>EXPOSE</h3><p>告诉 Docker 服务端容器对外映射的本地端口，需要在 docker run 的时候使用-p或    -P选项生效。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPOSE &lt;port&gt; [&lt;port&gt;...]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"ENV\"><a href=\"#ENV\" class=\"headerlink\" title=\"ENV\"></a>ENV</h3><p>指定一个环节变量，会被后续RUN指令使用，并在容器运行时保留。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ENV &lt;key&gt; &lt;value&gt;       # 只能设置一个变量</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"ADD\"><a href=\"#ADD\" class=\"headerlink\" title=\"ADD\"></a>ADD</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD &lt;src&gt; &lt;dest&gt;</span><br><span class=\"line\">ADD tomcat7.sh /etc/init.d/tomcat7</span><br></pre></td></tr></table></figure>\n<p>所有拷贝到container中的文件和文件夹权限为0755，uid和gid为0；如果是一个目录，    那么会将该目录下的所有文件添加到container中，不包括目录；如果文件是可识别的    压缩格式，则docker会帮忙解压缩（注意压缩格式）；如果<src>是文件且<dest>中不使    用斜杠结束，则会将<dest>视为文件，<src>的内容会写入<dest>；如果<src>是文件且    <dest>中使用斜杠结束，则会<src>文件拷贝到<dest>目录下。</dest></src></dest></src></dest></src></dest></dest></src></p>\n<p>ADD复制本地主机文件、目录或者远程文件 URLS 从 并且添加到容器指定路径中 。<br>支持通过 GO 的正则模糊匹配，具体规则可参见 Go filepath.Match<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD hom* /mydir/        # adds all files starting with &quot;hom&quot;</span><br><span class=\"line\">ADD hom?.txt /mydir/    # ? is replaced with any single character</span><br></pre></td></tr></table></figure></p>\n<p>路径必须是绝对路径，如果 不存在，会自动创建对应目录<br>路径必须是 Dockerfile 所在路径的相对路径<br>如果是一个目录，只会复制目录下的内容，而目录本身则不会被复制</p>\n<h3 id=\"COPY\"><a href=\"#COPY\" class=\"headerlink\" title=\"COPY\"></a>COPY</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">COPY &lt;src&gt; &lt;dest&gt;</span><br><span class=\"line\">COPY tomcat7.sh /etc/init.d/tomcat7</span><br></pre></td></tr></table></figure>\n<p>COPY复制新文件或者目录从 并且添加到容器指定路径中 。用法同ADD，唯一的不同    是不能指定远程文件 URLS。</p>\n<h3 id=\"ENTRYPOINT\"><a href=\"#ENTRYPOINT\" class=\"headerlink\" title=\"ENTRYPOINT\"></a>ENTRYPOINT</h3><p>设置指令，指定容器启动时执行的命令，可以多次设置，但是只有最后一个有效。<br>两种格式:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] #(like an exec, the preferred form)  </span><br><span class=\"line\">ENTRYPOINT command param1 param2 #(as a shell)</span><br></pre></td></tr></table></figure></p>\n<p>该指令的使用分为两种情况，一种是独自使用，另一种和CMD指令配合使用。<br>当独自使用时，如果你还使用了CMD命令且CMD是一个完整的可执行的命令，那么    CMD指令和ENTRYPOINT会互相覆盖只有最后一个CMD或者ENTRYPOINT有效。</p>\n<h4 id=\"CMD指令将不会被执行，只有ENTRYPOINT指令被执行\"><a href=\"#CMD指令将不会被执行，只有ENTRYPOINT指令被执行\" class=\"headerlink\" title=\"CMD指令将不会被执行，只有ENTRYPOINT指令被执行\"></a>CMD指令将不会被执行，只有ENTRYPOINT指令被执行</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CMD echo “Hello, World!”  </span><br><span class=\"line\">ENTRYPOINT ls -l</span><br></pre></td></tr></table></figure>\n<p>另一种用法和CMD指令配合使用来指定ENTRYPOINT的默认参数，这时CMD指令不是    一个完整的可执行命令，仅仅是参数部分；ENTRYPOINT指令只能使用JSON方式指定执    行命令，而不能指定参数。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM ubuntu  </span><br><span class=\"line\">CMD [&quot;-l&quot;]  </span><br><span class=\"line\">ENTRYPOINT [&quot;/usr/bin/ls&quot;]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"VOLUME\"><a href=\"#VOLUME\" class=\"headerlink\" title=\"VOLUME\"></a>VOLUME</h3><p>使容器中的一个目录具有持久化存储数据的功能，该目录可以被容器本身使用，也可以    共享给其他容器使用。我们知道容器使用的是AUFS，这种文件系统不能持久化数据，    当容器关闭后，所有的更改都会丢失。当容器中的应用有持久化数据的需求时可以在    Dockerfile中使用该指令。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">VOLUME [&quot;/tmp/data&quot;]</span><br></pre></td></tr></table></figure></p>\n<p>运行通过该Dockerfile生成image的容器，/tmp/data目录中的数据在容器关闭后，里面    的数据还存在。例如另一个容器也有持久化数据的需求，且想使用上面容器共享的    /tmp/data目录，那么可以运行下面的命令启动一个容器：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -t -i -rm -volumes-from container1 image2 bash</span><br></pre></td></tr></table></figure></p>\n<p>container1为第一个容器的ID，image2为第二个容器运行image的名字。</p>\n<h3 id=\"USER\"><a href=\"#USER\" class=\"headerlink\" title=\"USER\"></a>USER</h3><p>容器运行用户<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">USER root</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"WORKDIR\"><a href=\"#WORKDIR\" class=\"headerlink\" title=\"WORKDIR\"></a>WORKDIR</h3><p>可以多次切换(相当于cd命令)，对RUN,CMD,ENTRYPOINT生效。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">WORKDIR /path/to/workdir</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Dockerfile例子\"><a href=\"#Dockerfile例子\" class=\"headerlink\" title=\"Dockerfile例子\"></a>Dockerfile例子</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Pull base image  </span><br><span class=\"line\">FROM ubuntu:13.10  </span><br><span class=\"line\">  </span><br><span class=\"line\">MAINTAINER xx &quot;email&quot;  </span><br><span class=\"line\">  </span><br><span class=\"line\"># all command in one</span><br><span class=\"line\">RUN echo &quot;deb http://archive.ubuntu.com/ubuntu precise main universe&quot;&gt; /etc/apt/sources.list &amp;&amp; \\ </span><br><span class=\"line\">   apt-get update &amp;&amp; \\</span><br><span class=\"line\">   apt-get -y install curl &amp;&amp; \\</span><br><span class=\"line\">   cd /tmp &amp;&amp;  curl -L &apos;http://download.oracle.com/otn-pub/java/jdk/7u65-b17/jdk-7u65-linux-x64.tar.gz&apos; -H &apos;Cookie: oraclelicense=accept-securebackup-cookie; gpw_e24=Dockerfile&apos; | tar -xz  &amp;&amp; \\</span><br><span class=\"line\">   mkdir -p /usr/lib/jvm &amp;&amp; \\</span><br><span class=\"line\">   mv /tmp/jdk1.7.0_65/ /usr/lib/jvm/java-7-oracle/  &amp;&amp; \\</span><br><span class=\"line\">   pdate-alternatives --install /usr/bin/java java /usr/lib/jvm/java-7-oracle/bin/java 300 &amp;&amp; \\</span><br><span class=\"line\">   update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-7-oracle/bin/javac 300 &amp;&amp; \\</span><br><span class=\"line\">   cd /tmp &amp;&amp; curl -L &apos;http://archive.apache.org/dist/tomcat/tomcat-7/v7.0.8/bin/apache-tomcat-7.0.8.tar.gz&apos; | tar -xz &amp;&amp; \\</span><br><span class=\"line\">   mv /tmp/apache-tomcat-7.0.8/ /opt/tomcat7/ &amp;&amp; \\</span><br><span class=\"line\">   chmod 755 /etc/init.d/tomcat7 </span><br><span class=\"line\"> </span><br><span class=\"line\">ENV JAVA_HOME /usr/lib/jvm/java-7-oracle/  </span><br><span class=\"line\">ENV CATALINA_HOME /opt/tomcat7  </span><br><span class=\"line\">ENV PATH $PATH:$CATALINA_HOME/bin  </span><br><span class=\"line\"></span><br><span class=\"line\">ADD tomcat7.sh /etc/init.d/tomcat7  </span><br><span class=\"line\"></span><br><span class=\"line\"># Expose ports.  </span><br><span class=\"line\">EXPOSE 8080  </span><br><span class=\"line\">  </span><br><span class=\"line\"># Define default command.  </span><br><span class=\"line\">ENTRYPOINT service tomcat7 start &amp;&amp; tail -f /opt/tomcat7/logs/catalina.out</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>Dockerfile语法结构</p>\n<h2 id=\"Dockerfile-指令\"><a href=\"#Dockerfile-指令\" class=\"headerlink\" title=\"Dockerfile 指令\"></a>Dockerfile 指令</h2><p>Dockerfile 有以下指令选项:</p>\n<h3 id=\"FROM\"><a href=\"#FROM\" class=\"headerlink\" title=\"FROM\"></a>FROM</h3><p>FROM指定构建镜像的基础源镜像，如果本地没有指定的镜像，则会自动从 Docker 的    公共库 pull 镜像下来。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM &lt;image&gt;:&lt;tag&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"MAINTAINER\"><a href=\"#MAINTAINER\" class=\"headerlink\" title=\"MAINTAINER\"></a>MAINTAINER</h3><p>指定创建镜像的用户<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MAINTAINER &lt;name&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"RUN\"><a href=\"#RUN\" class=\"headerlink\" title=\"RUN\"></a>RUN</h3><p>有两种使用方式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RUN &quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;</span><br></pre></td></tr></table></figure></p>\n<p>每条RUN指令将在当前镜像基础上执行指定命令，并提交为新的镜像，后续的RUN都    在之前RUN提交后的镜像为基础，镜像是分层的，可以通过一个镜像的任何一个历史    提交点来创建，类似源码的版本控制。</p>\n<p>exec 方式会被解析为一个 JSON 数组，所以必须使用双引号而不是单引号。exec 方式    不会调用一个命令 shell，所以也就不会继承相应的变量，如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RUN [ &quot;echo&quot;, &quot;$HOME&quot; ]</span><br></pre></td></tr></table></figure></p>\n<p>这种方式是不会达到输出 HOME 变量的，正确的方式应该是这样的<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RUN [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo&quot;, &quot;$HOME&quot; ]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"CMD\"><a href=\"#CMD\" class=\"headerlink\" title=\"CMD\"></a>CMD</h3><p>CMD有三种使用方式:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CMD &quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;</span><br><span class=\"line\">CMD &quot;param1&quot;,&quot;param2&quot;</span><br><span class=\"line\">CMD command param1 param2 (shell form)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"EXPOSE\"><a href=\"#EXPOSE\" class=\"headerlink\" title=\"EXPOSE\"></a>EXPOSE</h3><p>告诉 Docker 服务端容器对外映射的本地端口，需要在 docker run 的时候使用-p或    -P选项生效。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPOSE &lt;port&gt; [&lt;port&gt;...]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"ENV\"><a href=\"#ENV\" class=\"headerlink\" title=\"ENV\"></a>ENV</h3><p>指定一个环节变量，会被后续RUN指令使用，并在容器运行时保留。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ENV &lt;key&gt; &lt;value&gt;       # 只能设置一个变量</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"ADD\"><a href=\"#ADD\" class=\"headerlink\" title=\"ADD\"></a>ADD</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD &lt;src&gt; &lt;dest&gt;</span><br><span class=\"line\">ADD tomcat7.sh /etc/init.d/tomcat7</span><br></pre></td></tr></table></figure>\n<p>所有拷贝到container中的文件和文件夹权限为0755，uid和gid为0；如果是一个目录，    那么会将该目录下的所有文件添加到container中，不包括目录；如果文件是可识别的    压缩格式，则docker会帮忙解压缩（注意压缩格式）；如果<src>是文件且<dest>中不使    用斜杠结束，则会将<dest>视为文件，<src>的内容会写入<dest>；如果<src>是文件且    <dest>中使用斜杠结束，则会<src>文件拷贝到<dest>目录下。</dest></src></dest></src></dest></src></dest></dest></src></p>\n<p>ADD复制本地主机文件、目录或者远程文件 URLS 从 并且添加到容器指定路径中 。<br>支持通过 GO 的正则模糊匹配，具体规则可参见 Go filepath.Match<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD hom* /mydir/        # adds all files starting with &quot;hom&quot;</span><br><span class=\"line\">ADD hom?.txt /mydir/    # ? is replaced with any single character</span><br></pre></td></tr></table></figure></p>\n<p>路径必须是绝对路径，如果 不存在，会自动创建对应目录<br>路径必须是 Dockerfile 所在路径的相对路径<br>如果是一个目录，只会复制目录下的内容，而目录本身则不会被复制</p>\n<h3 id=\"COPY\"><a href=\"#COPY\" class=\"headerlink\" title=\"COPY\"></a>COPY</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">COPY &lt;src&gt; &lt;dest&gt;</span><br><span class=\"line\">COPY tomcat7.sh /etc/init.d/tomcat7</span><br></pre></td></tr></table></figure>\n<p>COPY复制新文件或者目录从 并且添加到容器指定路径中 。用法同ADD，唯一的不同    是不能指定远程文件 URLS。</p>\n<h3 id=\"ENTRYPOINT\"><a href=\"#ENTRYPOINT\" class=\"headerlink\" title=\"ENTRYPOINT\"></a>ENTRYPOINT</h3><p>设置指令，指定容器启动时执行的命令，可以多次设置，但是只有最后一个有效。<br>两种格式:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] #(like an exec, the preferred form)  </span><br><span class=\"line\">ENTRYPOINT command param1 param2 #(as a shell)</span><br></pre></td></tr></table></figure></p>\n<p>该指令的使用分为两种情况，一种是独自使用，另一种和CMD指令配合使用。<br>当独自使用时，如果你还使用了CMD命令且CMD是一个完整的可执行的命令，那么    CMD指令和ENTRYPOINT会互相覆盖只有最后一个CMD或者ENTRYPOINT有效。</p>\n<h4 id=\"CMD指令将不会被执行，只有ENTRYPOINT指令被执行\"><a href=\"#CMD指令将不会被执行，只有ENTRYPOINT指令被执行\" class=\"headerlink\" title=\"CMD指令将不会被执行，只有ENTRYPOINT指令被执行\"></a>CMD指令将不会被执行，只有ENTRYPOINT指令被执行</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CMD echo “Hello, World!”  </span><br><span class=\"line\">ENTRYPOINT ls -l</span><br></pre></td></tr></table></figure>\n<p>另一种用法和CMD指令配合使用来指定ENTRYPOINT的默认参数，这时CMD指令不是    一个完整的可执行命令，仅仅是参数部分；ENTRYPOINT指令只能使用JSON方式指定执    行命令，而不能指定参数。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM ubuntu  </span><br><span class=\"line\">CMD [&quot;-l&quot;]  </span><br><span class=\"line\">ENTRYPOINT [&quot;/usr/bin/ls&quot;]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"VOLUME\"><a href=\"#VOLUME\" class=\"headerlink\" title=\"VOLUME\"></a>VOLUME</h3><p>使容器中的一个目录具有持久化存储数据的功能，该目录可以被容器本身使用，也可以    共享给其他容器使用。我们知道容器使用的是AUFS，这种文件系统不能持久化数据，    当容器关闭后，所有的更改都会丢失。当容器中的应用有持久化数据的需求时可以在    Dockerfile中使用该指令。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">VOLUME [&quot;/tmp/data&quot;]</span><br></pre></td></tr></table></figure></p>\n<p>运行通过该Dockerfile生成image的容器，/tmp/data目录中的数据在容器关闭后，里面    的数据还存在。例如另一个容器也有持久化数据的需求，且想使用上面容器共享的    /tmp/data目录，那么可以运行下面的命令启动一个容器：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -t -i -rm -volumes-from container1 image2 bash</span><br></pre></td></tr></table></figure></p>\n<p>container1为第一个容器的ID，image2为第二个容器运行image的名字。</p>\n<h3 id=\"USER\"><a href=\"#USER\" class=\"headerlink\" title=\"USER\"></a>USER</h3><p>容器运行用户<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">USER root</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"WORKDIR\"><a href=\"#WORKDIR\" class=\"headerlink\" title=\"WORKDIR\"></a>WORKDIR</h3><p>可以多次切换(相当于cd命令)，对RUN,CMD,ENTRYPOINT生效。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">WORKDIR /path/to/workdir</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Dockerfile例子\"><a href=\"#Dockerfile例子\" class=\"headerlink\" title=\"Dockerfile例子\"></a>Dockerfile例子</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Pull base image  </span><br><span class=\"line\">FROM ubuntu:13.10  </span><br><span class=\"line\">  </span><br><span class=\"line\">MAINTAINER xx &quot;email&quot;  </span><br><span class=\"line\">  </span><br><span class=\"line\"># all command in one</span><br><span class=\"line\">RUN echo &quot;deb http://archive.ubuntu.com/ubuntu precise main universe&quot;&gt; /etc/apt/sources.list &amp;&amp; \\ </span><br><span class=\"line\">   apt-get update &amp;&amp; \\</span><br><span class=\"line\">   apt-get -y install curl &amp;&amp; \\</span><br><span class=\"line\">   cd /tmp &amp;&amp;  curl -L &apos;http://download.oracle.com/otn-pub/java/jdk/7u65-b17/jdk-7u65-linux-x64.tar.gz&apos; -H &apos;Cookie: oraclelicense=accept-securebackup-cookie; gpw_e24=Dockerfile&apos; | tar -xz  &amp;&amp; \\</span><br><span class=\"line\">   mkdir -p /usr/lib/jvm &amp;&amp; \\</span><br><span class=\"line\">   mv /tmp/jdk1.7.0_65/ /usr/lib/jvm/java-7-oracle/  &amp;&amp; \\</span><br><span class=\"line\">   pdate-alternatives --install /usr/bin/java java /usr/lib/jvm/java-7-oracle/bin/java 300 &amp;&amp; \\</span><br><span class=\"line\">   update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-7-oracle/bin/javac 300 &amp;&amp; \\</span><br><span class=\"line\">   cd /tmp &amp;&amp; curl -L &apos;http://archive.apache.org/dist/tomcat/tomcat-7/v7.0.8/bin/apache-tomcat-7.0.8.tar.gz&apos; | tar -xz &amp;&amp; \\</span><br><span class=\"line\">   mv /tmp/apache-tomcat-7.0.8/ /opt/tomcat7/ &amp;&amp; \\</span><br><span class=\"line\">   chmod 755 /etc/init.d/tomcat7 </span><br><span class=\"line\"> </span><br><span class=\"line\">ENV JAVA_HOME /usr/lib/jvm/java-7-oracle/  </span><br><span class=\"line\">ENV CATALINA_HOME /opt/tomcat7  </span><br><span class=\"line\">ENV PATH $PATH:$CATALINA_HOME/bin  </span><br><span class=\"line\"></span><br><span class=\"line\">ADD tomcat7.sh /etc/init.d/tomcat7  </span><br><span class=\"line\"></span><br><span class=\"line\"># Expose ports.  </span><br><span class=\"line\">EXPOSE 8080  </span><br><span class=\"line\">  </span><br><span class=\"line\"># Define default command.  </span><br><span class=\"line\">ENTRYPOINT service tomcat7 start &amp;&amp; tail -f /opt/tomcat7/logs/catalina.out</span><br></pre></td></tr></table></figure>"},{"title":"Kubernetes部署Jenkins并动态资源分配","date":"2018-03-10T16:00:00.000Z","_content":"基于Kubernetes部署Jenkins，JenkinsSlave动态分配\n\n## 部署nfs服务\n需要把jenkins的home目录做持久化，解决方案用nfs\n  ①用外部nfs服务\n  ②在docker里面部署一个nfs服务\n  我们使用第二种方案\n这里还可以把编排到某个node，可以参考<br>https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n\n### 部署nfs\nnfs所在机器ip：192.168.10.93\n```\n$ docker run -d -p 2049:2049 \\\n   --name nfs \\\n   --privileged \\ \n   -e SHARED_DIRECTORY=/nfsshare \\\n   -v /opt/nfsshare:/nfsshare \\\n   --restart=always \\\n   itsthenetwork/nfs-server-alpine:7\n```\n\n### 查看nfs服务启动情况\n```\n$ docker logs nfs\n```\n```\n...\nStarting NFS in the background...\nrpc.nfsd: knfsd is currently down\nrpc.nfsd: Writing version string to kernel: -2 -3 +4 \nrpc.nfsd: Created AF_INET TCP socket.\nrpc.nfsd: Created AF_INET6 TCP socket.\nExporting File System...\nexporting *:/nfsshare\nStarting Mountd in the background...\n```\n\n### 测试nfs服务是否正常\n```\n#创建一个test目录\n$ mkdir -p /opt/nfsshare/test\n\n#把nfs根目录挂载到/mnt/下\n$ mount -t nfs 192.168.10.93:/ /mnt/\n\n#查看/mnt/下是否有test目录, 如果有，表示正常\n$ ls /mnt/\n\ntest\n```\n\n## 使用PersistentVolume持久化卷\n参考：https://jimmysong.io/kubernetes-handbook/concepts/persistent-volume.html\n### jenkins slave 数据持久化卷\njenkins slave 数据持久化卷\ncipvc.yaml\n```\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: cipv01\n  namespace: ci\nspec:\n  capacity:\n    storage: 2Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Recycle\n  storageClassName: slow\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    path: /jenkins\n    #这里填nfs服务器ip（前面nfs的地址）\n    server: 192.168.10.93\n    \n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: cipvc01\n  namespace: ci\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: slow\n```\n\n部署持久化卷\n```      \nkubelet apply -f cipvc.yaml\n```\n\n### mavan本地仓库久化卷\nmvnpvc.yaml\n```\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: mvnpv\n  namespace: ci\nspec:\n  capacity:\n    storage: 2Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Recycle\n  storageClassName: slow\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    path: /mavenLocalRepo\n    server: 192.168.10.93\n\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: mvnpvc\n  namespace: ci\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: slow\n```\n部署持久化卷\n```\nkubelet apply -f mvnpvc.yaml\n```\n\n### 查看pcv\n```\n$ kubectl get pvc -n ci\n\nNAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                 STORAGECLASS   REASON    AGE\ncipv01            2Gi        RWX            Recycle          Bound     ci/cipvc01            slow                     16h\nmvnpv             2Gi        RWX            Recycle          Bound     ci/mvnpvc             slow                     16h\n\n状态是Bound才可用\n```\n\n## 部署jenkins\njenkins.yaml\n```\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ci\n  \n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins\n  namespace: ci\n  \n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: jenkins\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: jenkins\n  namespace: ci\n  \n---\nkind: Deployment\napiVersion: apps/v1beta2\nmetadata:\n  name: jenkins\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jenkins\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: jenkins\n    spec:\n      serviceAccount: jenkins\n      securityContext:\n       runAsUser: 0\n      containers:\n      - name: jenkins\n        image: jenkinsci/blueocean:1.3.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: web\n          protocol: TCP\n        env:\n        - name: PORT\n          value: '8000'\n        - containerPort: 50000\n          name: agent\n          protocol: TCP\n        volumeMounts:\n        - name: jenkinshome\n          mountPath: /var/jenkins_home/\n        env:\n        - name: JAVA_OPTS\n          value: \"-Duser.timezone=Asia/Shanghai\"\n      volumes:\n      - name: jenkinshome\n      \t#pvc模式\n        persistentVolumeClaim:\n          claimName: cipvc01\n        #主机目录模式\n        #hostPath:\n         # directory location on host\n         #path: /opt/jenkins-blueocean-data\n         # this field is optional\n         #type: DirectoryOrCreate\n\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: jenkine-data-pv\n  namespace: ci\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Recycle\n  storageClassName: slow\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    path: /jenkins-blueocean-data\n    server: 192.168.10.93\n    \n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: jenkine-data-pvc\n  namespace: ci\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: slow\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    app: jenkins\n  name: jenkins\n  namespace: ci\nspec:\n  ports:\n  - port: 8090\n    targetPort: 8080\n    name: web\n  - port: 50000\n    targetPort: 50000\n    name: agent\n  selector:\n    app: jenkins\n  type: LoadBalancer\n```\n\n部署jenkins\n```\n$ kubectl apply -f jenkins.yaml\n```\n\n查看svc\n```\n$ kubectl get svc -n ci  \nNAME      TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE\njenkins   LoadBalancer   10.101.147.82   <pending>     8090:31830/TCP,50000:31870/TCP   8h\n```\n\n## 配置jenkins\n访问master/node:31830 端口，可用访问到jenkins\n配置好jenkins后，打开系统管理>插件管理>可选插件\n安装Kubernetes plugin\n安装好后，系统管理>系统设置>新增一个Kubernetes\n\n配置一下几项即可\n```\nName           kubernetes\nKubernetes URL https://kubernetes.default.svc.cluster.local\n#用svc的域名访问。端口是jenkins service的端口\nJenkins URL    http://jenkins.ci.svc.cluster.local:8090\n```\n\n## 测试\n\n接下来新建一个测试pipeline job\n\n填入以下代码：\n```\npodTemplate(label: 'slave',  containers: [\n    containerTemplate(\n            name: 'jnlp',\n            image: 'dongamp1990/jenkins-slave-docker-glibc-jdk8-git-maven',\n            args: '${computer.jnlpmac} ${computer.name}',\n            command: ''\n    )\n  ]\n  ,volumes: [\n        persistentVolumeClaim(mountPath: '/home/jenkins', claimName: 'cipvc01', readOnly: false),\n        persistentVolumeClaim(mountPath: '/root/.m2/repository', claimName: 'mvnpvc', readOnly: false),\n        //hostPathVolume(hostPath: '/jenkins-blueocean-data', mountPath: '/home/jenkins'),\n        hostPathVolume(hostPath: '/var/run/docker.sock', mountPath: '/var/run/docker.sock'),\n        hostPathVolume(hostPath: '/tmp/', mountPath: '/tmp/'),\n]) \n{    node ('slave') {\n        container('jnlp') {\n            stage('colne code') {\n                try{\n                    sh 'git clone --local https://github.com/dongamp1990/demo.git'\n                }catch(e){\n                    dir('demo/demo'){\n                        sh 'git reset --hard && git pull'\n                    }\n                }\n                dir('demo/demo') {\n                    sh 'mvn -X clean install'\n                    sh 'ls -l ./target'\n                }\n            }\n            stage('dockerinfo') {\n                sh 'pwd' \n            }    \n        }\n    }\n}\n```\n\n保存后，点立即构建，查看Console Output，不出意外，应该会有maven的编译日志和ls -l ./target的日志输出","source":"_posts/Kubernetes-deploy-Jenkins-slave.md","raw":"---\ntitle: Kubernetes部署Jenkins并动态资源分配\ntags: [kubernetes, jenkins]\ndate: 2018-03-11\n---\n基于Kubernetes部署Jenkins，JenkinsSlave动态分配\n\n## 部署nfs服务\n需要把jenkins的home目录做持久化，解决方案用nfs\n  ①用外部nfs服务\n  ②在docker里面部署一个nfs服务\n  我们使用第二种方案\n这里还可以把编排到某个node，可以参考<br>https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n\n### 部署nfs\nnfs所在机器ip：192.168.10.93\n```\n$ docker run -d -p 2049:2049 \\\n   --name nfs \\\n   --privileged \\ \n   -e SHARED_DIRECTORY=/nfsshare \\\n   -v /opt/nfsshare:/nfsshare \\\n   --restart=always \\\n   itsthenetwork/nfs-server-alpine:7\n```\n\n### 查看nfs服务启动情况\n```\n$ docker logs nfs\n```\n```\n...\nStarting NFS in the background...\nrpc.nfsd: knfsd is currently down\nrpc.nfsd: Writing version string to kernel: -2 -3 +4 \nrpc.nfsd: Created AF_INET TCP socket.\nrpc.nfsd: Created AF_INET6 TCP socket.\nExporting File System...\nexporting *:/nfsshare\nStarting Mountd in the background...\n```\n\n### 测试nfs服务是否正常\n```\n#创建一个test目录\n$ mkdir -p /opt/nfsshare/test\n\n#把nfs根目录挂载到/mnt/下\n$ mount -t nfs 192.168.10.93:/ /mnt/\n\n#查看/mnt/下是否有test目录, 如果有，表示正常\n$ ls /mnt/\n\ntest\n```\n\n## 使用PersistentVolume持久化卷\n参考：https://jimmysong.io/kubernetes-handbook/concepts/persistent-volume.html\n### jenkins slave 数据持久化卷\njenkins slave 数据持久化卷\ncipvc.yaml\n```\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: cipv01\n  namespace: ci\nspec:\n  capacity:\n    storage: 2Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Recycle\n  storageClassName: slow\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    path: /jenkins\n    #这里填nfs服务器ip（前面nfs的地址）\n    server: 192.168.10.93\n    \n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: cipvc01\n  namespace: ci\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: slow\n```\n\n部署持久化卷\n```      \nkubelet apply -f cipvc.yaml\n```\n\n### mavan本地仓库久化卷\nmvnpvc.yaml\n```\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: mvnpv\n  namespace: ci\nspec:\n  capacity:\n    storage: 2Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Recycle\n  storageClassName: slow\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    path: /mavenLocalRepo\n    server: 192.168.10.93\n\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: mvnpvc\n  namespace: ci\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: slow\n```\n部署持久化卷\n```\nkubelet apply -f mvnpvc.yaml\n```\n\n### 查看pcv\n```\n$ kubectl get pvc -n ci\n\nNAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                 STORAGECLASS   REASON    AGE\ncipv01            2Gi        RWX            Recycle          Bound     ci/cipvc01            slow                     16h\nmvnpv             2Gi        RWX            Recycle          Bound     ci/mvnpvc             slow                     16h\n\n状态是Bound才可用\n```\n\n## 部署jenkins\njenkins.yaml\n```\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ci\n  \n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins\n  namespace: ci\n  \n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: jenkins\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: jenkins\n  namespace: ci\n  \n---\nkind: Deployment\napiVersion: apps/v1beta2\nmetadata:\n  name: jenkins\n  namespace: ci\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jenkins\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: jenkins\n    spec:\n      serviceAccount: jenkins\n      securityContext:\n       runAsUser: 0\n      containers:\n      - name: jenkins\n        image: jenkinsci/blueocean:1.3.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: web\n          protocol: TCP\n        env:\n        - name: PORT\n          value: '8000'\n        - containerPort: 50000\n          name: agent\n          protocol: TCP\n        volumeMounts:\n        - name: jenkinshome\n          mountPath: /var/jenkins_home/\n        env:\n        - name: JAVA_OPTS\n          value: \"-Duser.timezone=Asia/Shanghai\"\n      volumes:\n      - name: jenkinshome\n      \t#pvc模式\n        persistentVolumeClaim:\n          claimName: cipvc01\n        #主机目录模式\n        #hostPath:\n         # directory location on host\n         #path: /opt/jenkins-blueocean-data\n         # this field is optional\n         #type: DirectoryOrCreate\n\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: jenkine-data-pv\n  namespace: ci\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Recycle\n  storageClassName: slow\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    path: /jenkins-blueocean-data\n    server: 192.168.10.93\n    \n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: jenkine-data-pvc\n  namespace: ci\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: slow\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    app: jenkins\n  name: jenkins\n  namespace: ci\nspec:\n  ports:\n  - port: 8090\n    targetPort: 8080\n    name: web\n  - port: 50000\n    targetPort: 50000\n    name: agent\n  selector:\n    app: jenkins\n  type: LoadBalancer\n```\n\n部署jenkins\n```\n$ kubectl apply -f jenkins.yaml\n```\n\n查看svc\n```\n$ kubectl get svc -n ci  \nNAME      TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE\njenkins   LoadBalancer   10.101.147.82   <pending>     8090:31830/TCP,50000:31870/TCP   8h\n```\n\n## 配置jenkins\n访问master/node:31830 端口，可用访问到jenkins\n配置好jenkins后，打开系统管理>插件管理>可选插件\n安装Kubernetes plugin\n安装好后，系统管理>系统设置>新增一个Kubernetes\n\n配置一下几项即可\n```\nName           kubernetes\nKubernetes URL https://kubernetes.default.svc.cluster.local\n#用svc的域名访问。端口是jenkins service的端口\nJenkins URL    http://jenkins.ci.svc.cluster.local:8090\n```\n\n## 测试\n\n接下来新建一个测试pipeline job\n\n填入以下代码：\n```\npodTemplate(label: 'slave',  containers: [\n    containerTemplate(\n            name: 'jnlp',\n            image: 'dongamp1990/jenkins-slave-docker-glibc-jdk8-git-maven',\n            args: '${computer.jnlpmac} ${computer.name}',\n            command: ''\n    )\n  ]\n  ,volumes: [\n        persistentVolumeClaim(mountPath: '/home/jenkins', claimName: 'cipvc01', readOnly: false),\n        persistentVolumeClaim(mountPath: '/root/.m2/repository', claimName: 'mvnpvc', readOnly: false),\n        //hostPathVolume(hostPath: '/jenkins-blueocean-data', mountPath: '/home/jenkins'),\n        hostPathVolume(hostPath: '/var/run/docker.sock', mountPath: '/var/run/docker.sock'),\n        hostPathVolume(hostPath: '/tmp/', mountPath: '/tmp/'),\n]) \n{    node ('slave') {\n        container('jnlp') {\n            stage('colne code') {\n                try{\n                    sh 'git clone --local https://github.com/dongamp1990/demo.git'\n                }catch(e){\n                    dir('demo/demo'){\n                        sh 'git reset --hard && git pull'\n                    }\n                }\n                dir('demo/demo') {\n                    sh 'mvn -X clean install'\n                    sh 'ls -l ./target'\n                }\n            }\n            stage('dockerinfo') {\n                sh 'pwd' \n            }    \n        }\n    }\n}\n```\n\n保存后，点立即构建，查看Console Output，不出意外，应该会有maven的编译日志和ls -l ./target的日志输出","slug":"Kubernetes-deploy-Jenkins-slave","published":1,"updated":"2018-05-23T02:11:18.191Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjncsum4l00017gvrnhgjrbql","content":"<p>基于Kubernetes部署Jenkins，JenkinsSlave动态分配</p>\n<h2 id=\"部署nfs服务\"><a href=\"#部署nfs服务\" class=\"headerlink\" title=\"部署nfs服务\"></a>部署nfs服务</h2><p>需要把jenkins的home目录做持久化，解决方案用nfs<br>  ①用外部nfs服务<br>  ②在docker里面部署一个nfs服务<br>  我们使用第二种方案<br>这里还可以把编排到某个node，可以参考<br><a href=\"https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</a></p>\n<h3 id=\"部署nfs\"><a href=\"#部署nfs\" class=\"headerlink\" title=\"部署nfs\"></a>部署nfs</h3><p>nfs所在机器ip：192.168.10.93<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d -p 2049:2049 \\</span><br><span class=\"line\">   --name nfs \\</span><br><span class=\"line\">   --privileged \\ </span><br><span class=\"line\">   -e SHARED_DIRECTORY=/nfsshare \\</span><br><span class=\"line\">   -v /opt/nfsshare:/nfsshare \\</span><br><span class=\"line\">   --restart=always \\</span><br><span class=\"line\">   itsthenetwork/nfs-server-alpine:7</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"查看nfs服务启动情况\"><a href=\"#查看nfs服务启动情况\" class=\"headerlink\" title=\"查看nfs服务启动情况\"></a>查看nfs服务启动情况</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker logs nfs</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">Starting NFS in the background...</span><br><span class=\"line\">rpc.nfsd: knfsd is currently down</span><br><span class=\"line\">rpc.nfsd: Writing version string to kernel: -2 -3 +4 </span><br><span class=\"line\">rpc.nfsd: Created AF_INET TCP socket.</span><br><span class=\"line\">rpc.nfsd: Created AF_INET6 TCP socket.</span><br><span class=\"line\">Exporting File System...</span><br><span class=\"line\">exporting *:/nfsshare</span><br><span class=\"line\">Starting Mountd in the background...</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试nfs服务是否正常\"><a href=\"#测试nfs服务是否正常\" class=\"headerlink\" title=\"测试nfs服务是否正常\"></a>测试nfs服务是否正常</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#创建一个test目录</span><br><span class=\"line\">$ mkdir -p /opt/nfsshare/test</span><br><span class=\"line\"></span><br><span class=\"line\">#把nfs根目录挂载到/mnt/下</span><br><span class=\"line\">$ mount -t nfs 192.168.10.93:/ /mnt/</span><br><span class=\"line\"></span><br><span class=\"line\">#查看/mnt/下是否有test目录, 如果有，表示正常</span><br><span class=\"line\">$ ls /mnt/</span><br><span class=\"line\"></span><br><span class=\"line\">test</span><br></pre></td></tr></table></figure>\n<h2 id=\"使用PersistentVolume持久化卷\"><a href=\"#使用PersistentVolume持久化卷\" class=\"headerlink\" title=\"使用PersistentVolume持久化卷\"></a>使用PersistentVolume持久化卷</h2><p>参考：<a href=\"https://jimmysong.io/kubernetes-handbook/concepts/persistent-volume.html\" target=\"_blank\" rel=\"noopener\">https://jimmysong.io/kubernetes-handbook/concepts/persistent-volume.html</a></p>\n<h3 id=\"jenkins-slave-数据持久化卷\"><a href=\"#jenkins-slave-数据持久化卷\" class=\"headerlink\" title=\"jenkins slave 数据持久化卷\"></a>jenkins slave 数据持久化卷</h3><p>jenkins slave 数据持久化卷<br>cipvc.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: PersistentVolume</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: cipv01</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  capacity:</span><br><span class=\"line\">    storage: 2Gi</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  persistentVolumeReclaimPolicy: Recycle</span><br><span class=\"line\">  storageClassName: slow</span><br><span class=\"line\">  mountOptions:</span><br><span class=\"line\">    - hard</span><br><span class=\"line\">    - nfsvers=4.1</span><br><span class=\"line\">  nfs:</span><br><span class=\"line\">    path: /jenkins</span><br><span class=\"line\">    #这里填nfs服务器ip（前面nfs的地址）</span><br><span class=\"line\">    server: 192.168.10.93</span><br><span class=\"line\">    </span><br><span class=\"line\">---</span><br><span class=\"line\">kind: PersistentVolumeClaim</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: cipvc01</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  resources:</span><br><span class=\"line\">    requests:</span><br><span class=\"line\">      storage: 2Gi</span><br><span class=\"line\">  storageClassName: slow</span><br></pre></td></tr></table></figure></p>\n<p>部署持久化卷<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubelet apply -f cipvc.yaml</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"mavan本地仓库久化卷\"><a href=\"#mavan本地仓库久化卷\" class=\"headerlink\" title=\"mavan本地仓库久化卷\"></a>mavan本地仓库久化卷</h3><p>mvnpvc.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: PersistentVolume</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: mvnpv</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  capacity:</span><br><span class=\"line\">    storage: 2Gi</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  persistentVolumeReclaimPolicy: Recycle</span><br><span class=\"line\">  storageClassName: slow</span><br><span class=\"line\">  mountOptions:</span><br><span class=\"line\">    - hard</span><br><span class=\"line\">    - nfsvers=4.1</span><br><span class=\"line\">  nfs:</span><br><span class=\"line\">    path: /mavenLocalRepo</span><br><span class=\"line\">    server: 192.168.10.93</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">kind: PersistentVolumeClaim</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: mvnpvc</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  resources:</span><br><span class=\"line\">    requests:</span><br><span class=\"line\">      storage: 2Gi</span><br><span class=\"line\">  storageClassName: slow</span><br></pre></td></tr></table></figure></p>\n<p>部署持久化卷<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubelet apply -f mvnpvc.yaml</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"查看pcv\"><a href=\"#查看pcv\" class=\"headerlink\" title=\"查看pcv\"></a>查看pcv</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pvc -n ci</span><br><span class=\"line\"></span><br><span class=\"line\">NAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                 STORAGECLASS   REASON    AGE</span><br><span class=\"line\">cipv01            2Gi        RWX            Recycle          Bound     ci/cipvc01            slow                     16h</span><br><span class=\"line\">mvnpv             2Gi        RWX            Recycle          Bound     ci/mvnpvc             slow                     16h</span><br><span class=\"line\"></span><br><span class=\"line\">状态是Bound才可用</span><br></pre></td></tr></table></figure>\n<h2 id=\"部署jenkins\"><a href=\"#部署jenkins\" class=\"headerlink\" title=\"部署jenkins\"></a>部署jenkins</h2><p>jenkins.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Namespace</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: ci</span><br><span class=\"line\">  </span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: jenkins</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">  </span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: jenkins</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: cluster-admin</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: jenkins</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">  </span><br><span class=\"line\">---</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">apiVersion: apps/v1beta2</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: jenkins</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      app: jenkins</span><br><span class=\"line\">  strategy:</span><br><span class=\"line\">    type: RollingUpdate</span><br><span class=\"line\">    rollingUpdate:</span><br><span class=\"line\">      maxSurge: 2</span><br><span class=\"line\">      maxUnavailable: 0</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        app: jenkins</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      serviceAccount: jenkins</span><br><span class=\"line\">      securityContext:</span><br><span class=\"line\">       runAsUser: 0</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: jenkins</span><br><span class=\"line\">        image: jenkinsci/blueocean:1.3.5</span><br><span class=\"line\">        imagePullPolicy: IfNotPresent</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">        - containerPort: 8080</span><br><span class=\"line\">          name: web</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        env:</span><br><span class=\"line\">        - name: PORT</span><br><span class=\"line\">          value: &apos;8000&apos;</span><br><span class=\"line\">        - containerPort: 50000</span><br><span class=\"line\">          name: agent</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - name: jenkinshome</span><br><span class=\"line\">          mountPath: /var/jenkins_home/</span><br><span class=\"line\">        env:</span><br><span class=\"line\">        - name: JAVA_OPTS</span><br><span class=\"line\">          value: &quot;-Duser.timezone=Asia/Shanghai&quot;</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">      - name: jenkinshome</span><br><span class=\"line\">      \t#pvc模式</span><br><span class=\"line\">        persistentVolumeClaim:</span><br><span class=\"line\">          claimName: cipvc01</span><br><span class=\"line\">        #主机目录模式</span><br><span class=\"line\">        #hostPath:</span><br><span class=\"line\">         # directory location on host</span><br><span class=\"line\">         #path: /opt/jenkins-blueocean-data</span><br><span class=\"line\">         # this field is optional</span><br><span class=\"line\">         #type: DirectoryOrCreate</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: PersistentVolume</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: jenkine-data-pv</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  capacity:</span><br><span class=\"line\">    storage: 5Gi</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  persistentVolumeReclaimPolicy: Recycle</span><br><span class=\"line\">  storageClassName: slow</span><br><span class=\"line\">  mountOptions:</span><br><span class=\"line\">    - hard</span><br><span class=\"line\">    - nfsvers=4.1</span><br><span class=\"line\">  nfs:</span><br><span class=\"line\">    path: /jenkins-blueocean-data</span><br><span class=\"line\">    server: 192.168.10.93</span><br><span class=\"line\">    </span><br><span class=\"line\">---</span><br><span class=\"line\">kind: PersistentVolumeClaim</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: jenkine-data-pvc</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  resources:</span><br><span class=\"line\">    requests:</span><br><span class=\"line\">      storage: 5Gi</span><br><span class=\"line\">  storageClassName: slow</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    app: jenkins</span><br><span class=\"line\">  name: jenkins</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">  - port: 8090</span><br><span class=\"line\">    targetPort: 8080</span><br><span class=\"line\">    name: web</span><br><span class=\"line\">  - port: 50000</span><br><span class=\"line\">    targetPort: 50000</span><br><span class=\"line\">    name: agent</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    app: jenkins</span><br><span class=\"line\">  type: LoadBalancer</span><br></pre></td></tr></table></figure></p>\n<p>部署jenkins<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f jenkins.yaml</span><br></pre></td></tr></table></figure></p>\n<p>查看svc<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get svc -n ci  </span><br><span class=\"line\">NAME      TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE</span><br><span class=\"line\">jenkins   LoadBalancer   10.101.147.82   &lt;pending&gt;     8090:31830/TCP,50000:31870/TCP   8h</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"配置jenkins\"><a href=\"#配置jenkins\" class=\"headerlink\" title=\"配置jenkins\"></a>配置jenkins</h2><p>访问master/node:31830 端口，可用访问到jenkins<br>配置好jenkins后，打开系统管理&gt;插件管理&gt;可选插件<br>安装Kubernetes plugin<br>安装好后，系统管理&gt;系统设置&gt;新增一个Kubernetes</p>\n<p>配置一下几项即可<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Name           kubernetes</span><br><span class=\"line\">Kubernetes URL https://kubernetes.default.svc.cluster.local</span><br><span class=\"line\">#用svc的域名访问。端口是jenkins service的端口</span><br><span class=\"line\">Jenkins URL    http://jenkins.ci.svc.cluster.local:8090</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>接下来新建一个测试pipeline job</p>\n<p>填入以下代码：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">podTemplate(label: &apos;slave&apos;,  containers: [</span><br><span class=\"line\">    containerTemplate(</span><br><span class=\"line\">            name: &apos;jnlp&apos;,</span><br><span class=\"line\">            image: &apos;dongamp1990/jenkins-slave-docker-glibc-jdk8-git-maven&apos;,</span><br><span class=\"line\">            args: &apos;$&#123;computer.jnlpmac&#125; $&#123;computer.name&#125;&apos;,</span><br><span class=\"line\">            command: &apos;&apos;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  ]</span><br><span class=\"line\">  ,volumes: [</span><br><span class=\"line\">        persistentVolumeClaim(mountPath: &apos;/home/jenkins&apos;, claimName: &apos;cipvc01&apos;, readOnly: false),</span><br><span class=\"line\">        persistentVolumeClaim(mountPath: &apos;/root/.m2/repository&apos;, claimName: &apos;mvnpvc&apos;, readOnly: false),</span><br><span class=\"line\">        //hostPathVolume(hostPath: &apos;/jenkins-blueocean-data&apos;, mountPath: &apos;/home/jenkins&apos;),</span><br><span class=\"line\">        hostPathVolume(hostPath: &apos;/var/run/docker.sock&apos;, mountPath: &apos;/var/run/docker.sock&apos;),</span><br><span class=\"line\">        hostPathVolume(hostPath: &apos;/tmp/&apos;, mountPath: &apos;/tmp/&apos;),</span><br><span class=\"line\">]) </span><br><span class=\"line\">&#123;    node (&apos;slave&apos;) &#123;</span><br><span class=\"line\">        container(&apos;jnlp&apos;) &#123;</span><br><span class=\"line\">            stage(&apos;colne code&apos;) &#123;</span><br><span class=\"line\">                try&#123;</span><br><span class=\"line\">                    sh &apos;git clone --local https://github.com/dongamp1990/demo.git&apos;</span><br><span class=\"line\">                &#125;catch(e)&#123;</span><br><span class=\"line\">                    dir(&apos;demo/demo&apos;)&#123;</span><br><span class=\"line\">                        sh &apos;git reset --hard &amp;&amp; git pull&apos;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                dir(&apos;demo/demo&apos;) &#123;</span><br><span class=\"line\">                    sh &apos;mvn -X clean install&apos;</span><br><span class=\"line\">                    sh &apos;ls -l ./target&apos;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            stage(&apos;dockerinfo&apos;) &#123;</span><br><span class=\"line\">                sh &apos;pwd&apos; </span><br><span class=\"line\">            &#125;    </span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>保存后，点立即构建，查看Console Output，不出意外，应该会有maven的编译日志和ls -l ./target的日志输出</p>\n","site":{"data":{}},"excerpt":"","more":"<p>基于Kubernetes部署Jenkins，JenkinsSlave动态分配</p>\n<h2 id=\"部署nfs服务\"><a href=\"#部署nfs服务\" class=\"headerlink\" title=\"部署nfs服务\"></a>部署nfs服务</h2><p>需要把jenkins的home目录做持久化，解决方案用nfs<br>  ①用外部nfs服务<br>  ②在docker里面部署一个nfs服务<br>  我们使用第二种方案<br>这里还可以把编排到某个node，可以参考<br><a href=\"https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</a></p>\n<h3 id=\"部署nfs\"><a href=\"#部署nfs\" class=\"headerlink\" title=\"部署nfs\"></a>部署nfs</h3><p>nfs所在机器ip：192.168.10.93<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d -p 2049:2049 \\</span><br><span class=\"line\">   --name nfs \\</span><br><span class=\"line\">   --privileged \\ </span><br><span class=\"line\">   -e SHARED_DIRECTORY=/nfsshare \\</span><br><span class=\"line\">   -v /opt/nfsshare:/nfsshare \\</span><br><span class=\"line\">   --restart=always \\</span><br><span class=\"line\">   itsthenetwork/nfs-server-alpine:7</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"查看nfs服务启动情况\"><a href=\"#查看nfs服务启动情况\" class=\"headerlink\" title=\"查看nfs服务启动情况\"></a>查看nfs服务启动情况</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker logs nfs</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">Starting NFS in the background...</span><br><span class=\"line\">rpc.nfsd: knfsd is currently down</span><br><span class=\"line\">rpc.nfsd: Writing version string to kernel: -2 -3 +4 </span><br><span class=\"line\">rpc.nfsd: Created AF_INET TCP socket.</span><br><span class=\"line\">rpc.nfsd: Created AF_INET6 TCP socket.</span><br><span class=\"line\">Exporting File System...</span><br><span class=\"line\">exporting *:/nfsshare</span><br><span class=\"line\">Starting Mountd in the background...</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试nfs服务是否正常\"><a href=\"#测试nfs服务是否正常\" class=\"headerlink\" title=\"测试nfs服务是否正常\"></a>测试nfs服务是否正常</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#创建一个test目录</span><br><span class=\"line\">$ mkdir -p /opt/nfsshare/test</span><br><span class=\"line\"></span><br><span class=\"line\">#把nfs根目录挂载到/mnt/下</span><br><span class=\"line\">$ mount -t nfs 192.168.10.93:/ /mnt/</span><br><span class=\"line\"></span><br><span class=\"line\">#查看/mnt/下是否有test目录, 如果有，表示正常</span><br><span class=\"line\">$ ls /mnt/</span><br><span class=\"line\"></span><br><span class=\"line\">test</span><br></pre></td></tr></table></figure>\n<h2 id=\"使用PersistentVolume持久化卷\"><a href=\"#使用PersistentVolume持久化卷\" class=\"headerlink\" title=\"使用PersistentVolume持久化卷\"></a>使用PersistentVolume持久化卷</h2><p>参考：<a href=\"https://jimmysong.io/kubernetes-handbook/concepts/persistent-volume.html\" target=\"_blank\" rel=\"noopener\">https://jimmysong.io/kubernetes-handbook/concepts/persistent-volume.html</a></p>\n<h3 id=\"jenkins-slave-数据持久化卷\"><a href=\"#jenkins-slave-数据持久化卷\" class=\"headerlink\" title=\"jenkins slave 数据持久化卷\"></a>jenkins slave 数据持久化卷</h3><p>jenkins slave 数据持久化卷<br>cipvc.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: PersistentVolume</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: cipv01</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  capacity:</span><br><span class=\"line\">    storage: 2Gi</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  persistentVolumeReclaimPolicy: Recycle</span><br><span class=\"line\">  storageClassName: slow</span><br><span class=\"line\">  mountOptions:</span><br><span class=\"line\">    - hard</span><br><span class=\"line\">    - nfsvers=4.1</span><br><span class=\"line\">  nfs:</span><br><span class=\"line\">    path: /jenkins</span><br><span class=\"line\">    #这里填nfs服务器ip（前面nfs的地址）</span><br><span class=\"line\">    server: 192.168.10.93</span><br><span class=\"line\">    </span><br><span class=\"line\">---</span><br><span class=\"line\">kind: PersistentVolumeClaim</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: cipvc01</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  resources:</span><br><span class=\"line\">    requests:</span><br><span class=\"line\">      storage: 2Gi</span><br><span class=\"line\">  storageClassName: slow</span><br></pre></td></tr></table></figure></p>\n<p>部署持久化卷<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubelet apply -f cipvc.yaml</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"mavan本地仓库久化卷\"><a href=\"#mavan本地仓库久化卷\" class=\"headerlink\" title=\"mavan本地仓库久化卷\"></a>mavan本地仓库久化卷</h3><p>mvnpvc.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: PersistentVolume</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: mvnpv</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  capacity:</span><br><span class=\"line\">    storage: 2Gi</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  persistentVolumeReclaimPolicy: Recycle</span><br><span class=\"line\">  storageClassName: slow</span><br><span class=\"line\">  mountOptions:</span><br><span class=\"line\">    - hard</span><br><span class=\"line\">    - nfsvers=4.1</span><br><span class=\"line\">  nfs:</span><br><span class=\"line\">    path: /mavenLocalRepo</span><br><span class=\"line\">    server: 192.168.10.93</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">kind: PersistentVolumeClaim</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: mvnpvc</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  resources:</span><br><span class=\"line\">    requests:</span><br><span class=\"line\">      storage: 2Gi</span><br><span class=\"line\">  storageClassName: slow</span><br></pre></td></tr></table></figure></p>\n<p>部署持久化卷<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubelet apply -f mvnpvc.yaml</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"查看pcv\"><a href=\"#查看pcv\" class=\"headerlink\" title=\"查看pcv\"></a>查看pcv</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pvc -n ci</span><br><span class=\"line\"></span><br><span class=\"line\">NAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                 STORAGECLASS   REASON    AGE</span><br><span class=\"line\">cipv01            2Gi        RWX            Recycle          Bound     ci/cipvc01            slow                     16h</span><br><span class=\"line\">mvnpv             2Gi        RWX            Recycle          Bound     ci/mvnpvc             slow                     16h</span><br><span class=\"line\"></span><br><span class=\"line\">状态是Bound才可用</span><br></pre></td></tr></table></figure>\n<h2 id=\"部署jenkins\"><a href=\"#部署jenkins\" class=\"headerlink\" title=\"部署jenkins\"></a>部署jenkins</h2><p>jenkins.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Namespace</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: ci</span><br><span class=\"line\">  </span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: jenkins</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">  </span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: jenkins</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: cluster-admin</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: jenkins</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">  </span><br><span class=\"line\">---</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">apiVersion: apps/v1beta2</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: jenkins</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      app: jenkins</span><br><span class=\"line\">  strategy:</span><br><span class=\"line\">    type: RollingUpdate</span><br><span class=\"line\">    rollingUpdate:</span><br><span class=\"line\">      maxSurge: 2</span><br><span class=\"line\">      maxUnavailable: 0</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        app: jenkins</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      serviceAccount: jenkins</span><br><span class=\"line\">      securityContext:</span><br><span class=\"line\">       runAsUser: 0</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: jenkins</span><br><span class=\"line\">        image: jenkinsci/blueocean:1.3.5</span><br><span class=\"line\">        imagePullPolicy: IfNotPresent</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">        - containerPort: 8080</span><br><span class=\"line\">          name: web</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        env:</span><br><span class=\"line\">        - name: PORT</span><br><span class=\"line\">          value: &apos;8000&apos;</span><br><span class=\"line\">        - containerPort: 50000</span><br><span class=\"line\">          name: agent</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - name: jenkinshome</span><br><span class=\"line\">          mountPath: /var/jenkins_home/</span><br><span class=\"line\">        env:</span><br><span class=\"line\">        - name: JAVA_OPTS</span><br><span class=\"line\">          value: &quot;-Duser.timezone=Asia/Shanghai&quot;</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">      - name: jenkinshome</span><br><span class=\"line\">      \t#pvc模式</span><br><span class=\"line\">        persistentVolumeClaim:</span><br><span class=\"line\">          claimName: cipvc01</span><br><span class=\"line\">        #主机目录模式</span><br><span class=\"line\">        #hostPath:</span><br><span class=\"line\">         # directory location on host</span><br><span class=\"line\">         #path: /opt/jenkins-blueocean-data</span><br><span class=\"line\">         # this field is optional</span><br><span class=\"line\">         #type: DirectoryOrCreate</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: PersistentVolume</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: jenkine-data-pv</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  capacity:</span><br><span class=\"line\">    storage: 5Gi</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  persistentVolumeReclaimPolicy: Recycle</span><br><span class=\"line\">  storageClassName: slow</span><br><span class=\"line\">  mountOptions:</span><br><span class=\"line\">    - hard</span><br><span class=\"line\">    - nfsvers=4.1</span><br><span class=\"line\">  nfs:</span><br><span class=\"line\">    path: /jenkins-blueocean-data</span><br><span class=\"line\">    server: 192.168.10.93</span><br><span class=\"line\">    </span><br><span class=\"line\">---</span><br><span class=\"line\">kind: PersistentVolumeClaim</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: jenkine-data-pvc</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  accessModes:</span><br><span class=\"line\">    - ReadWriteMany</span><br><span class=\"line\">  resources:</span><br><span class=\"line\">    requests:</span><br><span class=\"line\">      storage: 5Gi</span><br><span class=\"line\">  storageClassName: slow</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    app: jenkins</span><br><span class=\"line\">  name: jenkins</span><br><span class=\"line\">  namespace: ci</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">  - port: 8090</span><br><span class=\"line\">    targetPort: 8080</span><br><span class=\"line\">    name: web</span><br><span class=\"line\">  - port: 50000</span><br><span class=\"line\">    targetPort: 50000</span><br><span class=\"line\">    name: agent</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    app: jenkins</span><br><span class=\"line\">  type: LoadBalancer</span><br></pre></td></tr></table></figure></p>\n<p>部署jenkins<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f jenkins.yaml</span><br></pre></td></tr></table></figure></p>\n<p>查看svc<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get svc -n ci  </span><br><span class=\"line\">NAME      TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE</span><br><span class=\"line\">jenkins   LoadBalancer   10.101.147.82   &lt;pending&gt;     8090:31830/TCP,50000:31870/TCP   8h</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"配置jenkins\"><a href=\"#配置jenkins\" class=\"headerlink\" title=\"配置jenkins\"></a>配置jenkins</h2><p>访问master/node:31830 端口，可用访问到jenkins<br>配置好jenkins后，打开系统管理&gt;插件管理&gt;可选插件<br>安装Kubernetes plugin<br>安装好后，系统管理&gt;系统设置&gt;新增一个Kubernetes</p>\n<p>配置一下几项即可<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Name           kubernetes</span><br><span class=\"line\">Kubernetes URL https://kubernetes.default.svc.cluster.local</span><br><span class=\"line\">#用svc的域名访问。端口是jenkins service的端口</span><br><span class=\"line\">Jenkins URL    http://jenkins.ci.svc.cluster.local:8090</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><p>接下来新建一个测试pipeline job</p>\n<p>填入以下代码：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">podTemplate(label: &apos;slave&apos;,  containers: [</span><br><span class=\"line\">    containerTemplate(</span><br><span class=\"line\">            name: &apos;jnlp&apos;,</span><br><span class=\"line\">            image: &apos;dongamp1990/jenkins-slave-docker-glibc-jdk8-git-maven&apos;,</span><br><span class=\"line\">            args: &apos;$&#123;computer.jnlpmac&#125; $&#123;computer.name&#125;&apos;,</span><br><span class=\"line\">            command: &apos;&apos;</span><br><span class=\"line\">    )</span><br><span class=\"line\">  ]</span><br><span class=\"line\">  ,volumes: [</span><br><span class=\"line\">        persistentVolumeClaim(mountPath: &apos;/home/jenkins&apos;, claimName: &apos;cipvc01&apos;, readOnly: false),</span><br><span class=\"line\">        persistentVolumeClaim(mountPath: &apos;/root/.m2/repository&apos;, claimName: &apos;mvnpvc&apos;, readOnly: false),</span><br><span class=\"line\">        //hostPathVolume(hostPath: &apos;/jenkins-blueocean-data&apos;, mountPath: &apos;/home/jenkins&apos;),</span><br><span class=\"line\">        hostPathVolume(hostPath: &apos;/var/run/docker.sock&apos;, mountPath: &apos;/var/run/docker.sock&apos;),</span><br><span class=\"line\">        hostPathVolume(hostPath: &apos;/tmp/&apos;, mountPath: &apos;/tmp/&apos;),</span><br><span class=\"line\">]) </span><br><span class=\"line\">&#123;    node (&apos;slave&apos;) &#123;</span><br><span class=\"line\">        container(&apos;jnlp&apos;) &#123;</span><br><span class=\"line\">            stage(&apos;colne code&apos;) &#123;</span><br><span class=\"line\">                try&#123;</span><br><span class=\"line\">                    sh &apos;git clone --local https://github.com/dongamp1990/demo.git&apos;</span><br><span class=\"line\">                &#125;catch(e)&#123;</span><br><span class=\"line\">                    dir(&apos;demo/demo&apos;)&#123;</span><br><span class=\"line\">                        sh &apos;git reset --hard &amp;&amp; git pull&apos;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                dir(&apos;demo/demo&apos;) &#123;</span><br><span class=\"line\">                    sh &apos;mvn -X clean install&apos;</span><br><span class=\"line\">                    sh &apos;ls -l ./target&apos;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            stage(&apos;dockerinfo&apos;) &#123;</span><br><span class=\"line\">                sh &apos;pwd&apos; </span><br><span class=\"line\">            &#125;    </span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>保存后，点立即构建，查看Console Output，不出意外，应该会有maven的编译日志和ls -l ./target的日志输出</p>\n"},{"title":"安装Docker日志","date":"2018-01-10T16:00:00.000Z","_content":"安装Docker日志\n## 安装Docker\n\n### 系统要求\n查看官方文档 https://docs.docker.com/engine/installation/\n文内使用店是centos7\n\n\n### 删除旧的版本\n```\n$ sudo yum remove docker \\\n docker-common \\\n docker-selinux \\\n docker-engine\n```\n\n### 设置Docker存储库\n安装必要的一些系统工具\n```\n$ sudo yum install -y yum-utils \\\n\tdevice-mapper-persistent-data \\\n\tlvm2\n```\n\n添加软件源信息\n```\n$ sudo yum-config-manager \\\n --add-repo \\\n https://download.docker.com/linux/centos/docker-ce.repo\n```\n \n### 安装docker-ce\n安装docker稳定版\n```\n$ sudo yum install docker-ce-stable\n```\n显示可用的版本\n\n```\n$ yum list docker-ce --showduplicates | sort -r\ndocker-ce.x86_64   17.03.0.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.03.1.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.03.2.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.06.0.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.06.1.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.06.2.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.09.0.ce-1.el7.centos   docker-ce-stable\n```\n\n指定安装版本\n```\n$ yum install -y docker-ce-17.06.2.ce\n```\n\n### 增加阿里云docker镜像，加快镜像pull速度\n```\n#如果没有该文件可以创建一个\n$ vi /etc/docker/daemon.json \n{\n \"registry-mirrors\": [\"https://zln0jqua.mirror.aliyuncs.com\"]\n}\n```\n\n### 启动docker\n```\n$ sudo systemctl start docker\n```\n\n### 运行hello-word验证安装是否正确\n```\n$ sudo docker run hello-world\n```\n\n### 如果启动不了，可以查看日志文件\n```\n$ cat /var/log/upstart/docker.log\n```\n\n### 更多内容可查阅官方的安装文档\nhttps://docs.docker.com/engine/installation/linux/docker-ce/centos/\n\n\n## 安装问题汇总\n安装后启动不了，查看详细日志\n```\n$ ail /var/log/upstart/docker.log\n```\n\n### 启动docker报no available network。\n<font color=red>Error starting daemon: Error initializing network controller: list bridge addresses failed: no available network。</font><br>\n手工添加bridge虚拟网络即可解决\n```\n$ sudo ip link add name docker0 type bridge\n$ sudo ip addr add dev docker0 172.17.0.1/16\n```\n\n### 集群部署，woker节点报错\n<font color=red>starting container failed: error creating external connectivity network: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network</font><br>\n可能是docker0的桥接网络网段冲突了。\n```\n查看网络ip\n$ ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 00:16:3e:00:01:f0 brd ff:ff:ff:ff:ff:ff\n    inet 10.170.122.155/21 brd 10.170.127.255 scope global eth0\n       valid_lft forever preferred_lft forever\n3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 00:16:3e:00:0e:c8 brd ff:ff:ff:ff:ff:ff\n    inet 120.25.105.218/22 brd 120.25.107.255 scope global eth1\n       valid_lft forever preferred_lft forever\n57: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default \n    link/ether 66:f6:dc:19:a1:96 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.1/20 scope global docker0\n       valid_lft forever preferred_lft forever\n\n\n#查看路由表\n$ ip route \ndefault via 120.25.107.247 dev eth1 \n10.0.0.0/8 via 10.170.127.247 dev eth0 \n10.170.120.0/21 dev eth0  proto kernel  scope link  src 10.170.122.155 \n11.0.0.0/8 via 10.170.127.247 dev eth0 \n100.64.0.0/10 via 10.170.127.247 dev eth0 \n120.25.104.0/22 dev eth1  proto kernel  scope link  src 120.25.105.218 \n172.16.0.0/12 via 10.170.127.247 dev eth0 \n192.168.0.0/20 dev docker0  proto kernel  scope link  src 192.168.0.1 \n\n#把路由表的192.168.0.0/20删掉，解决问题。\n\n```\n\n### 配置了内存限制，容器启动报task: non-zero exit (137)\n查看日志/var/log/upstart/docker.log 发现<br>\n<font color=red>Your kernel does not support swap limit capabilities,or the cgroup is not mounted. Memory limited without swap.</font><br>\n\n```\n#修改/etc/default/grub 增加\nGRUB_CMDLINE_LINUX=\"cgroup_enable=memory swapaccount=1\"\n保存\n#更新GRUB\n$ sudo update-grub\n#重启系统\n$ reboot\n```\n\n## 构建docker镜像\n构建一个带了jdk的ubuntu镜像\n\n```\n$ vi Dockerfile\n#基础镜像名称\nFROM ubuntu\n#用来指定镜像创建者信息\nMAINTAINER dong “dongamp1990@gmail.com”\n#工作目录 \nWORKDIR /opt/\n#复制在Dockerfile目录下 复制jdk1.8.0_101.tar.gz 到 /opt目录下, 如果是docker可以识别的压缩包，目的目录不写/会自\t动解压\nADD jdk1.8.0_101.tar.gz /opt\n#执行命令\nRUN mv /opt/jdk1.8.0_101 /opt/jdk8\n#设置环境\nENV JAVA_HOME /opt/jdk8 \nENV PATH $PATH:$JAVA_HOME/bin\n```\n\n打包镜像\n```\n# docker build [OPTIONS] PATH | URL |\n$ docker build -t ubuntu-jdk8 .\n```\n\n查看镜像\n```\n$ docker images\n```\n\n运行镜像\n```\n$ docker run -it ubuntu-jdk8 javac\n```\n\n## Docker使用\n\n### 查看当前镜像\n```\n$ docker images \n```\n\n### 拉取镜像\n```\n$ docker pull ubuntu #拉取latest tag的镜像\n$ docker pull ubuntu:13.10 #拉取指定tag的镜像\n```\n\n### 运行容器\n```\n$ docker run -d -v /opt/eureka/logs/:/opt/logs/ --name eureka -p 8000:8761 springcloud/eureka\n -d 后台运行 \n -v 文件挂载 把容器内的/opt/logs映射到/opt/eureka/logs目录下\n --name 指定容器名称，如果不知道docker会随机生成应用名\n-p 端口映射 8000是外部端口 8761是容器内的端口\n```\n\n### 查看当前运行的容器\n```\n$ docker ps\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS  PORTS  NAMES\n容器ID         镜像      命令        创建时间   状态    端口   容器名称\t\n\ndocker ps -a 查看所有容器\n```\n\n### 停止容器\n```\n$ docker stop CONTAINERID\n```\n\n### 启动容器\n```\n$ docker start CONTAINERID\n```\n\n### 删除镜像 \n```\n$ docker rmi -f imagename[:TAG]\n```\n\n### 容器资源使用统计信息\n官方文档地址：https://docs.docker.com/engine/reference/commandline/stats/\n```\n$ docker stats\n\nCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS\n221f321b1324        0.12%               467.3MiB / 3.858GiB   11.83%              1.08MB / 1.16MB     754kB / 0B          30\nfac3348f831b        0.11%               322.7MiB / 512MiB     63.02%              467MB / 497MB       61.4kB / 0B         52\nd960f8146f1d        1.29%               328.7MiB / 512MiB     64.19%              15.5GB / 18.3GB     20.5kB / 0B         66\nf1441134f6d4        0.87%               367.3MiB / 512MiB     71.73%              5.31GB / 3.81GB     610kB / 0B          71\n9007528c7621        0.00%               6.395MiB / 3.858GiB   0.16%               11.2MB / 51.8MB     5.41MB / 254kB      7\n\n\n使用--format自定义显示数据内容\n\n$ docker stats --format \"table {{.Name}}\\t{{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.NetIO}}\\t{{.BlockIO}}\"\n\nNAME                                              CONTAINER           CPU %               MEM USAGE / LIMIT     NET I/O             BLOCK I/O\nbrave_kare                                        221f321b1324        0.11%               467.3MiB / 3.858GiB   1.08MB / 1.16MB     754kB / 0B\n1_maxfunPayNewAdmin.1.639izogqf896v2jd4qmyl9c77   fac3348f831b        0.12%               322.7MiB / 512MiB     467MB / 497MB       61.4kB / 0B\n1_maxfunGatewayZuul.1.0gn8x5yd229bjkj35ejjnyi7o   d960f8146f1d        1.41%               328.7MiB / 512MiB     15.5GB / 18.3GB     20.5kB / 0B\n1_maxfunEureka.1.0q690ptcca0cxy7div3o1z92p        f1441134f6d4        0.92%               367.3MiB / 512MiB     5.31GB / 3.81GB     610kB / 0B\nportainer                                         9007528c7621        0.00%               6.395MiB / 3.858GiB   11.2MB / 51.8MB     5.41MB / 254kB\n```\n\n## Docker私服Registry搭建\n\n### 拉取registry v2镜像\n```\n$ docker pull registry:2\n```\n\n### 启动仓库\n```\n$ docker run -d -p 5000:5000 --restart=always --name registry \\ \n  -v /opt/registry/:/var/lib/registry \\ \n  -v /opt/registry/:/tmp/docker-registry.db registry:2\n\n# -v 把registry容器内的/var/lib/registry目录，挂载到/opt/registry目录下，\n  可以把镜像内容也挂载存储到磁盘上，防止registry停止后丢失数据\n\n```\n\n### 标记镜像\n```\n$ docker tag registry:2 192.168.42.132:5000/registry:v2\n```\n\n### 推送镜像到仓库\n```\n$ docker push 192.168.42.132:5000/registry:v2\n```\n如果出现<br>\n<font color=\"red\">Error response from daemon: Get https://192.168.42.132:5000/v2/users/: http: server gave HTTP response to HTTPS client</font><br>\n在/etc/docker/daemon.json里面增加以下配置\n```\n\"insecure-registries\":[\"192.168.42.132:5000\"]\t\n```\n\n### 重启 docker\n```\n$ systemctl restart docker\n```\n\n### 重新推送tag到仓库\n```\n$ docker push 192.168.42.132:5000/registry:v2\n```\n### 查看仓库\n```\n$ curl http://192.168.42.132:5000/v2/_catalog \n# 如果没错，会返回如下内容。\n{\"repositories\":[\"registry\"]}\n```","source":"_posts/docker-install-note.md","raw":"---\ntitle: 安装Docker日志\ntags: [docker]\ndate: 2018-01-11\n---\n安装Docker日志\n## 安装Docker\n\n### 系统要求\n查看官方文档 https://docs.docker.com/engine/installation/\n文内使用店是centos7\n\n\n### 删除旧的版本\n```\n$ sudo yum remove docker \\\n docker-common \\\n docker-selinux \\\n docker-engine\n```\n\n### 设置Docker存储库\n安装必要的一些系统工具\n```\n$ sudo yum install -y yum-utils \\\n\tdevice-mapper-persistent-data \\\n\tlvm2\n```\n\n添加软件源信息\n```\n$ sudo yum-config-manager \\\n --add-repo \\\n https://download.docker.com/linux/centos/docker-ce.repo\n```\n \n### 安装docker-ce\n安装docker稳定版\n```\n$ sudo yum install docker-ce-stable\n```\n显示可用的版本\n\n```\n$ yum list docker-ce --showduplicates | sort -r\ndocker-ce.x86_64   17.03.0.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.03.1.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.03.2.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.06.0.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.06.1.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.06.2.ce-1.el7.centos   docker-ce-stable\ndocker-ce.x86_64   17.09.0.ce-1.el7.centos   docker-ce-stable\n```\n\n指定安装版本\n```\n$ yum install -y docker-ce-17.06.2.ce\n```\n\n### 增加阿里云docker镜像，加快镜像pull速度\n```\n#如果没有该文件可以创建一个\n$ vi /etc/docker/daemon.json \n{\n \"registry-mirrors\": [\"https://zln0jqua.mirror.aliyuncs.com\"]\n}\n```\n\n### 启动docker\n```\n$ sudo systemctl start docker\n```\n\n### 运行hello-word验证安装是否正确\n```\n$ sudo docker run hello-world\n```\n\n### 如果启动不了，可以查看日志文件\n```\n$ cat /var/log/upstart/docker.log\n```\n\n### 更多内容可查阅官方的安装文档\nhttps://docs.docker.com/engine/installation/linux/docker-ce/centos/\n\n\n## 安装问题汇总\n安装后启动不了，查看详细日志\n```\n$ ail /var/log/upstart/docker.log\n```\n\n### 启动docker报no available network。\n<font color=red>Error starting daemon: Error initializing network controller: list bridge addresses failed: no available network。</font><br>\n手工添加bridge虚拟网络即可解决\n```\n$ sudo ip link add name docker0 type bridge\n$ sudo ip addr add dev docker0 172.17.0.1/16\n```\n\n### 集群部署，woker节点报错\n<font color=red>starting container failed: error creating external connectivity network: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network</font><br>\n可能是docker0的桥接网络网段冲突了。\n```\n查看网络ip\n$ ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 00:16:3e:00:01:f0 brd ff:ff:ff:ff:ff:ff\n    inet 10.170.122.155/21 brd 10.170.127.255 scope global eth0\n       valid_lft forever preferred_lft forever\n3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 00:16:3e:00:0e:c8 brd ff:ff:ff:ff:ff:ff\n    inet 120.25.105.218/22 brd 120.25.107.255 scope global eth1\n       valid_lft forever preferred_lft forever\n57: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default \n    link/ether 66:f6:dc:19:a1:96 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.1/20 scope global docker0\n       valid_lft forever preferred_lft forever\n\n\n#查看路由表\n$ ip route \ndefault via 120.25.107.247 dev eth1 \n10.0.0.0/8 via 10.170.127.247 dev eth0 \n10.170.120.0/21 dev eth0  proto kernel  scope link  src 10.170.122.155 \n11.0.0.0/8 via 10.170.127.247 dev eth0 \n100.64.0.0/10 via 10.170.127.247 dev eth0 \n120.25.104.0/22 dev eth1  proto kernel  scope link  src 120.25.105.218 \n172.16.0.0/12 via 10.170.127.247 dev eth0 \n192.168.0.0/20 dev docker0  proto kernel  scope link  src 192.168.0.1 \n\n#把路由表的192.168.0.0/20删掉，解决问题。\n\n```\n\n### 配置了内存限制，容器启动报task: non-zero exit (137)\n查看日志/var/log/upstart/docker.log 发现<br>\n<font color=red>Your kernel does not support swap limit capabilities,or the cgroup is not mounted. Memory limited without swap.</font><br>\n\n```\n#修改/etc/default/grub 增加\nGRUB_CMDLINE_LINUX=\"cgroup_enable=memory swapaccount=1\"\n保存\n#更新GRUB\n$ sudo update-grub\n#重启系统\n$ reboot\n```\n\n## 构建docker镜像\n构建一个带了jdk的ubuntu镜像\n\n```\n$ vi Dockerfile\n#基础镜像名称\nFROM ubuntu\n#用来指定镜像创建者信息\nMAINTAINER dong “dongamp1990@gmail.com”\n#工作目录 \nWORKDIR /opt/\n#复制在Dockerfile目录下 复制jdk1.8.0_101.tar.gz 到 /opt目录下, 如果是docker可以识别的压缩包，目的目录不写/会自\t动解压\nADD jdk1.8.0_101.tar.gz /opt\n#执行命令\nRUN mv /opt/jdk1.8.0_101 /opt/jdk8\n#设置环境\nENV JAVA_HOME /opt/jdk8 \nENV PATH $PATH:$JAVA_HOME/bin\n```\n\n打包镜像\n```\n# docker build [OPTIONS] PATH | URL |\n$ docker build -t ubuntu-jdk8 .\n```\n\n查看镜像\n```\n$ docker images\n```\n\n运行镜像\n```\n$ docker run -it ubuntu-jdk8 javac\n```\n\n## Docker使用\n\n### 查看当前镜像\n```\n$ docker images \n```\n\n### 拉取镜像\n```\n$ docker pull ubuntu #拉取latest tag的镜像\n$ docker pull ubuntu:13.10 #拉取指定tag的镜像\n```\n\n### 运行容器\n```\n$ docker run -d -v /opt/eureka/logs/:/opt/logs/ --name eureka -p 8000:8761 springcloud/eureka\n -d 后台运行 \n -v 文件挂载 把容器内的/opt/logs映射到/opt/eureka/logs目录下\n --name 指定容器名称，如果不知道docker会随机生成应用名\n-p 端口映射 8000是外部端口 8761是容器内的端口\n```\n\n### 查看当前运行的容器\n```\n$ docker ps\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS  PORTS  NAMES\n容器ID         镜像      命令        创建时间   状态    端口   容器名称\t\n\ndocker ps -a 查看所有容器\n```\n\n### 停止容器\n```\n$ docker stop CONTAINERID\n```\n\n### 启动容器\n```\n$ docker start CONTAINERID\n```\n\n### 删除镜像 \n```\n$ docker rmi -f imagename[:TAG]\n```\n\n### 容器资源使用统计信息\n官方文档地址：https://docs.docker.com/engine/reference/commandline/stats/\n```\n$ docker stats\n\nCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS\n221f321b1324        0.12%               467.3MiB / 3.858GiB   11.83%              1.08MB / 1.16MB     754kB / 0B          30\nfac3348f831b        0.11%               322.7MiB / 512MiB     63.02%              467MB / 497MB       61.4kB / 0B         52\nd960f8146f1d        1.29%               328.7MiB / 512MiB     64.19%              15.5GB / 18.3GB     20.5kB / 0B         66\nf1441134f6d4        0.87%               367.3MiB / 512MiB     71.73%              5.31GB / 3.81GB     610kB / 0B          71\n9007528c7621        0.00%               6.395MiB / 3.858GiB   0.16%               11.2MB / 51.8MB     5.41MB / 254kB      7\n\n\n使用--format自定义显示数据内容\n\n$ docker stats --format \"table {{.Name}}\\t{{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.NetIO}}\\t{{.BlockIO}}\"\n\nNAME                                              CONTAINER           CPU %               MEM USAGE / LIMIT     NET I/O             BLOCK I/O\nbrave_kare                                        221f321b1324        0.11%               467.3MiB / 3.858GiB   1.08MB / 1.16MB     754kB / 0B\n1_maxfunPayNewAdmin.1.639izogqf896v2jd4qmyl9c77   fac3348f831b        0.12%               322.7MiB / 512MiB     467MB / 497MB       61.4kB / 0B\n1_maxfunGatewayZuul.1.0gn8x5yd229bjkj35ejjnyi7o   d960f8146f1d        1.41%               328.7MiB / 512MiB     15.5GB / 18.3GB     20.5kB / 0B\n1_maxfunEureka.1.0q690ptcca0cxy7div3o1z92p        f1441134f6d4        0.92%               367.3MiB / 512MiB     5.31GB / 3.81GB     610kB / 0B\nportainer                                         9007528c7621        0.00%               6.395MiB / 3.858GiB   11.2MB / 51.8MB     5.41MB / 254kB\n```\n\n## Docker私服Registry搭建\n\n### 拉取registry v2镜像\n```\n$ docker pull registry:2\n```\n\n### 启动仓库\n```\n$ docker run -d -p 5000:5000 --restart=always --name registry \\ \n  -v /opt/registry/:/var/lib/registry \\ \n  -v /opt/registry/:/tmp/docker-registry.db registry:2\n\n# -v 把registry容器内的/var/lib/registry目录，挂载到/opt/registry目录下，\n  可以把镜像内容也挂载存储到磁盘上，防止registry停止后丢失数据\n\n```\n\n### 标记镜像\n```\n$ docker tag registry:2 192.168.42.132:5000/registry:v2\n```\n\n### 推送镜像到仓库\n```\n$ docker push 192.168.42.132:5000/registry:v2\n```\n如果出现<br>\n<font color=\"red\">Error response from daemon: Get https://192.168.42.132:5000/v2/users/: http: server gave HTTP response to HTTPS client</font><br>\n在/etc/docker/daemon.json里面增加以下配置\n```\n\"insecure-registries\":[\"192.168.42.132:5000\"]\t\n```\n\n### 重启 docker\n```\n$ systemctl restart docker\n```\n\n### 重新推送tag到仓库\n```\n$ docker push 192.168.42.132:5000/registry:v2\n```\n### 查看仓库\n```\n$ curl http://192.168.42.132:5000/v2/_catalog \n# 如果没错，会返回如下内容。\n{\"repositories\":[\"registry\"]}\n```","slug":"docker-install-note","published":1,"updated":"2018-05-23T02:11:43.749Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjncsum4p00037gvrhovnm5c1","content":"<p>安装Docker日志</p>\n<h2 id=\"安装Docker\"><a href=\"#安装Docker\" class=\"headerlink\" title=\"安装Docker\"></a>安装Docker</h2><h3 id=\"系统要求\"><a href=\"#系统要求\" class=\"headerlink\" title=\"系统要求\"></a>系统要求</h3><p>查看官方文档 <a href=\"https://docs.docker.com/engine/installation/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/installation/</a><br>文内使用店是centos7</p>\n<h3 id=\"删除旧的版本\"><a href=\"#删除旧的版本\" class=\"headerlink\" title=\"删除旧的版本\"></a>删除旧的版本</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo yum remove docker \\</span><br><span class=\"line\"> docker-common \\</span><br><span class=\"line\"> docker-selinux \\</span><br><span class=\"line\"> docker-engine</span><br></pre></td></tr></table></figure>\n<h3 id=\"设置Docker存储库\"><a href=\"#设置Docker存储库\" class=\"headerlink\" title=\"设置Docker存储库\"></a>设置Docker存储库</h3><p>安装必要的一些系统工具<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo yum install -y yum-utils \\</span><br><span class=\"line\">\tdevice-mapper-persistent-data \\</span><br><span class=\"line\">\tlvm2</span><br></pre></td></tr></table></figure></p>\n<p>添加软件源信息<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo yum-config-manager \\</span><br><span class=\"line\"> --add-repo \\</span><br><span class=\"line\"> https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"安装docker-ce\"><a href=\"#安装docker-ce\" class=\"headerlink\" title=\"安装docker-ce\"></a>安装docker-ce</h3><p>安装docker稳定版<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo yum install docker-ce-stable</span><br></pre></td></tr></table></figure></p>\n<p>显示可用的版本</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ yum list docker-ce --showduplicates | sort -r</span><br><span class=\"line\">docker-ce.x86_64   17.03.0.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.03.1.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.03.2.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.06.0.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.06.1.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.06.2.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.09.0.ce-1.el7.centos   docker-ce-stable</span><br></pre></td></tr></table></figure>\n<p>指定安装版本<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ yum install -y docker-ce-17.06.2.ce</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"增加阿里云docker镜像，加快镜像pull速度\"><a href=\"#增加阿里云docker镜像，加快镜像pull速度\" class=\"headerlink\" title=\"增加阿里云docker镜像，加快镜像pull速度\"></a>增加阿里云docker镜像，加快镜像pull速度</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#如果没有该文件可以创建一个</span><br><span class=\"line\">$ vi /etc/docker/daemon.json </span><br><span class=\"line\">&#123;</span><br><span class=\"line\"> &quot;registry-mirrors&quot;: [&quot;https://zln0jqua.mirror.aliyuncs.com&quot;]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动docker\"><a href=\"#启动docker\" class=\"headerlink\" title=\"启动docker\"></a>启动docker</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo systemctl start docker</span><br></pre></td></tr></table></figure>\n<h3 id=\"运行hello-word验证安装是否正确\"><a href=\"#运行hello-word验证安装是否正确\" class=\"headerlink\" title=\"运行hello-word验证安装是否正确\"></a>运行hello-word验证安装是否正确</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run hello-world</span><br></pre></td></tr></table></figure>\n<h3 id=\"如果启动不了，可以查看日志文件\"><a href=\"#如果启动不了，可以查看日志文件\" class=\"headerlink\" title=\"如果启动不了，可以查看日志文件\"></a>如果启动不了，可以查看日志文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat /var/log/upstart/docker.log</span><br></pre></td></tr></table></figure>\n<h3 id=\"更多内容可查阅官方的安装文档\"><a href=\"#更多内容可查阅官方的安装文档\" class=\"headerlink\" title=\"更多内容可查阅官方的安装文档\"></a>更多内容可查阅官方的安装文档</h3><p><a href=\"https://docs.docker.com/engine/installation/linux/docker-ce/centos/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/installation/linux/docker-ce/centos/</a></p>\n<h2 id=\"安装问题汇总\"><a href=\"#安装问题汇总\" class=\"headerlink\" title=\"安装问题汇总\"></a>安装问题汇总</h2><p>安装后启动不了，查看详细日志<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ail /var/log/upstart/docker.log</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"启动docker报no-available-network。\"><a href=\"#启动docker报no-available-network。\" class=\"headerlink\" title=\"启动docker报no available network。\"></a>启动docker报no available network。</h3><p><font color=\"red\">Error starting daemon: Error initializing network controller: list bridge addresses failed: no available network。</font><br><br>手工添加bridge虚拟网络即可解决<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo ip link add name docker0 type bridge</span><br><span class=\"line\">$ sudo ip addr add dev docker0 172.17.0.1/16</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"集群部署，woker节点报错\"><a href=\"#集群部署，woker节点报错\" class=\"headerlink\" title=\"集群部署，woker节点报错\"></a>集群部署，woker节点报错</h3><p><font color=\"red\">starting container failed: error creating external connectivity network: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network</font><br><br>可能是docker0的桥接网络网段冲突了。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">查看网络ip</span><br><span class=\"line\">$ ip addr</span><br><span class=\"line\">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default </span><br><span class=\"line\">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class=\"line\">    inet 127.0.0.1/8 scope host lo</span><br><span class=\"line\">       valid_lft forever preferred_lft forever</span><br><span class=\"line\">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class=\"line\">    link/ether 00:16:3e:00:01:f0 brd ff:ff:ff:ff:ff:ff</span><br><span class=\"line\">    inet 10.170.122.155/21 brd 10.170.127.255 scope global eth0</span><br><span class=\"line\">       valid_lft forever preferred_lft forever</span><br><span class=\"line\">3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class=\"line\">    link/ether 00:16:3e:00:0e:c8 brd ff:ff:ff:ff:ff:ff</span><br><span class=\"line\">    inet 120.25.105.218/22 brd 120.25.107.255 scope global eth1</span><br><span class=\"line\">       valid_lft forever preferred_lft forever</span><br><span class=\"line\">57: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default </span><br><span class=\"line\">    link/ether 66:f6:dc:19:a1:96 brd ff:ff:ff:ff:ff:ff</span><br><span class=\"line\">    inet 192.168.0.1/20 scope global docker0</span><br><span class=\"line\">       valid_lft forever preferred_lft forever</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">#查看路由表</span><br><span class=\"line\">$ ip route </span><br><span class=\"line\">default via 120.25.107.247 dev eth1 </span><br><span class=\"line\">10.0.0.0/8 via 10.170.127.247 dev eth0 </span><br><span class=\"line\">10.170.120.0/21 dev eth0  proto kernel  scope link  src 10.170.122.155 </span><br><span class=\"line\">11.0.0.0/8 via 10.170.127.247 dev eth0 </span><br><span class=\"line\">100.64.0.0/10 via 10.170.127.247 dev eth0 </span><br><span class=\"line\">120.25.104.0/22 dev eth1  proto kernel  scope link  src 120.25.105.218 </span><br><span class=\"line\">172.16.0.0/12 via 10.170.127.247 dev eth0 </span><br><span class=\"line\">192.168.0.0/20 dev docker0  proto kernel  scope link  src 192.168.0.1 </span><br><span class=\"line\"></span><br><span class=\"line\">#把路由表的192.168.0.0/20删掉，解决问题。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"配置了内存限制，容器启动报task-non-zero-exit-137\"><a href=\"#配置了内存限制，容器启动报task-non-zero-exit-137\" class=\"headerlink\" title=\"配置了内存限制，容器启动报task: non-zero exit (137)\"></a>配置了内存限制，容器启动报task: non-zero exit (137)</h3><p>查看日志/var/log/upstart/docker.log 发现<br></p>\n<p><font color=\"red\">Your kernel does not support swap limit capabilities,or the cgroup is not mounted. Memory limited without swap.</font><br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#修改/etc/default/grub 增加</span><br><span class=\"line\">GRUB_CMDLINE_LINUX=&quot;cgroup_enable=memory swapaccount=1&quot;</span><br><span class=\"line\">保存</span><br><span class=\"line\">#更新GRUB</span><br><span class=\"line\">$ sudo update-grub</span><br><span class=\"line\">#重启系统</span><br><span class=\"line\">$ reboot</span><br></pre></td></tr></table></figure>\n<h2 id=\"构建docker镜像\"><a href=\"#构建docker镜像\" class=\"headerlink\" title=\"构建docker镜像\"></a>构建docker镜像</h2><p>构建一个带了jdk的ubuntu镜像</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vi Dockerfile</span><br><span class=\"line\">#基础镜像名称</span><br><span class=\"line\">FROM ubuntu</span><br><span class=\"line\">#用来指定镜像创建者信息</span><br><span class=\"line\">MAINTAINER dong “dongamp1990@gmail.com”</span><br><span class=\"line\">#工作目录 </span><br><span class=\"line\">WORKDIR /opt/</span><br><span class=\"line\">#复制在Dockerfile目录下 复制jdk1.8.0_101.tar.gz 到 /opt目录下, 如果是docker可以识别的压缩包，目的目录不写/会自\t动解压</span><br><span class=\"line\">ADD jdk1.8.0_101.tar.gz /opt</span><br><span class=\"line\">#执行命令</span><br><span class=\"line\">RUN mv /opt/jdk1.8.0_101 /opt/jdk8</span><br><span class=\"line\">#设置环境</span><br><span class=\"line\">ENV JAVA_HOME /opt/jdk8 </span><br><span class=\"line\">ENV PATH $PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>\n<p>打包镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker build [OPTIONS] PATH | URL |</span><br><span class=\"line\">$ docker build -t ubuntu-jdk8 .</span><br></pre></td></tr></table></figure></p>\n<p>查看镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker images</span><br></pre></td></tr></table></figure></p>\n<p>运行镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -it ubuntu-jdk8 javac</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Docker使用\"><a href=\"#Docker使用\" class=\"headerlink\" title=\"Docker使用\"></a>Docker使用</h2><h3 id=\"查看当前镜像\"><a href=\"#查看当前镜像\" class=\"headerlink\" title=\"查看当前镜像\"></a>查看当前镜像</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker images</span><br></pre></td></tr></table></figure>\n<h3 id=\"拉取镜像\"><a href=\"#拉取镜像\" class=\"headerlink\" title=\"拉取镜像\"></a>拉取镜像</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull ubuntu #拉取latest tag的镜像</span><br><span class=\"line\">$ docker pull ubuntu:13.10 #拉取指定tag的镜像</span><br></pre></td></tr></table></figure>\n<h3 id=\"运行容器\"><a href=\"#运行容器\" class=\"headerlink\" title=\"运行容器\"></a>运行容器</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d -v /opt/eureka/logs/:/opt/logs/ --name eureka -p 8000:8761 springcloud/eureka</span><br><span class=\"line\"> -d 后台运行 </span><br><span class=\"line\"> -v 文件挂载 把容器内的/opt/logs映射到/opt/eureka/logs目录下</span><br><span class=\"line\"> --name 指定容器名称，如果不知道docker会随机生成应用名</span><br><span class=\"line\">-p 端口映射 8000是外部端口 8761是容器内的端口</span><br></pre></td></tr></table></figure>\n<h3 id=\"查看当前运行的容器\"><a href=\"#查看当前运行的容器\" class=\"headerlink\" title=\"查看当前运行的容器\"></a>查看当前运行的容器</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker ps</span><br><span class=\"line\">CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS  PORTS  NAMES</span><br><span class=\"line\">容器ID         镜像      命令        创建时间   状态    端口   容器名称\t</span><br><span class=\"line\"></span><br><span class=\"line\">docker ps -a 查看所有容器</span><br></pre></td></tr></table></figure>\n<h3 id=\"停止容器\"><a href=\"#停止容器\" class=\"headerlink\" title=\"停止容器\"></a>停止容器</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker stop CONTAINERID</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动容器\"><a href=\"#启动容器\" class=\"headerlink\" title=\"启动容器\"></a>启动容器</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker start CONTAINERID</span><br></pre></td></tr></table></figure>\n<h3 id=\"删除镜像\"><a href=\"#删除镜像\" class=\"headerlink\" title=\"删除镜像\"></a>删除镜像</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker rmi -f imagename[:TAG]</span><br></pre></td></tr></table></figure>\n<h3 id=\"容器资源使用统计信息\"><a href=\"#容器资源使用统计信息\" class=\"headerlink\" title=\"容器资源使用统计信息\"></a>容器资源使用统计信息</h3><p>官方文档地址：<a href=\"https://docs.docker.com/engine/reference/commandline/stats/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/reference/commandline/stats/</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker stats</span><br><span class=\"line\"></span><br><span class=\"line\">CONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS</span><br><span class=\"line\">221f321b1324        0.12%               467.3MiB / 3.858GiB   11.83%              1.08MB / 1.16MB     754kB / 0B          30</span><br><span class=\"line\">fac3348f831b        0.11%               322.7MiB / 512MiB     63.02%              467MB / 497MB       61.4kB / 0B         52</span><br><span class=\"line\">d960f8146f1d        1.29%               328.7MiB / 512MiB     64.19%              15.5GB / 18.3GB     20.5kB / 0B         66</span><br><span class=\"line\">f1441134f6d4        0.87%               367.3MiB / 512MiB     71.73%              5.31GB / 3.81GB     610kB / 0B          71</span><br><span class=\"line\">9007528c7621        0.00%               6.395MiB / 3.858GiB   0.16%               11.2MB / 51.8MB     5.41MB / 254kB      7</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">使用--format自定义显示数据内容</span><br><span class=\"line\"></span><br><span class=\"line\">$ docker stats --format &quot;table &#123;&#123;.Name&#125;&#125;\\t&#123;&#123;.Container&#125;&#125;\\t&#123;&#123;.CPUPerc&#125;&#125;\\t&#123;&#123;.MemUsage&#125;&#125;\\t&#123;&#123;.NetIO&#125;&#125;\\t&#123;&#123;.BlockIO&#125;&#125;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                              CONTAINER           CPU %               MEM USAGE / LIMIT     NET I/O             BLOCK I/O</span><br><span class=\"line\">brave_kare                                        221f321b1324        0.11%               467.3MiB / 3.858GiB   1.08MB / 1.16MB     754kB / 0B</span><br><span class=\"line\">1_maxfunPayNewAdmin.1.639izogqf896v2jd4qmyl9c77   fac3348f831b        0.12%               322.7MiB / 512MiB     467MB / 497MB       61.4kB / 0B</span><br><span class=\"line\">1_maxfunGatewayZuul.1.0gn8x5yd229bjkj35ejjnyi7o   d960f8146f1d        1.41%               328.7MiB / 512MiB     15.5GB / 18.3GB     20.5kB / 0B</span><br><span class=\"line\">1_maxfunEureka.1.0q690ptcca0cxy7div3o1z92p        f1441134f6d4        0.92%               367.3MiB / 512MiB     5.31GB / 3.81GB     610kB / 0B</span><br><span class=\"line\">portainer                                         9007528c7621        0.00%               6.395MiB / 3.858GiB   11.2MB / 51.8MB     5.41MB / 254kB</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Docker私服Registry搭建\"><a href=\"#Docker私服Registry搭建\" class=\"headerlink\" title=\"Docker私服Registry搭建\"></a>Docker私服Registry搭建</h2><h3 id=\"拉取registry-v2镜像\"><a href=\"#拉取registry-v2镜像\" class=\"headerlink\" title=\"拉取registry v2镜像\"></a>拉取registry v2镜像</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull registry:2</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动仓库\"><a href=\"#启动仓库\" class=\"headerlink\" title=\"启动仓库\"></a>启动仓库</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d -p 5000:5000 --restart=always --name registry \\ </span><br><span class=\"line\">  -v /opt/registry/:/var/lib/registry \\ </span><br><span class=\"line\">  -v /opt/registry/:/tmp/docker-registry.db registry:2</span><br><span class=\"line\"></span><br><span class=\"line\"># -v 把registry容器内的/var/lib/registry目录，挂载到/opt/registry目录下，</span><br><span class=\"line\">  可以把镜像内容也挂载存储到磁盘上，防止registry停止后丢失数据</span><br></pre></td></tr></table></figure>\n<h3 id=\"标记镜像\"><a href=\"#标记镜像\" class=\"headerlink\" title=\"标记镜像\"></a>标记镜像</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker tag registry:2 192.168.42.132:5000/registry:v2</span><br></pre></td></tr></table></figure>\n<h3 id=\"推送镜像到仓库\"><a href=\"#推送镜像到仓库\" class=\"headerlink\" title=\"推送镜像到仓库\"></a>推送镜像到仓库</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker push 192.168.42.132:5000/registry:v2</span><br></pre></td></tr></table></figure>\n<p>如果出现<br></p>\n<p><font color=\"red\">Error response from daemon: Get <a href=\"https://192.168.42.132:5000/v2/users/\" target=\"_blank\" rel=\"noopener\">https://192.168.42.132:5000/v2/users/</a>: http: server gave HTTP response to HTTPS client</font><br><br>在/etc/docker/daemon.json里面增加以下配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;insecure-registries&quot;:[&quot;192.168.42.132:5000&quot;]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"重启-docker\"><a href=\"#重启-docker\" class=\"headerlink\" title=\"重启 docker\"></a>重启 docker</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl restart docker</span><br></pre></td></tr></table></figure>\n<h3 id=\"重新推送tag到仓库\"><a href=\"#重新推送tag到仓库\" class=\"headerlink\" title=\"重新推送tag到仓库\"></a>重新推送tag到仓库</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker push 192.168.42.132:5000/registry:v2</span><br></pre></td></tr></table></figure>\n<h3 id=\"查看仓库\"><a href=\"#查看仓库\" class=\"headerlink\" title=\"查看仓库\"></a>查看仓库</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://192.168.42.132:5000/v2/_catalog </span><br><span class=\"line\"># 如果没错，会返回如下内容。</span><br><span class=\"line\">&#123;&quot;repositories&quot;:[&quot;registry&quot;]&#125;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>安装Docker日志</p>\n<h2 id=\"安装Docker\"><a href=\"#安装Docker\" class=\"headerlink\" title=\"安装Docker\"></a>安装Docker</h2><h3 id=\"系统要求\"><a href=\"#系统要求\" class=\"headerlink\" title=\"系统要求\"></a>系统要求</h3><p>查看官方文档 <a href=\"https://docs.docker.com/engine/installation/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/installation/</a><br>文内使用店是centos7</p>\n<h3 id=\"删除旧的版本\"><a href=\"#删除旧的版本\" class=\"headerlink\" title=\"删除旧的版本\"></a>删除旧的版本</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo yum remove docker \\</span><br><span class=\"line\"> docker-common \\</span><br><span class=\"line\"> docker-selinux \\</span><br><span class=\"line\"> docker-engine</span><br></pre></td></tr></table></figure>\n<h3 id=\"设置Docker存储库\"><a href=\"#设置Docker存储库\" class=\"headerlink\" title=\"设置Docker存储库\"></a>设置Docker存储库</h3><p>安装必要的一些系统工具<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo yum install -y yum-utils \\</span><br><span class=\"line\">\tdevice-mapper-persistent-data \\</span><br><span class=\"line\">\tlvm2</span><br></pre></td></tr></table></figure></p>\n<p>添加软件源信息<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo yum-config-manager \\</span><br><span class=\"line\"> --add-repo \\</span><br><span class=\"line\"> https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"安装docker-ce\"><a href=\"#安装docker-ce\" class=\"headerlink\" title=\"安装docker-ce\"></a>安装docker-ce</h3><p>安装docker稳定版<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo yum install docker-ce-stable</span><br></pre></td></tr></table></figure></p>\n<p>显示可用的版本</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ yum list docker-ce --showduplicates | sort -r</span><br><span class=\"line\">docker-ce.x86_64   17.03.0.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.03.1.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.03.2.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.06.0.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.06.1.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.06.2.ce-1.el7.centos   docker-ce-stable</span><br><span class=\"line\">docker-ce.x86_64   17.09.0.ce-1.el7.centos   docker-ce-stable</span><br></pre></td></tr></table></figure>\n<p>指定安装版本<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ yum install -y docker-ce-17.06.2.ce</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"增加阿里云docker镜像，加快镜像pull速度\"><a href=\"#增加阿里云docker镜像，加快镜像pull速度\" class=\"headerlink\" title=\"增加阿里云docker镜像，加快镜像pull速度\"></a>增加阿里云docker镜像，加快镜像pull速度</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#如果没有该文件可以创建一个</span><br><span class=\"line\">$ vi /etc/docker/daemon.json </span><br><span class=\"line\">&#123;</span><br><span class=\"line\"> &quot;registry-mirrors&quot;: [&quot;https://zln0jqua.mirror.aliyuncs.com&quot;]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动docker\"><a href=\"#启动docker\" class=\"headerlink\" title=\"启动docker\"></a>启动docker</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo systemctl start docker</span><br></pre></td></tr></table></figure>\n<h3 id=\"运行hello-word验证安装是否正确\"><a href=\"#运行hello-word验证安装是否正确\" class=\"headerlink\" title=\"运行hello-word验证安装是否正确\"></a>运行hello-word验证安装是否正确</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run hello-world</span><br></pre></td></tr></table></figure>\n<h3 id=\"如果启动不了，可以查看日志文件\"><a href=\"#如果启动不了，可以查看日志文件\" class=\"headerlink\" title=\"如果启动不了，可以查看日志文件\"></a>如果启动不了，可以查看日志文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat /var/log/upstart/docker.log</span><br></pre></td></tr></table></figure>\n<h3 id=\"更多内容可查阅官方的安装文档\"><a href=\"#更多内容可查阅官方的安装文档\" class=\"headerlink\" title=\"更多内容可查阅官方的安装文档\"></a>更多内容可查阅官方的安装文档</h3><p><a href=\"https://docs.docker.com/engine/installation/linux/docker-ce/centos/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/installation/linux/docker-ce/centos/</a></p>\n<h2 id=\"安装问题汇总\"><a href=\"#安装问题汇总\" class=\"headerlink\" title=\"安装问题汇总\"></a>安装问题汇总</h2><p>安装后启动不了，查看详细日志<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ail /var/log/upstart/docker.log</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"启动docker报no-available-network。\"><a href=\"#启动docker报no-available-network。\" class=\"headerlink\" title=\"启动docker报no available network。\"></a>启动docker报no available network。</h3><p><font color=\"red\">Error starting daemon: Error initializing network controller: list bridge addresses failed: no available network。</font><br><br>手工添加bridge虚拟网络即可解决<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo ip link add name docker0 type bridge</span><br><span class=\"line\">$ sudo ip addr add dev docker0 172.17.0.1/16</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"集群部署，woker节点报错\"><a href=\"#集群部署，woker节点报错\" class=\"headerlink\" title=\"集群部署，woker节点报错\"></a>集群部署，woker节点报错</h3><p><font color=\"red\">starting container failed: error creating external connectivity network: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network</font><br><br>可能是docker0的桥接网络网段冲突了。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">查看网络ip</span><br><span class=\"line\">$ ip addr</span><br><span class=\"line\">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default </span><br><span class=\"line\">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class=\"line\">    inet 127.0.0.1/8 scope host lo</span><br><span class=\"line\">       valid_lft forever preferred_lft forever</span><br><span class=\"line\">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class=\"line\">    link/ether 00:16:3e:00:01:f0 brd ff:ff:ff:ff:ff:ff</span><br><span class=\"line\">    inet 10.170.122.155/21 brd 10.170.127.255 scope global eth0</span><br><span class=\"line\">       valid_lft forever preferred_lft forever</span><br><span class=\"line\">3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class=\"line\">    link/ether 00:16:3e:00:0e:c8 brd ff:ff:ff:ff:ff:ff</span><br><span class=\"line\">    inet 120.25.105.218/22 brd 120.25.107.255 scope global eth1</span><br><span class=\"line\">       valid_lft forever preferred_lft forever</span><br><span class=\"line\">57: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default </span><br><span class=\"line\">    link/ether 66:f6:dc:19:a1:96 brd ff:ff:ff:ff:ff:ff</span><br><span class=\"line\">    inet 192.168.0.1/20 scope global docker0</span><br><span class=\"line\">       valid_lft forever preferred_lft forever</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">#查看路由表</span><br><span class=\"line\">$ ip route </span><br><span class=\"line\">default via 120.25.107.247 dev eth1 </span><br><span class=\"line\">10.0.0.0/8 via 10.170.127.247 dev eth0 </span><br><span class=\"line\">10.170.120.0/21 dev eth0  proto kernel  scope link  src 10.170.122.155 </span><br><span class=\"line\">11.0.0.0/8 via 10.170.127.247 dev eth0 </span><br><span class=\"line\">100.64.0.0/10 via 10.170.127.247 dev eth0 </span><br><span class=\"line\">120.25.104.0/22 dev eth1  proto kernel  scope link  src 120.25.105.218 </span><br><span class=\"line\">172.16.0.0/12 via 10.170.127.247 dev eth0 </span><br><span class=\"line\">192.168.0.0/20 dev docker0  proto kernel  scope link  src 192.168.0.1 </span><br><span class=\"line\"></span><br><span class=\"line\">#把路由表的192.168.0.0/20删掉，解决问题。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"配置了内存限制，容器启动报task-non-zero-exit-137\"><a href=\"#配置了内存限制，容器启动报task-non-zero-exit-137\" class=\"headerlink\" title=\"配置了内存限制，容器启动报task: non-zero exit (137)\"></a>配置了内存限制，容器启动报task: non-zero exit (137)</h3><p>查看日志/var/log/upstart/docker.log 发现<br></p>\n<p><font color=\"red\">Your kernel does not support swap limit capabilities,or the cgroup is not mounted. Memory limited without swap.</font><br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#修改/etc/default/grub 增加</span><br><span class=\"line\">GRUB_CMDLINE_LINUX=&quot;cgroup_enable=memory swapaccount=1&quot;</span><br><span class=\"line\">保存</span><br><span class=\"line\">#更新GRUB</span><br><span class=\"line\">$ sudo update-grub</span><br><span class=\"line\">#重启系统</span><br><span class=\"line\">$ reboot</span><br></pre></td></tr></table></figure>\n<h2 id=\"构建docker镜像\"><a href=\"#构建docker镜像\" class=\"headerlink\" title=\"构建docker镜像\"></a>构建docker镜像</h2><p>构建一个带了jdk的ubuntu镜像</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vi Dockerfile</span><br><span class=\"line\">#基础镜像名称</span><br><span class=\"line\">FROM ubuntu</span><br><span class=\"line\">#用来指定镜像创建者信息</span><br><span class=\"line\">MAINTAINER dong “dongamp1990@gmail.com”</span><br><span class=\"line\">#工作目录 </span><br><span class=\"line\">WORKDIR /opt/</span><br><span class=\"line\">#复制在Dockerfile目录下 复制jdk1.8.0_101.tar.gz 到 /opt目录下, 如果是docker可以识别的压缩包，目的目录不写/会自\t动解压</span><br><span class=\"line\">ADD jdk1.8.0_101.tar.gz /opt</span><br><span class=\"line\">#执行命令</span><br><span class=\"line\">RUN mv /opt/jdk1.8.0_101 /opt/jdk8</span><br><span class=\"line\">#设置环境</span><br><span class=\"line\">ENV JAVA_HOME /opt/jdk8 </span><br><span class=\"line\">ENV PATH $PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>\n<p>打包镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker build [OPTIONS] PATH | URL |</span><br><span class=\"line\">$ docker build -t ubuntu-jdk8 .</span><br></pre></td></tr></table></figure></p>\n<p>查看镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker images</span><br></pre></td></tr></table></figure></p>\n<p>运行镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -it ubuntu-jdk8 javac</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Docker使用\"><a href=\"#Docker使用\" class=\"headerlink\" title=\"Docker使用\"></a>Docker使用</h2><h3 id=\"查看当前镜像\"><a href=\"#查看当前镜像\" class=\"headerlink\" title=\"查看当前镜像\"></a>查看当前镜像</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker images</span><br></pre></td></tr></table></figure>\n<h3 id=\"拉取镜像\"><a href=\"#拉取镜像\" class=\"headerlink\" title=\"拉取镜像\"></a>拉取镜像</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull ubuntu #拉取latest tag的镜像</span><br><span class=\"line\">$ docker pull ubuntu:13.10 #拉取指定tag的镜像</span><br></pre></td></tr></table></figure>\n<h3 id=\"运行容器\"><a href=\"#运行容器\" class=\"headerlink\" title=\"运行容器\"></a>运行容器</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d -v /opt/eureka/logs/:/opt/logs/ --name eureka -p 8000:8761 springcloud/eureka</span><br><span class=\"line\"> -d 后台运行 </span><br><span class=\"line\"> -v 文件挂载 把容器内的/opt/logs映射到/opt/eureka/logs目录下</span><br><span class=\"line\"> --name 指定容器名称，如果不知道docker会随机生成应用名</span><br><span class=\"line\">-p 端口映射 8000是外部端口 8761是容器内的端口</span><br></pre></td></tr></table></figure>\n<h3 id=\"查看当前运行的容器\"><a href=\"#查看当前运行的容器\" class=\"headerlink\" title=\"查看当前运行的容器\"></a>查看当前运行的容器</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker ps</span><br><span class=\"line\">CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS  PORTS  NAMES</span><br><span class=\"line\">容器ID         镜像      命令        创建时间   状态    端口   容器名称\t</span><br><span class=\"line\"></span><br><span class=\"line\">docker ps -a 查看所有容器</span><br></pre></td></tr></table></figure>\n<h3 id=\"停止容器\"><a href=\"#停止容器\" class=\"headerlink\" title=\"停止容器\"></a>停止容器</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker stop CONTAINERID</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动容器\"><a href=\"#启动容器\" class=\"headerlink\" title=\"启动容器\"></a>启动容器</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker start CONTAINERID</span><br></pre></td></tr></table></figure>\n<h3 id=\"删除镜像\"><a href=\"#删除镜像\" class=\"headerlink\" title=\"删除镜像\"></a>删除镜像</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker rmi -f imagename[:TAG]</span><br></pre></td></tr></table></figure>\n<h3 id=\"容器资源使用统计信息\"><a href=\"#容器资源使用统计信息\" class=\"headerlink\" title=\"容器资源使用统计信息\"></a>容器资源使用统计信息</h3><p>官方文档地址：<a href=\"https://docs.docker.com/engine/reference/commandline/stats/\" target=\"_blank\" rel=\"noopener\">https://docs.docker.com/engine/reference/commandline/stats/</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker stats</span><br><span class=\"line\"></span><br><span class=\"line\">CONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS</span><br><span class=\"line\">221f321b1324        0.12%               467.3MiB / 3.858GiB   11.83%              1.08MB / 1.16MB     754kB / 0B          30</span><br><span class=\"line\">fac3348f831b        0.11%               322.7MiB / 512MiB     63.02%              467MB / 497MB       61.4kB / 0B         52</span><br><span class=\"line\">d960f8146f1d        1.29%               328.7MiB / 512MiB     64.19%              15.5GB / 18.3GB     20.5kB / 0B         66</span><br><span class=\"line\">f1441134f6d4        0.87%               367.3MiB / 512MiB     71.73%              5.31GB / 3.81GB     610kB / 0B          71</span><br><span class=\"line\">9007528c7621        0.00%               6.395MiB / 3.858GiB   0.16%               11.2MB / 51.8MB     5.41MB / 254kB      7</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">使用--format自定义显示数据内容</span><br><span class=\"line\"></span><br><span class=\"line\">$ docker stats --format &quot;table &#123;&#123;.Name&#125;&#125;\\t&#123;&#123;.Container&#125;&#125;\\t&#123;&#123;.CPUPerc&#125;&#125;\\t&#123;&#123;.MemUsage&#125;&#125;\\t&#123;&#123;.NetIO&#125;&#125;\\t&#123;&#123;.BlockIO&#125;&#125;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                              CONTAINER           CPU %               MEM USAGE / LIMIT     NET I/O             BLOCK I/O</span><br><span class=\"line\">brave_kare                                        221f321b1324        0.11%               467.3MiB / 3.858GiB   1.08MB / 1.16MB     754kB / 0B</span><br><span class=\"line\">1_maxfunPayNewAdmin.1.639izogqf896v2jd4qmyl9c77   fac3348f831b        0.12%               322.7MiB / 512MiB     467MB / 497MB       61.4kB / 0B</span><br><span class=\"line\">1_maxfunGatewayZuul.1.0gn8x5yd229bjkj35ejjnyi7o   d960f8146f1d        1.41%               328.7MiB / 512MiB     15.5GB / 18.3GB     20.5kB / 0B</span><br><span class=\"line\">1_maxfunEureka.1.0q690ptcca0cxy7div3o1z92p        f1441134f6d4        0.92%               367.3MiB / 512MiB     5.31GB / 3.81GB     610kB / 0B</span><br><span class=\"line\">portainer                                         9007528c7621        0.00%               6.395MiB / 3.858GiB   11.2MB / 51.8MB     5.41MB / 254kB</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Docker私服Registry搭建\"><a href=\"#Docker私服Registry搭建\" class=\"headerlink\" title=\"Docker私服Registry搭建\"></a>Docker私服Registry搭建</h2><h3 id=\"拉取registry-v2镜像\"><a href=\"#拉取registry-v2镜像\" class=\"headerlink\" title=\"拉取registry v2镜像\"></a>拉取registry v2镜像</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull registry:2</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动仓库\"><a href=\"#启动仓库\" class=\"headerlink\" title=\"启动仓库\"></a>启动仓库</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d -p 5000:5000 --restart=always --name registry \\ </span><br><span class=\"line\">  -v /opt/registry/:/var/lib/registry \\ </span><br><span class=\"line\">  -v /opt/registry/:/tmp/docker-registry.db registry:2</span><br><span class=\"line\"></span><br><span class=\"line\"># -v 把registry容器内的/var/lib/registry目录，挂载到/opt/registry目录下，</span><br><span class=\"line\">  可以把镜像内容也挂载存储到磁盘上，防止registry停止后丢失数据</span><br></pre></td></tr></table></figure>\n<h3 id=\"标记镜像\"><a href=\"#标记镜像\" class=\"headerlink\" title=\"标记镜像\"></a>标记镜像</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker tag registry:2 192.168.42.132:5000/registry:v2</span><br></pre></td></tr></table></figure>\n<h3 id=\"推送镜像到仓库\"><a href=\"#推送镜像到仓库\" class=\"headerlink\" title=\"推送镜像到仓库\"></a>推送镜像到仓库</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker push 192.168.42.132:5000/registry:v2</span><br></pre></td></tr></table></figure>\n<p>如果出现<br></p>\n<p><font color=\"red\">Error response from daemon: Get <a href=\"https://192.168.42.132:5000/v2/users/\" target=\"_blank\" rel=\"noopener\">https://192.168.42.132:5000/v2/users/</a>: http: server gave HTTP response to HTTPS client</font><br><br>在/etc/docker/daemon.json里面增加以下配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;insecure-registries&quot;:[&quot;192.168.42.132:5000&quot;]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"重启-docker\"><a href=\"#重启-docker\" class=\"headerlink\" title=\"重启 docker\"></a>重启 docker</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl restart docker</span><br></pre></td></tr></table></figure>\n<h3 id=\"重新推送tag到仓库\"><a href=\"#重新推送tag到仓库\" class=\"headerlink\" title=\"重新推送tag到仓库\"></a>重新推送tag到仓库</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker push 192.168.42.132:5000/registry:v2</span><br></pre></td></tr></table></figure>\n<h3 id=\"查看仓库\"><a href=\"#查看仓库\" class=\"headerlink\" title=\"查看仓库\"></a>查看仓库</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://192.168.42.132:5000/v2/_catalog </span><br><span class=\"line\"># 如果没错，会返回如下内容。</span><br><span class=\"line\">&#123;&quot;repositories&quot;:[&quot;registry&quot;]&#125;</span><br></pre></td></tr></table></figure>"},{"title":"Docker Swarm集群使用Traefik","date":"2018-04-01T16:00:00.000Z","_content":"\n## 前言\nTræfɪk 是一个为了让部署微服务更加便捷而诞生的现代HTTP反向代理、负载均衡工具。 它支持多种后台 (Docker, Swarm, Kubernetes, Marathon, Mesos, Consul, Etcd, Zookeeper, BoltDB, Rest API, file…) 来自动化、动态的应用它的配置文件设置。\n官方文档里面介绍了很多场景的使用，详情点击<a href='https://docs.traefik.cn/'>官方文档</a>\n\n## 部署Traefik\n### swarm网络\n\nswarm集群里面使用，需要使用overlay网络, 需先创建一个overlay网络，或者使用已有的overlay网络\n<font color='red'>（不要使用名字为ingress的那个网络，一开始我使用了，怎么弄都不成功，踩过得坑。谨记）</font>\n\n```\n$ docker network create traefik-net --driver overlay\n\n```\n查看网络\n```\n$ docker network ls\n\np86srgdn2zne        traefik-net         overlay             swarm\n```\n\ntraefik.yaml\n```\nversion: \"3\"\nservices:\n  traefik:\n    image: traefik\n    command: --docker --docker.swarmmode  --docker.domain=maxfun.co --docker.watch  --web --api --logLevel=DEBUG\n    ports:\n      - \"80:80\"\n      - \"8080:8080\"\n      - \"443:443\"\n    deploy:\n      placement:\n        constraints:\n        - node.role == manager \n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - $PWD/traefik.toml:/etc/traefik/traefik.toml\n    networks:\n    - traefik-net\nnetworks:\n  traefik-net:\n    external: true\n```\ntreafik配置文件:https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml\n启动traefik服务\n```\n$ docker stack deploy -c traefik.yaml traefik\n```\n或者\n\n```\ndocker service create \\\n    --name traefik \\\n    --constraint=node.role==manager \\\n    --publish 80:80 \\\n    --publish 8080:8080 \\\n    --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\\n    --mount type=bind,source=/opt/traefik/traefik.toml,target=/etc/traefik/traefik.toml \\\n    --network traefik-net \\\n    traefik \\\n    --docker \\\n    --docker.swarmmode \\\n    --docker.domain=maxfun.co \\\n    --docker.watch \\\n    --web \\\n    --api\n```\n\n\n###  部署jenkins应用\n在master节点部署jenkins，不适用端口来访问jenkins\n\nrun-jenkins.sh\n```\nmkdir -p /opt/jenkins-blueocean-data\nMVN_REPO_PATH=/home/coder/developtools/mavenRepository\ndocker service create \\\n  --name jenkins \\\n  --constraint=node.role==manager \\\n  -u root \\\n  -e JAVA_OPTS=-Duser.timezone=GMT+08 \\\n  -e JENKINS_OPTS=--httpPort=8080 \\\n  --publish 8089:8080 \\\n  --mount type=bind,source=/opt/jenkins-blueocean-data,target=/var/jenkins_home \\\n  --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\\n  --mount type=bind,source=$MVN_REPO_PATH,target=/root/.m2/repository \\\n  --mount type=bind,source=/opt/jenkins-keys,target=/root/.ssh \\\n  --hostname jenkins \\\n  --network traefik-net \\\n  --label traefik.port=8080 \\\n  --label traefik.docker.network=traefik-net \\\n  --label traefik.frontend.rule=Host:jenkins.maxfun.co \\\n  --label traefik.backend=jenkins-backend\n jenkinsci/blueocean:1.3.5\n```\n\n重点参数是下面几个\n```\n  --network traefik-net \\\n  --label traefik.port=8080 \\\n  --label traefik.docker.network=traefik-net \\\n  --label traefik.frontend.rule=Host:jenkins.maxfun.co \\\n  \n  --network \n   指定已创建好的overlay网络\n  --label traefik.port=8080\n   注册使用这个端口。当容器暴露出多个端口时非常有效。\n  --label traefik.docker.network=traefik-net \n   设置连接到这个容器的docker网络。 如果容易被链接到多个网络，\n   一定要设置合适的网络名称（你可以使用docker检查<container_id>）否则它将自动选择一个（取决于docker如何返回它们）。\n  --label traefik.frontend.rule=Host:jenkins.maxfun.co \n   覆盖默认前端规则（默认：Host:{containerName}.{domain}）\n  --label traefik.backend=jenkins-backend\n   将容器指向 enkins-backend 后端\n  --traefik.enable=false\n   可以使用docker service update servicename --label-add traefik.enable=false 调整该service不可用，从而达到把应用从负载均衡列表剔除\n  JENKINS_OPTS=--httpPort=8080\n   jenkins参数 --httpPort自定义jenkins端口 \n```\n\n更多traefik容器覆盖默认表现方式的Label：https://docs.traefik.cn/toml#docker-backend\n\n启动jenkins服务\n```\n$ sh run-jenkins.sh\n```\n\n测试\n```\n$ curl -H Host:jenkins.maxfun.co http://127.0.0.1\n```\n会返回以下内容，证明成功了。\n```\n<html><head><meta http-equiv='refresh' content='1;url=/login?from=%2F'/>\n<script>window.location.replace('/login?from=%2F');</script></head>\n<body style='background-color:white; color:white;'>\nAuthentication required\n<!--\nYou are authenticated as: anonymous\nGroups that you are in:\n  \nPermission you need to have (but didn't): hudson.model.Hudson.Read\n ... which is implied by: hudson.security.Permission.GenericRead\n ... which is implied by: hudson.model.Hudson.Administer\n-->\n</body></html>\n```\n就可以愉快的使用域名来访问jenkins啦。\n\n### 官方文档的例子\n\nwhoami.yaml\n```\nversion: \"3\"\nservices:\n  web:\n    image: emilevauge/whoami\n    networks: \n      - traefik-net\n    deploy:\n      labels:\n        - \"traefik.backend=whoami-backend\"\n        - \"traefik.port=80\"\n        - \"traefik.docker.network=traefik-net\"\n        - \"traefik.frontend.rule=Host:whoamistack.traefik\"\nnetworks:\n  traefik-net:\n    external: true\n```\n<font color='red'>注意： docker-compose文件 labels必须在deploy里面，结构跟上面所示</font>\n\n\n#### 启动第一个服务\n```\n$ docker stack deploy -c whoami.yaml whoami1\n```\n\n测试\n```\n$ curl -H Host:whoamistack.traefik http://127.0.0.1\n```\n\n```\nHostname: 9e402666ec66\nIP: 127.0.0.1\nIP: 10.0.1.9\nIP: 10.0.1.8\nIP: 192.168.16.10\nGET / HTTP/1.1\nHost: whoamistack.traefik\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 10.255.0.2\nX-Forwarded-Host: whoamistack.traefik\nX-Forwarded-Port: 80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 361dbc4b6660\nX-Real-Ip: 10.255.0.2\n```\n#### 启动第二个服务\n```\n$ docker stack deploy -c whoami.yaml whoami12\n```\n\n多次执行\n```\n$ curl -H Host:whoamistack.traefik http://127.0.0.1\n```\nHostname: 来回变化，这表明负载均衡功能正常工作了\n\n\n资料参考：https://docs.traefik.cn/toml#docker-backend","source":"_posts/docker-traefik-usage.md","raw":"---\ntitle: Docker Swarm集群使用Traefik\ntags: [docker]\ndate: 2018-04-02\n---\n\n## 前言\nTræfɪk 是一个为了让部署微服务更加便捷而诞生的现代HTTP反向代理、负载均衡工具。 它支持多种后台 (Docker, Swarm, Kubernetes, Marathon, Mesos, Consul, Etcd, Zookeeper, BoltDB, Rest API, file…) 来自动化、动态的应用它的配置文件设置。\n官方文档里面介绍了很多场景的使用，详情点击<a href='https://docs.traefik.cn/'>官方文档</a>\n\n## 部署Traefik\n### swarm网络\n\nswarm集群里面使用，需要使用overlay网络, 需先创建一个overlay网络，或者使用已有的overlay网络\n<font color='red'>（不要使用名字为ingress的那个网络，一开始我使用了，怎么弄都不成功，踩过得坑。谨记）</font>\n\n```\n$ docker network create traefik-net --driver overlay\n\n```\n查看网络\n```\n$ docker network ls\n\np86srgdn2zne        traefik-net         overlay             swarm\n```\n\ntraefik.yaml\n```\nversion: \"3\"\nservices:\n  traefik:\n    image: traefik\n    command: --docker --docker.swarmmode  --docker.domain=maxfun.co --docker.watch  --web --api --logLevel=DEBUG\n    ports:\n      - \"80:80\"\n      - \"8080:8080\"\n      - \"443:443\"\n    deploy:\n      placement:\n        constraints:\n        - node.role == manager \n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - $PWD/traefik.toml:/etc/traefik/traefik.toml\n    networks:\n    - traefik-net\nnetworks:\n  traefik-net:\n    external: true\n```\ntreafik配置文件:https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml\n启动traefik服务\n```\n$ docker stack deploy -c traefik.yaml traefik\n```\n或者\n\n```\ndocker service create \\\n    --name traefik \\\n    --constraint=node.role==manager \\\n    --publish 80:80 \\\n    --publish 8080:8080 \\\n    --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\\n    --mount type=bind,source=/opt/traefik/traefik.toml,target=/etc/traefik/traefik.toml \\\n    --network traefik-net \\\n    traefik \\\n    --docker \\\n    --docker.swarmmode \\\n    --docker.domain=maxfun.co \\\n    --docker.watch \\\n    --web \\\n    --api\n```\n\n\n###  部署jenkins应用\n在master节点部署jenkins，不适用端口来访问jenkins\n\nrun-jenkins.sh\n```\nmkdir -p /opt/jenkins-blueocean-data\nMVN_REPO_PATH=/home/coder/developtools/mavenRepository\ndocker service create \\\n  --name jenkins \\\n  --constraint=node.role==manager \\\n  -u root \\\n  -e JAVA_OPTS=-Duser.timezone=GMT+08 \\\n  -e JENKINS_OPTS=--httpPort=8080 \\\n  --publish 8089:8080 \\\n  --mount type=bind,source=/opt/jenkins-blueocean-data,target=/var/jenkins_home \\\n  --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\\n  --mount type=bind,source=$MVN_REPO_PATH,target=/root/.m2/repository \\\n  --mount type=bind,source=/opt/jenkins-keys,target=/root/.ssh \\\n  --hostname jenkins \\\n  --network traefik-net \\\n  --label traefik.port=8080 \\\n  --label traefik.docker.network=traefik-net \\\n  --label traefik.frontend.rule=Host:jenkins.maxfun.co \\\n  --label traefik.backend=jenkins-backend\n jenkinsci/blueocean:1.3.5\n```\n\n重点参数是下面几个\n```\n  --network traefik-net \\\n  --label traefik.port=8080 \\\n  --label traefik.docker.network=traefik-net \\\n  --label traefik.frontend.rule=Host:jenkins.maxfun.co \\\n  \n  --network \n   指定已创建好的overlay网络\n  --label traefik.port=8080\n   注册使用这个端口。当容器暴露出多个端口时非常有效。\n  --label traefik.docker.network=traefik-net \n   设置连接到这个容器的docker网络。 如果容易被链接到多个网络，\n   一定要设置合适的网络名称（你可以使用docker检查<container_id>）否则它将自动选择一个（取决于docker如何返回它们）。\n  --label traefik.frontend.rule=Host:jenkins.maxfun.co \n   覆盖默认前端规则（默认：Host:{containerName}.{domain}）\n  --label traefik.backend=jenkins-backend\n   将容器指向 enkins-backend 后端\n  --traefik.enable=false\n   可以使用docker service update servicename --label-add traefik.enable=false 调整该service不可用，从而达到把应用从负载均衡列表剔除\n  JENKINS_OPTS=--httpPort=8080\n   jenkins参数 --httpPort自定义jenkins端口 \n```\n\n更多traefik容器覆盖默认表现方式的Label：https://docs.traefik.cn/toml#docker-backend\n\n启动jenkins服务\n```\n$ sh run-jenkins.sh\n```\n\n测试\n```\n$ curl -H Host:jenkins.maxfun.co http://127.0.0.1\n```\n会返回以下内容，证明成功了。\n```\n<html><head><meta http-equiv='refresh' content='1;url=/login?from=%2F'/>\n<script>window.location.replace('/login?from=%2F');</script></head>\n<body style='background-color:white; color:white;'>\nAuthentication required\n<!--\nYou are authenticated as: anonymous\nGroups that you are in:\n  \nPermission you need to have (but didn't): hudson.model.Hudson.Read\n ... which is implied by: hudson.security.Permission.GenericRead\n ... which is implied by: hudson.model.Hudson.Administer\n-->\n</body></html>\n```\n就可以愉快的使用域名来访问jenkins啦。\n\n### 官方文档的例子\n\nwhoami.yaml\n```\nversion: \"3\"\nservices:\n  web:\n    image: emilevauge/whoami\n    networks: \n      - traefik-net\n    deploy:\n      labels:\n        - \"traefik.backend=whoami-backend\"\n        - \"traefik.port=80\"\n        - \"traefik.docker.network=traefik-net\"\n        - \"traefik.frontend.rule=Host:whoamistack.traefik\"\nnetworks:\n  traefik-net:\n    external: true\n```\n<font color='red'>注意： docker-compose文件 labels必须在deploy里面，结构跟上面所示</font>\n\n\n#### 启动第一个服务\n```\n$ docker stack deploy -c whoami.yaml whoami1\n```\n\n测试\n```\n$ curl -H Host:whoamistack.traefik http://127.0.0.1\n```\n\n```\nHostname: 9e402666ec66\nIP: 127.0.0.1\nIP: 10.0.1.9\nIP: 10.0.1.8\nIP: 192.168.16.10\nGET / HTTP/1.1\nHost: whoamistack.traefik\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 10.255.0.2\nX-Forwarded-Host: whoamistack.traefik\nX-Forwarded-Port: 80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 361dbc4b6660\nX-Real-Ip: 10.255.0.2\n```\n#### 启动第二个服务\n```\n$ docker stack deploy -c whoami.yaml whoami12\n```\n\n多次执行\n```\n$ curl -H Host:whoamistack.traefik http://127.0.0.1\n```\nHostname: 来回变化，这表明负载均衡功能正常工作了\n\n\n资料参考：https://docs.traefik.cn/toml#docker-backend","slug":"docker-traefik-usage","published":1,"updated":"2018-05-23T02:17:50.232Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjncsum4r00047gvrn7om26uv","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Træfɪk 是一个为了让部署微服务更加便捷而诞生的现代HTTP反向代理、负载均衡工具。 它支持多种后台 (Docker, Swarm, Kubernetes, Marathon, Mesos, Consul, Etcd, Zookeeper, BoltDB, Rest API, file…) 来自动化、动态的应用它的配置文件设置。<br>官方文档里面介绍了很多场景的使用，详情点击<a href=\"https://docs.traefik.cn/\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h2 id=\"部署Traefik\"><a href=\"#部署Traefik\" class=\"headerlink\" title=\"部署Traefik\"></a>部署Traefik</h2><h3 id=\"swarm网络\"><a href=\"#swarm网络\" class=\"headerlink\" title=\"swarm网络\"></a>swarm网络</h3><p>swarm集群里面使用，需要使用overlay网络, 需先创建一个overlay网络，或者使用已有的overlay网络</p>\n<font color=\"red\">（不要使用名字为ingress的那个网络，一开始我使用了，怎么弄都不成功，踩过得坑。谨记）</font>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network create traefik-net --driver overlay</span><br></pre></td></tr></table></figure>\n<p>查看网络<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network ls</span><br><span class=\"line\"></span><br><span class=\"line\">p86srgdn2zne        traefik-net         overlay             swarm</span><br></pre></td></tr></table></figure></p>\n<p>traefik.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &quot;3&quot;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  traefik:</span><br><span class=\"line\">    image: traefik</span><br><span class=\"line\">    command: --docker --docker.swarmmode  --docker.domain=maxfun.co --docker.watch  --web --api --logLevel=DEBUG</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;80:80&quot;</span><br><span class=\"line\">      - &quot;8080:8080&quot;</span><br><span class=\"line\">      - &quot;443:443&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      placement:</span><br><span class=\"line\">        constraints:</span><br><span class=\"line\">        - node.role == manager </span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - /var/run/docker.sock:/var/run/docker.sock</span><br><span class=\"line\">      - $PWD/traefik.toml:/etc/traefik/traefik.toml</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">    - traefik-net</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  traefik-net:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<p>treafik配置文件:<a href=\"https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml\" target=\"_blank\" rel=\"noopener\">https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml</a><br>启动traefik服务<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker stack deploy -c traefik.yaml traefik</span><br></pre></td></tr></table></figure></p>\n<p>或者</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker service create \\</span><br><span class=\"line\">    --name traefik \\</span><br><span class=\"line\">    --constraint=node.role==manager \\</span><br><span class=\"line\">    --publish 80:80 \\</span><br><span class=\"line\">    --publish 8080:8080 \\</span><br><span class=\"line\">    --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\</span><br><span class=\"line\">    --mount type=bind,source=/opt/traefik/traefik.toml,target=/etc/traefik/traefik.toml \\</span><br><span class=\"line\">    --network traefik-net \\</span><br><span class=\"line\">    traefik \\</span><br><span class=\"line\">    --docker \\</span><br><span class=\"line\">    --docker.swarmmode \\</span><br><span class=\"line\">    --docker.domain=maxfun.co \\</span><br><span class=\"line\">    --docker.watch \\</span><br><span class=\"line\">    --web \\</span><br><span class=\"line\">    --api</span><br></pre></td></tr></table></figure>\n<h3 id=\"部署jenkins应用\"><a href=\"#部署jenkins应用\" class=\"headerlink\" title=\"部署jenkins应用\"></a>部署jenkins应用</h3><p>在master节点部署jenkins，不适用端口来访问jenkins</p>\n<p>run-jenkins.sh<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /opt/jenkins-blueocean-data</span><br><span class=\"line\">MVN_REPO_PATH=/home/coder/developtools/mavenRepository</span><br><span class=\"line\">docker service create \\</span><br><span class=\"line\">  --name jenkins \\</span><br><span class=\"line\">  --constraint=node.role==manager \\</span><br><span class=\"line\">  -u root \\</span><br><span class=\"line\">  -e JAVA_OPTS=-Duser.timezone=GMT+08 \\</span><br><span class=\"line\">  -e JENKINS_OPTS=--httpPort=8080 \\</span><br><span class=\"line\">  --publish 8089:8080 \\</span><br><span class=\"line\">  --mount type=bind,source=/opt/jenkins-blueocean-data,target=/var/jenkins_home \\</span><br><span class=\"line\">  --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\</span><br><span class=\"line\">  --mount type=bind,source=$MVN_REPO_PATH,target=/root/.m2/repository \\</span><br><span class=\"line\">  --mount type=bind,source=/opt/jenkins-keys,target=/root/.ssh \\</span><br><span class=\"line\">  --hostname jenkins \\</span><br><span class=\"line\">  --network traefik-net \\</span><br><span class=\"line\">  --label traefik.port=8080 \\</span><br><span class=\"line\">  --label traefik.docker.network=traefik-net \\</span><br><span class=\"line\">  --label traefik.frontend.rule=Host:jenkins.maxfun.co \\</span><br><span class=\"line\">  --label traefik.backend=jenkins-backend</span><br><span class=\"line\"> jenkinsci/blueocean:1.3.5</span><br></pre></td></tr></table></figure></p>\n<p>重点参数是下面几个<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--network traefik-net \\</span><br><span class=\"line\">--label traefik.port=8080 \\</span><br><span class=\"line\">--label traefik.docker.network=traefik-net \\</span><br><span class=\"line\">--label traefik.frontend.rule=Host:jenkins.maxfun.co \\</span><br><span class=\"line\"></span><br><span class=\"line\">--network </span><br><span class=\"line\"> 指定已创建好的overlay网络</span><br><span class=\"line\">--label traefik.port=8080</span><br><span class=\"line\"> 注册使用这个端口。当容器暴露出多个端口时非常有效。</span><br><span class=\"line\">--label traefik.docker.network=traefik-net </span><br><span class=\"line\"> 设置连接到这个容器的docker网络。 如果容易被链接到多个网络，</span><br><span class=\"line\"> 一定要设置合适的网络名称（你可以使用docker检查&lt;container_id&gt;）否则它将自动选择一个（取决于docker如何返回它们）。</span><br><span class=\"line\">--label traefik.frontend.rule=Host:jenkins.maxfun.co </span><br><span class=\"line\"> 覆盖默认前端规则（默认：Host:&#123;containerName&#125;.&#123;domain&#125;）</span><br><span class=\"line\">--label traefik.backend=jenkins-backend</span><br><span class=\"line\"> 将容器指向 enkins-backend 后端</span><br><span class=\"line\">--traefik.enable=false</span><br><span class=\"line\"> 可以使用docker service update servicename --label-add traefik.enable=false 调整该service不可用，从而达到把应用从负载均衡列表剔除</span><br><span class=\"line\">JENKINS_OPTS=--httpPort=8080</span><br><span class=\"line\"> jenkins参数 --httpPort自定义jenkins端口</span><br></pre></td></tr></table></figure></p>\n<p>更多traefik容器覆盖默认表现方式的Label：<a href=\"https://docs.traefik.cn/toml#docker-backend\" target=\"_blank\" rel=\"noopener\">https://docs.traefik.cn/toml#docker-backend</a></p>\n<p>启动jenkins服务<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sh run-jenkins.sh</span><br></pre></td></tr></table></figure></p>\n<p>测试<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl -H Host:jenkins.maxfun.co http://127.0.0.1</span><br></pre></td></tr></table></figure></p>\n<p>会返回以下内容，证明成功了。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&apos;refresh&apos; content=&apos;1;url=/login?from=%2F&apos;/&gt;</span><br><span class=\"line\">&lt;script&gt;window.location.replace(&apos;/login?from=%2F&apos;);&lt;/script&gt;&lt;/head&gt;</span><br><span class=\"line\">&lt;body style=&apos;background-color:white; color:white;&apos;&gt;</span><br><span class=\"line\">Authentication required</span><br><span class=\"line\">&lt;!--</span><br><span class=\"line\">You are authenticated as: anonymous</span><br><span class=\"line\">Groups that you are in:</span><br><span class=\"line\">  </span><br><span class=\"line\">Permission you need to have (but didn&apos;t): hudson.model.Hudson.Read</span><br><span class=\"line\"> ... which is implied by: hudson.security.Permission.GenericRead</span><br><span class=\"line\"> ... which is implied by: hudson.model.Hudson.Administer</span><br><span class=\"line\">--&gt;</span><br><span class=\"line\">&lt;/body&gt;&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n<p>就可以愉快的使用域名来访问jenkins啦。</p>\n<h3 id=\"官方文档的例子\"><a href=\"#官方文档的例子\" class=\"headerlink\" title=\"官方文档的例子\"></a>官方文档的例子</h3><p>whoami.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &quot;3&quot;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    image: emilevauge/whoami</span><br><span class=\"line\">    networks: </span><br><span class=\"line\">      - traefik-net</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        - &quot;traefik.backend=whoami-backend&quot;</span><br><span class=\"line\">        - &quot;traefik.port=80&quot;</span><br><span class=\"line\">        - &quot;traefik.docker.network=traefik-net&quot;</span><br><span class=\"line\">        - &quot;traefik.frontend.rule=Host:whoamistack.traefik&quot;</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  traefik-net:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<font color=\"red\">注意： docker-compose文件 labels必须在deploy里面，结构跟上面所示</font>\n\n\n<h4 id=\"启动第一个服务\"><a href=\"#启动第一个服务\" class=\"headerlink\" title=\"启动第一个服务\"></a>启动第一个服务</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker stack deploy -c whoami.yaml whoami1</span><br></pre></td></tr></table></figure>\n<p>测试<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl -H Host:whoamistack.traefik http://127.0.0.1</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Hostname: 9e402666ec66</span><br><span class=\"line\">IP: 127.0.0.1</span><br><span class=\"line\">IP: 10.0.1.9</span><br><span class=\"line\">IP: 10.0.1.8</span><br><span class=\"line\">IP: 192.168.16.10</span><br><span class=\"line\">GET / HTTP/1.1</span><br><span class=\"line\">Host: whoamistack.traefik</span><br><span class=\"line\">User-Agent: curl/7.35.0</span><br><span class=\"line\">Accept: */*</span><br><span class=\"line\">Accept-Encoding: gzip</span><br><span class=\"line\">X-Forwarded-For: 10.255.0.2</span><br><span class=\"line\">X-Forwarded-Host: whoamistack.traefik</span><br><span class=\"line\">X-Forwarded-Port: 80</span><br><span class=\"line\">X-Forwarded-Proto: http</span><br><span class=\"line\">X-Forwarded-Server: 361dbc4b6660</span><br><span class=\"line\">X-Real-Ip: 10.255.0.2</span><br></pre></td></tr></table></figure>\n<h4 id=\"启动第二个服务\"><a href=\"#启动第二个服务\" class=\"headerlink\" title=\"启动第二个服务\"></a>启动第二个服务</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker stack deploy -c whoami.yaml whoami12</span><br></pre></td></tr></table></figure>\n<p>多次执行<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl -H Host:whoamistack.traefik http://127.0.0.1</span><br></pre></td></tr></table></figure></p>\n<p>Hostname: 来回变化，这表明负载均衡功能正常工作了</p>\n<p>资料参考：<a href=\"https://docs.traefik.cn/toml#docker-backend\" target=\"_blank\" rel=\"noopener\">https://docs.traefik.cn/toml#docker-backend</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Træfɪk 是一个为了让部署微服务更加便捷而诞生的现代HTTP反向代理、负载均衡工具。 它支持多种后台 (Docker, Swarm, Kubernetes, Marathon, Mesos, Consul, Etcd, Zookeeper, BoltDB, Rest API, file…) 来自动化、动态的应用它的配置文件设置。<br>官方文档里面介绍了很多场景的使用，详情点击<a href=\"https://docs.traefik.cn/\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h2 id=\"部署Traefik\"><a href=\"#部署Traefik\" class=\"headerlink\" title=\"部署Traefik\"></a>部署Traefik</h2><h3 id=\"swarm网络\"><a href=\"#swarm网络\" class=\"headerlink\" title=\"swarm网络\"></a>swarm网络</h3><p>swarm集群里面使用，需要使用overlay网络, 需先创建一个overlay网络，或者使用已有的overlay网络</p>\n<font color=\"red\">（不要使用名字为ingress的那个网络，一开始我使用了，怎么弄都不成功，踩过得坑。谨记）</font>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network create traefik-net --driver overlay</span><br></pre></td></tr></table></figure>\n<p>查看网络<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network ls</span><br><span class=\"line\"></span><br><span class=\"line\">p86srgdn2zne        traefik-net         overlay             swarm</span><br></pre></td></tr></table></figure></p>\n<p>traefik.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &quot;3&quot;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  traefik:</span><br><span class=\"line\">    image: traefik</span><br><span class=\"line\">    command: --docker --docker.swarmmode  --docker.domain=maxfun.co --docker.watch  --web --api --logLevel=DEBUG</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;80:80&quot;</span><br><span class=\"line\">      - &quot;8080:8080&quot;</span><br><span class=\"line\">      - &quot;443:443&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      placement:</span><br><span class=\"line\">        constraints:</span><br><span class=\"line\">        - node.role == manager </span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - /var/run/docker.sock:/var/run/docker.sock</span><br><span class=\"line\">      - $PWD/traefik.toml:/etc/traefik/traefik.toml</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">    - traefik-net</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  traefik-net:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<p>treafik配置文件:<a href=\"https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml\" target=\"_blank\" rel=\"noopener\">https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml</a><br>启动traefik服务<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker stack deploy -c traefik.yaml traefik</span><br></pre></td></tr></table></figure></p>\n<p>或者</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker service create \\</span><br><span class=\"line\">    --name traefik \\</span><br><span class=\"line\">    --constraint=node.role==manager \\</span><br><span class=\"line\">    --publish 80:80 \\</span><br><span class=\"line\">    --publish 8080:8080 \\</span><br><span class=\"line\">    --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\</span><br><span class=\"line\">    --mount type=bind,source=/opt/traefik/traefik.toml,target=/etc/traefik/traefik.toml \\</span><br><span class=\"line\">    --network traefik-net \\</span><br><span class=\"line\">    traefik \\</span><br><span class=\"line\">    --docker \\</span><br><span class=\"line\">    --docker.swarmmode \\</span><br><span class=\"line\">    --docker.domain=maxfun.co \\</span><br><span class=\"line\">    --docker.watch \\</span><br><span class=\"line\">    --web \\</span><br><span class=\"line\">    --api</span><br></pre></td></tr></table></figure>\n<h3 id=\"部署jenkins应用\"><a href=\"#部署jenkins应用\" class=\"headerlink\" title=\"部署jenkins应用\"></a>部署jenkins应用</h3><p>在master节点部署jenkins，不适用端口来访问jenkins</p>\n<p>run-jenkins.sh<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /opt/jenkins-blueocean-data</span><br><span class=\"line\">MVN_REPO_PATH=/home/coder/developtools/mavenRepository</span><br><span class=\"line\">docker service create \\</span><br><span class=\"line\">  --name jenkins \\</span><br><span class=\"line\">  --constraint=node.role==manager \\</span><br><span class=\"line\">  -u root \\</span><br><span class=\"line\">  -e JAVA_OPTS=-Duser.timezone=GMT+08 \\</span><br><span class=\"line\">  -e JENKINS_OPTS=--httpPort=8080 \\</span><br><span class=\"line\">  --publish 8089:8080 \\</span><br><span class=\"line\">  --mount type=bind,source=/opt/jenkins-blueocean-data,target=/var/jenkins_home \\</span><br><span class=\"line\">  --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\</span><br><span class=\"line\">  --mount type=bind,source=$MVN_REPO_PATH,target=/root/.m2/repository \\</span><br><span class=\"line\">  --mount type=bind,source=/opt/jenkins-keys,target=/root/.ssh \\</span><br><span class=\"line\">  --hostname jenkins \\</span><br><span class=\"line\">  --network traefik-net \\</span><br><span class=\"line\">  --label traefik.port=8080 \\</span><br><span class=\"line\">  --label traefik.docker.network=traefik-net \\</span><br><span class=\"line\">  --label traefik.frontend.rule=Host:jenkins.maxfun.co \\</span><br><span class=\"line\">  --label traefik.backend=jenkins-backend</span><br><span class=\"line\"> jenkinsci/blueocean:1.3.5</span><br></pre></td></tr></table></figure></p>\n<p>重点参数是下面几个<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--network traefik-net \\</span><br><span class=\"line\">--label traefik.port=8080 \\</span><br><span class=\"line\">--label traefik.docker.network=traefik-net \\</span><br><span class=\"line\">--label traefik.frontend.rule=Host:jenkins.maxfun.co \\</span><br><span class=\"line\"></span><br><span class=\"line\">--network </span><br><span class=\"line\"> 指定已创建好的overlay网络</span><br><span class=\"line\">--label traefik.port=8080</span><br><span class=\"line\"> 注册使用这个端口。当容器暴露出多个端口时非常有效。</span><br><span class=\"line\">--label traefik.docker.network=traefik-net </span><br><span class=\"line\"> 设置连接到这个容器的docker网络。 如果容易被链接到多个网络，</span><br><span class=\"line\"> 一定要设置合适的网络名称（你可以使用docker检查&lt;container_id&gt;）否则它将自动选择一个（取决于docker如何返回它们）。</span><br><span class=\"line\">--label traefik.frontend.rule=Host:jenkins.maxfun.co </span><br><span class=\"line\"> 覆盖默认前端规则（默认：Host:&#123;containerName&#125;.&#123;domain&#125;）</span><br><span class=\"line\">--label traefik.backend=jenkins-backend</span><br><span class=\"line\"> 将容器指向 enkins-backend 后端</span><br><span class=\"line\">--traefik.enable=false</span><br><span class=\"line\"> 可以使用docker service update servicename --label-add traefik.enable=false 调整该service不可用，从而达到把应用从负载均衡列表剔除</span><br><span class=\"line\">JENKINS_OPTS=--httpPort=8080</span><br><span class=\"line\"> jenkins参数 --httpPort自定义jenkins端口</span><br></pre></td></tr></table></figure></p>\n<p>更多traefik容器覆盖默认表现方式的Label：<a href=\"https://docs.traefik.cn/toml#docker-backend\" target=\"_blank\" rel=\"noopener\">https://docs.traefik.cn/toml#docker-backend</a></p>\n<p>启动jenkins服务<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sh run-jenkins.sh</span><br></pre></td></tr></table></figure></p>\n<p>测试<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl -H Host:jenkins.maxfun.co http://127.0.0.1</span><br></pre></td></tr></table></figure></p>\n<p>会返回以下内容，证明成功了。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&apos;refresh&apos; content=&apos;1;url=/login?from=%2F&apos;/&gt;</span><br><span class=\"line\">&lt;script&gt;window.location.replace(&apos;/login?from=%2F&apos;);&lt;/script&gt;&lt;/head&gt;</span><br><span class=\"line\">&lt;body style=&apos;background-color:white; color:white;&apos;&gt;</span><br><span class=\"line\">Authentication required</span><br><span class=\"line\">&lt;!--</span><br><span class=\"line\">You are authenticated as: anonymous</span><br><span class=\"line\">Groups that you are in:</span><br><span class=\"line\">  </span><br><span class=\"line\">Permission you need to have (but didn&apos;t): hudson.model.Hudson.Read</span><br><span class=\"line\"> ... which is implied by: hudson.security.Permission.GenericRead</span><br><span class=\"line\"> ... which is implied by: hudson.model.Hudson.Administer</span><br><span class=\"line\">--&gt;</span><br><span class=\"line\">&lt;/body&gt;&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n<p>就可以愉快的使用域名来访问jenkins啦。</p>\n<h3 id=\"官方文档的例子\"><a href=\"#官方文档的例子\" class=\"headerlink\" title=\"官方文档的例子\"></a>官方文档的例子</h3><p>whoami.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &quot;3&quot;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  web:</span><br><span class=\"line\">    image: emilevauge/whoami</span><br><span class=\"line\">    networks: </span><br><span class=\"line\">      - traefik-net</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        - &quot;traefik.backend=whoami-backend&quot;</span><br><span class=\"line\">        - &quot;traefik.port=80&quot;</span><br><span class=\"line\">        - &quot;traefik.docker.network=traefik-net&quot;</span><br><span class=\"line\">        - &quot;traefik.frontend.rule=Host:whoamistack.traefik&quot;</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  traefik-net:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<font color=\"red\">注意： docker-compose文件 labels必须在deploy里面，结构跟上面所示</font>\n\n\n<h4 id=\"启动第一个服务\"><a href=\"#启动第一个服务\" class=\"headerlink\" title=\"启动第一个服务\"></a>启动第一个服务</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker stack deploy -c whoami.yaml whoami1</span><br></pre></td></tr></table></figure>\n<p>测试<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl -H Host:whoamistack.traefik http://127.0.0.1</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Hostname: 9e402666ec66</span><br><span class=\"line\">IP: 127.0.0.1</span><br><span class=\"line\">IP: 10.0.1.9</span><br><span class=\"line\">IP: 10.0.1.8</span><br><span class=\"line\">IP: 192.168.16.10</span><br><span class=\"line\">GET / HTTP/1.1</span><br><span class=\"line\">Host: whoamistack.traefik</span><br><span class=\"line\">User-Agent: curl/7.35.0</span><br><span class=\"line\">Accept: */*</span><br><span class=\"line\">Accept-Encoding: gzip</span><br><span class=\"line\">X-Forwarded-For: 10.255.0.2</span><br><span class=\"line\">X-Forwarded-Host: whoamistack.traefik</span><br><span class=\"line\">X-Forwarded-Port: 80</span><br><span class=\"line\">X-Forwarded-Proto: http</span><br><span class=\"line\">X-Forwarded-Server: 361dbc4b6660</span><br><span class=\"line\">X-Real-Ip: 10.255.0.2</span><br></pre></td></tr></table></figure>\n<h4 id=\"启动第二个服务\"><a href=\"#启动第二个服务\" class=\"headerlink\" title=\"启动第二个服务\"></a>启动第二个服务</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker stack deploy -c whoami.yaml whoami12</span><br></pre></td></tr></table></figure>\n<p>多次执行<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl -H Host:whoamistack.traefik http://127.0.0.1</span><br></pre></td></tr></table></figure></p>\n<p>Hostname: 来回变化，这表明负载均衡功能正常工作了</p>\n<p>资料参考：<a href=\"https://docs.traefik.cn/toml#docker-backend\" target=\"_blank\" rel=\"noopener\">https://docs.traefik.cn/toml#docker-backend</a></p>\n"},{"title":"Elasticsearch+Fluentd+Kibana使用","date":"2018-05-23T16:00:00.000Z","_content":"\n## 前言\n分布式应用，部署在Docker中，日志分散在各个服务器上，为了方便查看日志，\n在应用中使用Fluentd把日志收集起来，存放到Elasticsearch中，使用Kibana来查看日志，分析等\n\n## 编译配置Fluentd\nFluentd配合elasticsearch使用需要安装fluent-plugin-elasticsearch\n下面来构建Dokerfile来编译一个带elasticsearch插件的Fluentd\n\n```\nFROM fluent/fluentd:v0.12-debian\nRUN [\"gem\", \"install\", \"fluent-plugin-elasticsearch\", \"--no-rdoc\", \"--no-ri\", \"--version\", \"1.9.2\"]\n\n```\n编译镜像\n```\n# docker build -t fluent-plugin-elastic:1.9.2 ./\n```\n准备好fluent配置文件\nfluentd/conf/fluent.conf\n```\n# fluentd/conf/fluent.conf\n<source>\n  @type forward\n  port 24224\n  bind 0.0.0.0\n</source>\n<match *.**>\n  @type copy\n  <store>\n    @type elasticsearch\n    host elasticsearch\n    port 9200\n    logstash_format true\n    logstash_prefix fluentd\n    logstash_dateformat %Y%m%d\n    include_tag_key true\n    type_name access_log\n    tag_key @log_name\n    flush_interval 1s\n  </store>\n  <store>\n    @type stdout\n  </store>\n</match>\n```\n详细配置文件语法：https://docs.fluentd.org/v1.0/articles/config-file\n\n## 部署efk(elasticsearch fluent kibana)\ndocker-compose.yaml\n```\nversion: '3'\nservices:\n  fluentd:\n    image: fluent-plugin-elastic:1.9.2\n    volumes:\n      #挂载fluent配置文件\n      - ./fluent/conf:/fluentd/etc\n    environment:\n      #配置elasticsearch主机端口\n      - \"ELASTICSEARCH_URL=http://172.19.0.1:9200\"\n    ports:\n      - \"24224:24224\"\n      - \"24224:24224/udp\"\n\n  elasticsearch:\n    image: elasticsearch:5.6.9-alpine\n    volumes:\n      - ./elasticsearch:/usr/share/elasticsearch/data\n    ports:\n      - \"9200:9200\"\n\n  kibana:\t\n    image: kibana:5.6.9\n    environment:\n      - \"ELASTICSEARCH_URL=http://172.19.0.1:9200\"\n    ports:\n      - \"5601:5601\"\n    networks: \n      - efk_default\nnetworks:\n  efk_default:\n    external: true\n\n```\n注意：ELASTICSEARCH_URL环境变量参数，可以配置成宿主机内网ip或者其他elasticsearch主机端口\n\n部署\n```\n# docker stack deploy -c docker-compose.yaml efk\n```\n\n## 在项目中使用fluentd收集日志\n以Java springboot项目为例子\n添加maven依赖\n```\n        <dependency>\n            <groupId>org.fluentd</groupId>\n            <artifactId>fluent-logger</artifactId>\n            <version>0.3.3</version>\n        </dependency>\n\n        <dependency>\n            <groupId>com.sndyuk</groupId>\n            <artifactId>logback-more-appenders</artifactId>\n            <version>1.4.3</version>\n        </dependency>\n```\n增加logback-spring.xml配置文件\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration scan=\"true\" scanPeriod=\"60 seconds\" debug=\"true\"\n\txmlns=\"http://ch.qos.logback/xml/ns/logback\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\txsi:schemaLocation=\"http://ch.qos.logback/xml/ns/logback http://ch.qos.logback/xml/ns/logback/logback.xsd\">\n\t<property name=\"fileName\" value=\"maxfunSpider-console\" />\n    \n    <!-- ${FLUENTD_HOST: -xx.xx.xx.xx 意思是从配置文件获取FLUENTD_HOST，如果没有就用默认值-->\n    <property name=\"FLUENTD_HOST\" value=\"${FLUENTD_HOST:-服务器IP}\" />\n    <property name=\"FLUENTD_PORT\" value=\"${FLUENTD_PORT:-24224}\" />\n    \n\t<include resource=\"org/springframework/boot/logging/logback/base.xml\" />\n  \n\t<appender name=\"FILE\"\n\t\tclass=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n\t\t<rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n\t\t\t<fileNamePattern>./log/${fileName}.%d{yyyy-MM-dd}.log\n\t\t\t</fileNamePattern>\n\t\t\t<maxHistory>30</maxHistory>\n\t\t</rollingPolicy>\n\n\t\t<encoder>\n\t\t\t<pattern>${CONSOLE_LOG_PATTERN}</pattern>\n\t\t</encoder>\n\t</appender>    \n    \n    <springProfile name=\"dev\">\n        <!-- 测试环境日志级别 -->\n        <property name=\"loggerLevel\" value=\"INFO\" />\n        <appender name=\"FLUENT\" class=\"ch.qos.logback.more.appenders.DataFluentAppender\">\n            <tag>maxfunCrawler</tag>\n            <label>dev</label>\n            <remoteHost>${FLUENTD_HOST}</remoteHost>\n            <port>${FLUENTD_PORT}</port>\n        </appender>\n    </springProfile>\n    \n    <springProfile name=\"prod\">\n        <!-- 生成环境日志级别 -->\n        <property name=\"loggerLevel\" value=\"ERROR\" />\n        <appender name=\"FLUENT\" class=\"ch.qos.logback.more.appenders.DataFluentAppender\">\n            <tag>maxfunCrawler</tag>\n            <label>prod</label>\n            <remoteHost>${FLUENTD_HOST}</remoteHost>\n            <port>${FLUENTD_PORT}</port>\n        </appender>\n    </springProfile>\n    \n    <root level=\"${loggerLevel}\">\n\t\t<appender-ref ref=\"FILE\" />\n\t\t<appender-ref ref=\"FLUENT\" />\n\t</root>\n</configuration>\n```\n\n启动springboot项目，不出意外的话，在Kibana就可以看到收集到的日志了\n\n## 使用\n部署成功后，访问ip:5601浏览Kibana页面了，第一次打开是如下界面。\n![avatar](https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana.png)\n\n把Index pattern改成fluentd-*，点Create。如果elasticsearch已经有日志的话，会显示出来，没有的就是这样子的\n![avatar](https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana-noresult.png)\n\n收集到的日志\n![avatar](https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana-log.png)\n\n## 增加http验证\n到处就可以用Kibana来分析收集到的日志了，目前任何人都访问kibana，没有设置登录验证，不安全，可以配合treafik做个简单的http auth验证。\n\n### 配置部署trarfik\n配置\ntrarfik.toml来自https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml\n把entryPoints改成下面这样。\n```\n[entryPoints]\n    [entryPoints.http]\n    address = \":80\"\n    [entryPoints.http.auth.basic]\n    #可以使用在线的htpasswd生成\n    users = [\"test:$apr1$CMMXhsE1$ltlr0YMcFYV16nKB.L9Ah0\"]\n\n```\n\ntraefik.yaml\n```\nersion: \"3\"\nservices:\n  traefik:\n    image: traefik\n    #--docker.domain可以根据自己需求修改\n    command: --docker --docker.swarmmode  --docker.domain=kevin.co --docker.watch  --web --api --logLevel=DEBUG\n    ports:\n      #这台机器的80端口被占用了，所以用81端口。\n      - \"81:80\"\n      #traefik dashboard端口\n      - \"8080:8080\"\n      - \"443:443\"\n    deploy:\n      placement:\n        constraints:\n        - node.role == manager\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      #挂载配置文件\n      - $PWD/traefik.toml:/etc/traefik/traefik.toml\n    networks:\n    #使用跟kibana同样的网络\n    - efk_default\nnetworks:\n  efk_default:\n    external: true\n```\n\n\n部署traefik\n```\n# docker stack deploy -c traefik.yaml efk\n```\n\n把kibana的docker-compose稍微改一下，增加deploy属性，加入相关traefik label。 \n```\n  kibana:\t\n    image: kibana:5.6.9\n    #links:\n    #  - \"elasticsearch\"\n    environment:\n      - \"ELASTICSEARCH_URL=http://172.19.0.1:9200\"\n    #ports:\n    #  - \"5601:5601\"\n    deploy:\n      labels:\n        - \"traefik.port=5601\"\n        - \"traefik.docker.network=efk_default\"\n        - \"traefik.backend=kibana\"\n        - \"traefik.frontend.rule=Host:kibana.kevin.co\"\n    networks: \n      - efk_default\n```\n### 重新部署kibana\n```\n# docker service rm efk_kibana\n# docker stack deploy -c docker-compose.yaml efk\n```\nkibana.kevin.co域名是没有经过dns解析的的所以要修改hosts文件指向kibana.kevin.co域名\n修改完后，就可以使用域名加端口访问Kinaba了，这时候就需要输入配置好的用户密码登录才能正常访问，否则会报401错误\n\n完。","source":"_posts/elasticsearch-fluent-kibana.md","raw":"---\ntitle: Elasticsearch+Fluentd+Kibana使用\ntags: [Elasticsearch, Docker, Fluentd, Kibana]\ndate: 2018-05-24\n---\n\n## 前言\n分布式应用，部署在Docker中，日志分散在各个服务器上，为了方便查看日志，\n在应用中使用Fluentd把日志收集起来，存放到Elasticsearch中，使用Kibana来查看日志，分析等\n\n## 编译配置Fluentd\nFluentd配合elasticsearch使用需要安装fluent-plugin-elasticsearch\n下面来构建Dokerfile来编译一个带elasticsearch插件的Fluentd\n\n```\nFROM fluent/fluentd:v0.12-debian\nRUN [\"gem\", \"install\", \"fluent-plugin-elasticsearch\", \"--no-rdoc\", \"--no-ri\", \"--version\", \"1.9.2\"]\n\n```\n编译镜像\n```\n# docker build -t fluent-plugin-elastic:1.9.2 ./\n```\n准备好fluent配置文件\nfluentd/conf/fluent.conf\n```\n# fluentd/conf/fluent.conf\n<source>\n  @type forward\n  port 24224\n  bind 0.0.0.0\n</source>\n<match *.**>\n  @type copy\n  <store>\n    @type elasticsearch\n    host elasticsearch\n    port 9200\n    logstash_format true\n    logstash_prefix fluentd\n    logstash_dateformat %Y%m%d\n    include_tag_key true\n    type_name access_log\n    tag_key @log_name\n    flush_interval 1s\n  </store>\n  <store>\n    @type stdout\n  </store>\n</match>\n```\n详细配置文件语法：https://docs.fluentd.org/v1.0/articles/config-file\n\n## 部署efk(elasticsearch fluent kibana)\ndocker-compose.yaml\n```\nversion: '3'\nservices:\n  fluentd:\n    image: fluent-plugin-elastic:1.9.2\n    volumes:\n      #挂载fluent配置文件\n      - ./fluent/conf:/fluentd/etc\n    environment:\n      #配置elasticsearch主机端口\n      - \"ELASTICSEARCH_URL=http://172.19.0.1:9200\"\n    ports:\n      - \"24224:24224\"\n      - \"24224:24224/udp\"\n\n  elasticsearch:\n    image: elasticsearch:5.6.9-alpine\n    volumes:\n      - ./elasticsearch:/usr/share/elasticsearch/data\n    ports:\n      - \"9200:9200\"\n\n  kibana:\t\n    image: kibana:5.6.9\n    environment:\n      - \"ELASTICSEARCH_URL=http://172.19.0.1:9200\"\n    ports:\n      - \"5601:5601\"\n    networks: \n      - efk_default\nnetworks:\n  efk_default:\n    external: true\n\n```\n注意：ELASTICSEARCH_URL环境变量参数，可以配置成宿主机内网ip或者其他elasticsearch主机端口\n\n部署\n```\n# docker stack deploy -c docker-compose.yaml efk\n```\n\n## 在项目中使用fluentd收集日志\n以Java springboot项目为例子\n添加maven依赖\n```\n        <dependency>\n            <groupId>org.fluentd</groupId>\n            <artifactId>fluent-logger</artifactId>\n            <version>0.3.3</version>\n        </dependency>\n\n        <dependency>\n            <groupId>com.sndyuk</groupId>\n            <artifactId>logback-more-appenders</artifactId>\n            <version>1.4.3</version>\n        </dependency>\n```\n增加logback-spring.xml配置文件\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration scan=\"true\" scanPeriod=\"60 seconds\" debug=\"true\"\n\txmlns=\"http://ch.qos.logback/xml/ns/logback\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\txsi:schemaLocation=\"http://ch.qos.logback/xml/ns/logback http://ch.qos.logback/xml/ns/logback/logback.xsd\">\n\t<property name=\"fileName\" value=\"maxfunSpider-console\" />\n    \n    <!-- ${FLUENTD_HOST: -xx.xx.xx.xx 意思是从配置文件获取FLUENTD_HOST，如果没有就用默认值-->\n    <property name=\"FLUENTD_HOST\" value=\"${FLUENTD_HOST:-服务器IP}\" />\n    <property name=\"FLUENTD_PORT\" value=\"${FLUENTD_PORT:-24224}\" />\n    \n\t<include resource=\"org/springframework/boot/logging/logback/base.xml\" />\n  \n\t<appender name=\"FILE\"\n\t\tclass=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n\t\t<rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n\t\t\t<fileNamePattern>./log/${fileName}.%d{yyyy-MM-dd}.log\n\t\t\t</fileNamePattern>\n\t\t\t<maxHistory>30</maxHistory>\n\t\t</rollingPolicy>\n\n\t\t<encoder>\n\t\t\t<pattern>${CONSOLE_LOG_PATTERN}</pattern>\n\t\t</encoder>\n\t</appender>    \n    \n    <springProfile name=\"dev\">\n        <!-- 测试环境日志级别 -->\n        <property name=\"loggerLevel\" value=\"INFO\" />\n        <appender name=\"FLUENT\" class=\"ch.qos.logback.more.appenders.DataFluentAppender\">\n            <tag>maxfunCrawler</tag>\n            <label>dev</label>\n            <remoteHost>${FLUENTD_HOST}</remoteHost>\n            <port>${FLUENTD_PORT}</port>\n        </appender>\n    </springProfile>\n    \n    <springProfile name=\"prod\">\n        <!-- 生成环境日志级别 -->\n        <property name=\"loggerLevel\" value=\"ERROR\" />\n        <appender name=\"FLUENT\" class=\"ch.qos.logback.more.appenders.DataFluentAppender\">\n            <tag>maxfunCrawler</tag>\n            <label>prod</label>\n            <remoteHost>${FLUENTD_HOST}</remoteHost>\n            <port>${FLUENTD_PORT}</port>\n        </appender>\n    </springProfile>\n    \n    <root level=\"${loggerLevel}\">\n\t\t<appender-ref ref=\"FILE\" />\n\t\t<appender-ref ref=\"FLUENT\" />\n\t</root>\n</configuration>\n```\n\n启动springboot项目，不出意外的话，在Kibana就可以看到收集到的日志了\n\n## 使用\n部署成功后，访问ip:5601浏览Kibana页面了，第一次打开是如下界面。\n![avatar](https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana.png)\n\n把Index pattern改成fluentd-*，点Create。如果elasticsearch已经有日志的话，会显示出来，没有的就是这样子的\n![avatar](https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana-noresult.png)\n\n收集到的日志\n![avatar](https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana-log.png)\n\n## 增加http验证\n到处就可以用Kibana来分析收集到的日志了，目前任何人都访问kibana，没有设置登录验证，不安全，可以配合treafik做个简单的http auth验证。\n\n### 配置部署trarfik\n配置\ntrarfik.toml来自https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml\n把entryPoints改成下面这样。\n```\n[entryPoints]\n    [entryPoints.http]\n    address = \":80\"\n    [entryPoints.http.auth.basic]\n    #可以使用在线的htpasswd生成\n    users = [\"test:$apr1$CMMXhsE1$ltlr0YMcFYV16nKB.L9Ah0\"]\n\n```\n\ntraefik.yaml\n```\nersion: \"3\"\nservices:\n  traefik:\n    image: traefik\n    #--docker.domain可以根据自己需求修改\n    command: --docker --docker.swarmmode  --docker.domain=kevin.co --docker.watch  --web --api --logLevel=DEBUG\n    ports:\n      #这台机器的80端口被占用了，所以用81端口。\n      - \"81:80\"\n      #traefik dashboard端口\n      - \"8080:8080\"\n      - \"443:443\"\n    deploy:\n      placement:\n        constraints:\n        - node.role == manager\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      #挂载配置文件\n      - $PWD/traefik.toml:/etc/traefik/traefik.toml\n    networks:\n    #使用跟kibana同样的网络\n    - efk_default\nnetworks:\n  efk_default:\n    external: true\n```\n\n\n部署traefik\n```\n# docker stack deploy -c traefik.yaml efk\n```\n\n把kibana的docker-compose稍微改一下，增加deploy属性，加入相关traefik label。 \n```\n  kibana:\t\n    image: kibana:5.6.9\n    #links:\n    #  - \"elasticsearch\"\n    environment:\n      - \"ELASTICSEARCH_URL=http://172.19.0.1:9200\"\n    #ports:\n    #  - \"5601:5601\"\n    deploy:\n      labels:\n        - \"traefik.port=5601\"\n        - \"traefik.docker.network=efk_default\"\n        - \"traefik.backend=kibana\"\n        - \"traefik.frontend.rule=Host:kibana.kevin.co\"\n    networks: \n      - efk_default\n```\n### 重新部署kibana\n```\n# docker service rm efk_kibana\n# docker stack deploy -c docker-compose.yaml efk\n```\nkibana.kevin.co域名是没有经过dns解析的的所以要修改hosts文件指向kibana.kevin.co域名\n修改完后，就可以使用域名加端口访问Kinaba了，这时候就需要输入配置好的用户密码登录才能正常访问，否则会报401错误\n\n完。","slug":"elasticsearch-fluent-kibana","published":1,"updated":"2018-10-17T06:52:31.141Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjncsum4s00057gvrktbm8l0c","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>分布式应用，部署在Docker中，日志分散在各个服务器上，为了方便查看日志，<br>在应用中使用Fluentd把日志收集起来，存放到Elasticsearch中，使用Kibana来查看日志，分析等</p>\n<h2 id=\"编译配置Fluentd\"><a href=\"#编译配置Fluentd\" class=\"headerlink\" title=\"编译配置Fluentd\"></a>编译配置Fluentd</h2><p>Fluentd配合elasticsearch使用需要安装fluent-plugin-elasticsearch<br>下面来构建Dokerfile来编译一个带elasticsearch插件的Fluentd</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM fluent/fluentd:v0.12-debian</span><br><span class=\"line\">RUN [&quot;gem&quot;, &quot;install&quot;, &quot;fluent-plugin-elasticsearch&quot;, &quot;--no-rdoc&quot;, &quot;--no-ri&quot;, &quot;--version&quot;, &quot;1.9.2&quot;]</span><br></pre></td></tr></table></figure>\n<p>编译镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker build -t fluent-plugin-elastic:1.9.2 ./</span><br></pre></td></tr></table></figure></p>\n<p>准备好fluent配置文件<br>fluentd/conf/fluent.conf<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># fluentd/conf/fluent.conf</span><br><span class=\"line\">&lt;source&gt;</span><br><span class=\"line\">  @type forward</span><br><span class=\"line\">  port 24224</span><br><span class=\"line\">  bind 0.0.0.0</span><br><span class=\"line\">&lt;/source&gt;</span><br><span class=\"line\">&lt;match *.**&gt;</span><br><span class=\"line\">  @type copy</span><br><span class=\"line\">  &lt;store&gt;</span><br><span class=\"line\">    @type elasticsearch</span><br><span class=\"line\">    host elasticsearch</span><br><span class=\"line\">    port 9200</span><br><span class=\"line\">    logstash_format true</span><br><span class=\"line\">    logstash_prefix fluentd</span><br><span class=\"line\">    logstash_dateformat %Y%m%d</span><br><span class=\"line\">    include_tag_key true</span><br><span class=\"line\">    type_name access_log</span><br><span class=\"line\">    tag_key @log_name</span><br><span class=\"line\">    flush_interval 1s</span><br><span class=\"line\">  &lt;/store&gt;</span><br><span class=\"line\">  &lt;store&gt;</span><br><span class=\"line\">    @type stdout</span><br><span class=\"line\">  &lt;/store&gt;</span><br><span class=\"line\">&lt;/match&gt;</span><br></pre></td></tr></table></figure></p>\n<p>详细配置文件语法：<a href=\"https://docs.fluentd.org/v1.0/articles/config-file\" target=\"_blank\" rel=\"noopener\">https://docs.fluentd.org/v1.0/articles/config-file</a></p>\n<h2 id=\"部署efk-elasticsearch-fluent-kibana\"><a href=\"#部署efk-elasticsearch-fluent-kibana\" class=\"headerlink\" title=\"部署efk(elasticsearch fluent kibana)\"></a>部署efk(elasticsearch fluent kibana)</h2><p>docker-compose.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3&apos;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  fluentd:</span><br><span class=\"line\">    image: fluent-plugin-elastic:1.9.2</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      #挂载fluent配置文件</span><br><span class=\"line\">      - ./fluent/conf:/fluentd/etc</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      #配置elasticsearch主机端口</span><br><span class=\"line\">      - &quot;ELASTICSEARCH_URL=http://172.19.0.1:9200&quot;</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;24224:24224&quot;</span><br><span class=\"line\">      - &quot;24224:24224/udp&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  elasticsearch:</span><br><span class=\"line\">    image: elasticsearch:5.6.9-alpine</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - ./elasticsearch:/usr/share/elasticsearch/data</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;9200:9200&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  kibana:\t</span><br><span class=\"line\">    image: kibana:5.6.9</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      - &quot;ELASTICSEARCH_URL=http://172.19.0.1:9200&quot;</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;5601:5601&quot;</span><br><span class=\"line\">    networks: </span><br><span class=\"line\">      - efk_default</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  efk_default:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<p>注意：ELASTICSEARCH_URL环境变量参数，可以配置成宿主机内网ip或者其他elasticsearch主机端口</p>\n<p>部署<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker stack deploy -c docker-compose.yaml efk</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"在项目中使用fluentd收集日志\"><a href=\"#在项目中使用fluentd收集日志\" class=\"headerlink\" title=\"在项目中使用fluentd收集日志\"></a>在项目中使用fluentd收集日志</h2><p>以Java springboot项目为例子<br>添加maven依赖<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.fluentd&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;fluent-logger&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;0.3.3&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.sndyuk&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;logback-more-appenders&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.4.3&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n<p>增加logback-spring.xml配置文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class=\"line\">&lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;true&quot;</span><br><span class=\"line\">\txmlns=&quot;http://ch.qos.logback/xml/ns/logback&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class=\"line\">\txsi:schemaLocation=&quot;http://ch.qos.logback/xml/ns/logback http://ch.qos.logback/xml/ns/logback/logback.xsd&quot;&gt;</span><br><span class=\"line\">\t&lt;property name=&quot;fileName&quot; value=&quot;maxfunSpider-console&quot; /&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &lt;!-- $&#123;FLUENTD_HOST: -xx.xx.xx.xx 意思是从配置文件获取FLUENTD_HOST，如果没有就用默认值--&gt;</span><br><span class=\"line\">    &lt;property name=&quot;FLUENTD_HOST&quot; value=&quot;$&#123;FLUENTD_HOST:-服务器IP&#125;&quot; /&gt;</span><br><span class=\"line\">    &lt;property name=&quot;FLUENTD_PORT&quot; value=&quot;$&#123;FLUENTD_PORT:-24224&#125;&quot; /&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">\t&lt;include resource=&quot;org/springframework/boot/logging/logback/base.xml&quot; /&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">\t&lt;appender name=&quot;FILE&quot;</span><br><span class=\"line\">\t\tclass=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class=\"line\">\t\t&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class=\"line\">\t\t\t&lt;fileNamePattern&gt;./log/$&#123;fileName&#125;.%d&#123;yyyy-MM-dd&#125;.log</span><br><span class=\"line\">\t\t\t&lt;/fileNamePattern&gt;</span><br><span class=\"line\">\t\t\t&lt;maxHistory&gt;30&lt;/maxHistory&gt;</span><br><span class=\"line\">\t\t&lt;/rollingPolicy&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&lt;encoder&gt;</span><br><span class=\"line\">\t\t\t&lt;pattern&gt;$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern&gt;</span><br><span class=\"line\">\t\t&lt;/encoder&gt;</span><br><span class=\"line\">\t&lt;/appender&gt;    </span><br><span class=\"line\">    </span><br><span class=\"line\">    &lt;springProfile name=&quot;dev&quot;&gt;</span><br><span class=\"line\">        &lt;!-- 测试环境日志级别 --&gt;</span><br><span class=\"line\">        &lt;property name=&quot;loggerLevel&quot; value=&quot;INFO&quot; /&gt;</span><br><span class=\"line\">        &lt;appender name=&quot;FLUENT&quot; class=&quot;ch.qos.logback.more.appenders.DataFluentAppender&quot;&gt;</span><br><span class=\"line\">            &lt;tag&gt;maxfunCrawler&lt;/tag&gt;</span><br><span class=\"line\">            &lt;label&gt;dev&lt;/label&gt;</span><br><span class=\"line\">            &lt;remoteHost&gt;$&#123;FLUENTD_HOST&#125;&lt;/remoteHost&gt;</span><br><span class=\"line\">            &lt;port&gt;$&#123;FLUENTD_PORT&#125;&lt;/port&gt;</span><br><span class=\"line\">        &lt;/appender&gt;</span><br><span class=\"line\">    &lt;/springProfile&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &lt;springProfile name=&quot;prod&quot;&gt;</span><br><span class=\"line\">        &lt;!-- 生成环境日志级别 --&gt;</span><br><span class=\"line\">        &lt;property name=&quot;loggerLevel&quot; value=&quot;ERROR&quot; /&gt;</span><br><span class=\"line\">        &lt;appender name=&quot;FLUENT&quot; class=&quot;ch.qos.logback.more.appenders.DataFluentAppender&quot;&gt;</span><br><span class=\"line\">            &lt;tag&gt;maxfunCrawler&lt;/tag&gt;</span><br><span class=\"line\">            &lt;label&gt;prod&lt;/label&gt;</span><br><span class=\"line\">            &lt;remoteHost&gt;$&#123;FLUENTD_HOST&#125;&lt;/remoteHost&gt;</span><br><span class=\"line\">            &lt;port&gt;$&#123;FLUENTD_PORT&#125;&lt;/port&gt;</span><br><span class=\"line\">        &lt;/appender&gt;</span><br><span class=\"line\">    &lt;/springProfile&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &lt;root level=&quot;$&#123;loggerLevel&#125;&quot;&gt;</span><br><span class=\"line\">\t\t&lt;appender-ref ref=&quot;FILE&quot; /&gt;</span><br><span class=\"line\">\t\t&lt;appender-ref ref=&quot;FLUENT&quot; /&gt;</span><br><span class=\"line\">\t&lt;/root&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>\n<p>启动springboot项目，不出意外的话，在Kibana就可以看到收集到的日志了</p>\n<h2 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h2><p>部署成功后，访问ip:5601浏览Kibana页面了，第一次打开是如下界面。<br><img src=\"https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana.png\" alt=\"avatar\"></p>\n<p>把Index pattern改成fluentd-*，点Create。如果elasticsearch已经有日志的话，会显示出来，没有的就是这样子的<br><img src=\"https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana-noresult.png\" alt=\"avatar\"></p>\n<p>收集到的日志<br><img src=\"https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana-log.png\" alt=\"avatar\"></p>\n<h2 id=\"增加http验证\"><a href=\"#增加http验证\" class=\"headerlink\" title=\"增加http验证\"></a>增加http验证</h2><p>到处就可以用Kibana来分析收集到的日志了，目前任何人都访问kibana，没有设置登录验证，不安全，可以配合treafik做个简单的http auth验证。</p>\n<h3 id=\"配置部署trarfik\"><a href=\"#配置部署trarfik\" class=\"headerlink\" title=\"配置部署trarfik\"></a>配置部署trarfik</h3><p>配置<br>trarfik.toml来自<a href=\"https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml\" target=\"_blank\" rel=\"noopener\">https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml</a><br>把entryPoints改成下面这样。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[entryPoints]</span><br><span class=\"line\">    [entryPoints.http]</span><br><span class=\"line\">    address = &quot;:80&quot;</span><br><span class=\"line\">    [entryPoints.http.auth.basic]</span><br><span class=\"line\">    #可以使用在线的htpasswd生成</span><br><span class=\"line\">    users = [&quot;test:$apr1$CMMXhsE1$ltlr0YMcFYV16nKB.L9Ah0&quot;]</span><br></pre></td></tr></table></figure></p>\n<p>traefik.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ersion: &quot;3&quot;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  traefik:</span><br><span class=\"line\">    image: traefik</span><br><span class=\"line\">    #--docker.domain可以根据自己需求修改</span><br><span class=\"line\">    command: --docker --docker.swarmmode  --docker.domain=kevin.co --docker.watch  --web --api --logLevel=DEBUG</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      #这台机器的80端口被占用了，所以用81端口。</span><br><span class=\"line\">      - &quot;81:80&quot;</span><br><span class=\"line\">      #traefik dashboard端口</span><br><span class=\"line\">      - &quot;8080:8080&quot;</span><br><span class=\"line\">      - &quot;443:443&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      placement:</span><br><span class=\"line\">        constraints:</span><br><span class=\"line\">        - node.role == manager</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - /var/run/docker.sock:/var/run/docker.sock</span><br><span class=\"line\">      #挂载配置文件</span><br><span class=\"line\">      - $PWD/traefik.toml:/etc/traefik/traefik.toml</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">    #使用跟kibana同样的网络</span><br><span class=\"line\">    - efk_default</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  efk_default:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<p>部署traefik<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker stack deploy -c traefik.yaml efk</span><br></pre></td></tr></table></figure></p>\n<p>把kibana的docker-compose稍微改一下，增加deploy属性，加入相关traefik label。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kibana:\t</span><br><span class=\"line\">  image: kibana:5.6.9</span><br><span class=\"line\">  #links:</span><br><span class=\"line\">  #  - &quot;elasticsearch&quot;</span><br><span class=\"line\">  environment:</span><br><span class=\"line\">    - &quot;ELASTICSEARCH_URL=http://172.19.0.1:9200&quot;</span><br><span class=\"line\">  #ports:</span><br><span class=\"line\">  #  - &quot;5601:5601&quot;</span><br><span class=\"line\">  deploy:</span><br><span class=\"line\">    labels:</span><br><span class=\"line\">      - &quot;traefik.port=5601&quot;</span><br><span class=\"line\">      - &quot;traefik.docker.network=efk_default&quot;</span><br><span class=\"line\">      - &quot;traefik.backend=kibana&quot;</span><br><span class=\"line\">      - &quot;traefik.frontend.rule=Host:kibana.kevin.co&quot;</span><br><span class=\"line\">  networks: </span><br><span class=\"line\">    - efk_default</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"重新部署kibana\"><a href=\"#重新部署kibana\" class=\"headerlink\" title=\"重新部署kibana\"></a>重新部署kibana</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker service rm efk_kibana</span><br><span class=\"line\"># docker stack deploy -c docker-compose.yaml efk</span><br></pre></td></tr></table></figure>\n<p>kibana.kevin.co域名是没有经过dns解析的的所以要修改hosts文件指向kibana.kevin.co域名<br>修改完后，就可以使用域名加端口访问Kinaba了，这时候就需要输入配置好的用户密码登录才能正常访问，否则会报401错误</p>\n<p>完。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>分布式应用，部署在Docker中，日志分散在各个服务器上，为了方便查看日志，<br>在应用中使用Fluentd把日志收集起来，存放到Elasticsearch中，使用Kibana来查看日志，分析等</p>\n<h2 id=\"编译配置Fluentd\"><a href=\"#编译配置Fluentd\" class=\"headerlink\" title=\"编译配置Fluentd\"></a>编译配置Fluentd</h2><p>Fluentd配合elasticsearch使用需要安装fluent-plugin-elasticsearch<br>下面来构建Dokerfile来编译一个带elasticsearch插件的Fluentd</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM fluent/fluentd:v0.12-debian</span><br><span class=\"line\">RUN [&quot;gem&quot;, &quot;install&quot;, &quot;fluent-plugin-elasticsearch&quot;, &quot;--no-rdoc&quot;, &quot;--no-ri&quot;, &quot;--version&quot;, &quot;1.9.2&quot;]</span><br></pre></td></tr></table></figure>\n<p>编译镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker build -t fluent-plugin-elastic:1.9.2 ./</span><br></pre></td></tr></table></figure></p>\n<p>准备好fluent配置文件<br>fluentd/conf/fluent.conf<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># fluentd/conf/fluent.conf</span><br><span class=\"line\">&lt;source&gt;</span><br><span class=\"line\">  @type forward</span><br><span class=\"line\">  port 24224</span><br><span class=\"line\">  bind 0.0.0.0</span><br><span class=\"line\">&lt;/source&gt;</span><br><span class=\"line\">&lt;match *.**&gt;</span><br><span class=\"line\">  @type copy</span><br><span class=\"line\">  &lt;store&gt;</span><br><span class=\"line\">    @type elasticsearch</span><br><span class=\"line\">    host elasticsearch</span><br><span class=\"line\">    port 9200</span><br><span class=\"line\">    logstash_format true</span><br><span class=\"line\">    logstash_prefix fluentd</span><br><span class=\"line\">    logstash_dateformat %Y%m%d</span><br><span class=\"line\">    include_tag_key true</span><br><span class=\"line\">    type_name access_log</span><br><span class=\"line\">    tag_key @log_name</span><br><span class=\"line\">    flush_interval 1s</span><br><span class=\"line\">  &lt;/store&gt;</span><br><span class=\"line\">  &lt;store&gt;</span><br><span class=\"line\">    @type stdout</span><br><span class=\"line\">  &lt;/store&gt;</span><br><span class=\"line\">&lt;/match&gt;</span><br></pre></td></tr></table></figure></p>\n<p>详细配置文件语法：<a href=\"https://docs.fluentd.org/v1.0/articles/config-file\" target=\"_blank\" rel=\"noopener\">https://docs.fluentd.org/v1.0/articles/config-file</a></p>\n<h2 id=\"部署efk-elasticsearch-fluent-kibana\"><a href=\"#部署efk-elasticsearch-fluent-kibana\" class=\"headerlink\" title=\"部署efk(elasticsearch fluent kibana)\"></a>部署efk(elasticsearch fluent kibana)</h2><p>docker-compose.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3&apos;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  fluentd:</span><br><span class=\"line\">    image: fluent-plugin-elastic:1.9.2</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      #挂载fluent配置文件</span><br><span class=\"line\">      - ./fluent/conf:/fluentd/etc</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      #配置elasticsearch主机端口</span><br><span class=\"line\">      - &quot;ELASTICSEARCH_URL=http://172.19.0.1:9200&quot;</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;24224:24224&quot;</span><br><span class=\"line\">      - &quot;24224:24224/udp&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  elasticsearch:</span><br><span class=\"line\">    image: elasticsearch:5.6.9-alpine</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - ./elasticsearch:/usr/share/elasticsearch/data</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;9200:9200&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  kibana:\t</span><br><span class=\"line\">    image: kibana:5.6.9</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      - &quot;ELASTICSEARCH_URL=http://172.19.0.1:9200&quot;</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;5601:5601&quot;</span><br><span class=\"line\">    networks: </span><br><span class=\"line\">      - efk_default</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  efk_default:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<p>注意：ELASTICSEARCH_URL环境变量参数，可以配置成宿主机内网ip或者其他elasticsearch主机端口</p>\n<p>部署<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker stack deploy -c docker-compose.yaml efk</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"在项目中使用fluentd收集日志\"><a href=\"#在项目中使用fluentd收集日志\" class=\"headerlink\" title=\"在项目中使用fluentd收集日志\"></a>在项目中使用fluentd收集日志</h2><p>以Java springboot项目为例子<br>添加maven依赖<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.fluentd&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;fluent-logger&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;0.3.3&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.sndyuk&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;logback-more-appenders&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.4.3&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n<p>增加logback-spring.xml配置文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class=\"line\">&lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;true&quot;</span><br><span class=\"line\">\txmlns=&quot;http://ch.qos.logback/xml/ns/logback&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class=\"line\">\txsi:schemaLocation=&quot;http://ch.qos.logback/xml/ns/logback http://ch.qos.logback/xml/ns/logback/logback.xsd&quot;&gt;</span><br><span class=\"line\">\t&lt;property name=&quot;fileName&quot; value=&quot;maxfunSpider-console&quot; /&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &lt;!-- $&#123;FLUENTD_HOST: -xx.xx.xx.xx 意思是从配置文件获取FLUENTD_HOST，如果没有就用默认值--&gt;</span><br><span class=\"line\">    &lt;property name=&quot;FLUENTD_HOST&quot; value=&quot;$&#123;FLUENTD_HOST:-服务器IP&#125;&quot; /&gt;</span><br><span class=\"line\">    &lt;property name=&quot;FLUENTD_PORT&quot; value=&quot;$&#123;FLUENTD_PORT:-24224&#125;&quot; /&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">\t&lt;include resource=&quot;org/springframework/boot/logging/logback/base.xml&quot; /&gt;</span><br><span class=\"line\">  </span><br><span class=\"line\">\t&lt;appender name=&quot;FILE&quot;</span><br><span class=\"line\">\t\tclass=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class=\"line\">\t\t&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class=\"line\">\t\t\t&lt;fileNamePattern&gt;./log/$&#123;fileName&#125;.%d&#123;yyyy-MM-dd&#125;.log</span><br><span class=\"line\">\t\t\t&lt;/fileNamePattern&gt;</span><br><span class=\"line\">\t\t\t&lt;maxHistory&gt;30&lt;/maxHistory&gt;</span><br><span class=\"line\">\t\t&lt;/rollingPolicy&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&lt;encoder&gt;</span><br><span class=\"line\">\t\t\t&lt;pattern&gt;$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern&gt;</span><br><span class=\"line\">\t\t&lt;/encoder&gt;</span><br><span class=\"line\">\t&lt;/appender&gt;    </span><br><span class=\"line\">    </span><br><span class=\"line\">    &lt;springProfile name=&quot;dev&quot;&gt;</span><br><span class=\"line\">        &lt;!-- 测试环境日志级别 --&gt;</span><br><span class=\"line\">        &lt;property name=&quot;loggerLevel&quot; value=&quot;INFO&quot; /&gt;</span><br><span class=\"line\">        &lt;appender name=&quot;FLUENT&quot; class=&quot;ch.qos.logback.more.appenders.DataFluentAppender&quot;&gt;</span><br><span class=\"line\">            &lt;tag&gt;maxfunCrawler&lt;/tag&gt;</span><br><span class=\"line\">            &lt;label&gt;dev&lt;/label&gt;</span><br><span class=\"line\">            &lt;remoteHost&gt;$&#123;FLUENTD_HOST&#125;&lt;/remoteHost&gt;</span><br><span class=\"line\">            &lt;port&gt;$&#123;FLUENTD_PORT&#125;&lt;/port&gt;</span><br><span class=\"line\">        &lt;/appender&gt;</span><br><span class=\"line\">    &lt;/springProfile&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &lt;springProfile name=&quot;prod&quot;&gt;</span><br><span class=\"line\">        &lt;!-- 生成环境日志级别 --&gt;</span><br><span class=\"line\">        &lt;property name=&quot;loggerLevel&quot; value=&quot;ERROR&quot; /&gt;</span><br><span class=\"line\">        &lt;appender name=&quot;FLUENT&quot; class=&quot;ch.qos.logback.more.appenders.DataFluentAppender&quot;&gt;</span><br><span class=\"line\">            &lt;tag&gt;maxfunCrawler&lt;/tag&gt;</span><br><span class=\"line\">            &lt;label&gt;prod&lt;/label&gt;</span><br><span class=\"line\">            &lt;remoteHost&gt;$&#123;FLUENTD_HOST&#125;&lt;/remoteHost&gt;</span><br><span class=\"line\">            &lt;port&gt;$&#123;FLUENTD_PORT&#125;&lt;/port&gt;</span><br><span class=\"line\">        &lt;/appender&gt;</span><br><span class=\"line\">    &lt;/springProfile&gt;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &lt;root level=&quot;$&#123;loggerLevel&#125;&quot;&gt;</span><br><span class=\"line\">\t\t&lt;appender-ref ref=&quot;FILE&quot; /&gt;</span><br><span class=\"line\">\t\t&lt;appender-ref ref=&quot;FLUENT&quot; /&gt;</span><br><span class=\"line\">\t&lt;/root&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>\n<p>启动springboot项目，不出意外的话，在Kibana就可以看到收集到的日志了</p>\n<h2 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h2><p>部署成功后，访问ip:5601浏览Kibana页面了，第一次打开是如下界面。<br><img src=\"https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana.png\" alt=\"avatar\"></p>\n<p>把Index pattern改成fluentd-*，点Create。如果elasticsearch已经有日志的话，会显示出来，没有的就是这样子的<br><img src=\"https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana-noresult.png\" alt=\"avatar\"></p>\n<p>收集到的日志<br><img src=\"https://raw.githubusercontent.com/dongamp1990/blog/master/images/kibana-log.png\" alt=\"avatar\"></p>\n<h2 id=\"增加http验证\"><a href=\"#增加http验证\" class=\"headerlink\" title=\"增加http验证\"></a>增加http验证</h2><p>到处就可以用Kibana来分析收集到的日志了，目前任何人都访问kibana，没有设置登录验证，不安全，可以配合treafik做个简单的http auth验证。</p>\n<h3 id=\"配置部署trarfik\"><a href=\"#配置部署trarfik\" class=\"headerlink\" title=\"配置部署trarfik\"></a>配置部署trarfik</h3><p>配置<br>trarfik.toml来自<a href=\"https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml\" target=\"_blank\" rel=\"noopener\">https://raw.githubusercontent.com/containous/traefik/master/traefik.sample.toml</a><br>把entryPoints改成下面这样。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[entryPoints]</span><br><span class=\"line\">    [entryPoints.http]</span><br><span class=\"line\">    address = &quot;:80&quot;</span><br><span class=\"line\">    [entryPoints.http.auth.basic]</span><br><span class=\"line\">    #可以使用在线的htpasswd生成</span><br><span class=\"line\">    users = [&quot;test:$apr1$CMMXhsE1$ltlr0YMcFYV16nKB.L9Ah0&quot;]</span><br></pre></td></tr></table></figure></p>\n<p>traefik.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ersion: &quot;3&quot;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  traefik:</span><br><span class=\"line\">    image: traefik</span><br><span class=\"line\">    #--docker.domain可以根据自己需求修改</span><br><span class=\"line\">    command: --docker --docker.swarmmode  --docker.domain=kevin.co --docker.watch  --web --api --logLevel=DEBUG</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      #这台机器的80端口被占用了，所以用81端口。</span><br><span class=\"line\">      - &quot;81:80&quot;</span><br><span class=\"line\">      #traefik dashboard端口</span><br><span class=\"line\">      - &quot;8080:8080&quot;</span><br><span class=\"line\">      - &quot;443:443&quot;</span><br><span class=\"line\">    deploy:</span><br><span class=\"line\">      placement:</span><br><span class=\"line\">        constraints:</span><br><span class=\"line\">        - node.role == manager</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - /var/run/docker.sock:/var/run/docker.sock</span><br><span class=\"line\">      #挂载配置文件</span><br><span class=\"line\">      - $PWD/traefik.toml:/etc/traefik/traefik.toml</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">    #使用跟kibana同样的网络</span><br><span class=\"line\">    - efk_default</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  efk_default:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<p>部署traefik<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker stack deploy -c traefik.yaml efk</span><br></pre></td></tr></table></figure></p>\n<p>把kibana的docker-compose稍微改一下，增加deploy属性，加入相关traefik label。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kibana:\t</span><br><span class=\"line\">  image: kibana:5.6.9</span><br><span class=\"line\">  #links:</span><br><span class=\"line\">  #  - &quot;elasticsearch&quot;</span><br><span class=\"line\">  environment:</span><br><span class=\"line\">    - &quot;ELASTICSEARCH_URL=http://172.19.0.1:9200&quot;</span><br><span class=\"line\">  #ports:</span><br><span class=\"line\">  #  - &quot;5601:5601&quot;</span><br><span class=\"line\">  deploy:</span><br><span class=\"line\">    labels:</span><br><span class=\"line\">      - &quot;traefik.port=5601&quot;</span><br><span class=\"line\">      - &quot;traefik.docker.network=efk_default&quot;</span><br><span class=\"line\">      - &quot;traefik.backend=kibana&quot;</span><br><span class=\"line\">      - &quot;traefik.frontend.rule=Host:kibana.kevin.co&quot;</span><br><span class=\"line\">  networks: </span><br><span class=\"line\">    - efk_default</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"重新部署kibana\"><a href=\"#重新部署kibana\" class=\"headerlink\" title=\"重新部署kibana\"></a>重新部署kibana</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker service rm efk_kibana</span><br><span class=\"line\"># docker stack deploy -c docker-compose.yaml efk</span><br></pre></td></tr></table></figure>\n<p>kibana.kevin.co域名是没有经过dns解析的的所以要修改hosts文件指向kibana.kevin.co域名<br>修改完后，就可以使用域名加端口访问Kinaba了，这时候就需要输入配置好的用户密码登录才能正常访问，否则会报401错误</p>\n<p>完。</p>\n"},{"title":"Kubernetes ConfigMap使用","date":"2018-03-28T16:00:00.000Z","_content":"摘自：https://jimmysong.io/kubernetes-handbook/concepts/configmap.html\n## ConfigMap\n其实ConfigMap功能在Kubernetes1.2版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与docker image解耦，你总不能每修改一个配置就重做一个image吧？ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。\n\n## ConfigMap概览\nConfigMap API资源用来保存key-value pair配置数据，这个数据可以在pods里使用，或者被用来为像controller一样的系统组件存储配置数据。虽然ConfigMap跟Secrets类似，但是ConfigMap更方便的处理不含敏感信息的字符串。 注意：ConfigMaps不是属性配置文件的替代品。ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的/etc目录，专门用来存储配置文件的目录。下面举个例子，使用ConfigMap配置来创建Kuberntes Volumes，ConfigMap中的每个data项都会成为一个新文件。\n\n```\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  creationTimestamp: 2016-02-18T19:14:38Z\n  name: example-config\n  namespace: default\ndata:\n  example.property.1: hello\n  example.property.2: world\n  example.property.file: |-\n    property.1=value-1\n    property.2=value-2\n    property.3=value-3\n```\n\ndata一栏包括了配置数据，ConfigMap可以被用来保存单个属性，也可以用来保存一个配置文件。 配置数据可以通过很多种方式在Pods里被使用。ConfigMaps可以被用来：\n\n设置环境变量的值\n在容器里设置命令行参数\n在数据卷里面创建config文件\n用户和系统组件两者都可以在ConfigMap里面存储配置数据。\n\n其实不用看下面的文章，直接从kubectl create configmap -h的帮助信息中就可以对ConfigMap究竟如何创建略知一二了。\n\n```\nExamples:\n  # Create a new configmap named my-config based on folder bar\n  kubectl create configmap my-config --from-file=path/to/bar\n\n  # Create a new configmap named my-config with specified keys instead of file basenames on disk\n  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt\n\n  # Create a new configmap named my-config with key1=config1 and key2=config2\n  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2\n```\n\n## 创建ConfigMaps\n可以使用该命令，用给定值、文件或目录来创建ConfigMap。\n```\nkubectl create configmap\n```\n\n### 使用目录创建\n比如我们已经有个了包含一些配置文件，其中包含了我们想要设置的ConfigMap的值：\n```\n$ ls docs/user-guide/configmap/kubectl/\ngame.properties\nui.properties\n\n$ cat docs/user-guide/configmap/kubectl/game.properties\nenemies=aliens\nlives=3\nenemies.cheat=true\nenemies.cheat.level=noGoodRotten\nsecret.code.passphrase=UUDDLRLRBABAS\nsecret.code.allowed=true\nsecret.code.lives=30\n\n$ cat docs/user-guide/configmap/kubectl/ui.properties\ncolor.good=purple\ncolor.bad=yellow\nallow.textmode=true\nhow.nice.to.look=fairlyNice\n```\n使用下面的命令可以创建一个包含目录中所有文件的ConfigMap。\n\n```\n$ kubectl create configmap game-config --from-file=docs/user-guide/configmap/kubectl\n```\n—from-file指定在目录下的所有文件都会被用在ConfigMap里面创建一个键值对，键的名字就是文件名，值就是文件的内容。\n\n让我们来看一下这个命令创建的ConfigMap：\n```\n$ kubectl describe configmaps game-config\nName:           game-config\nNamespace:      default\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\ngame.properties:        158 bytes\nui.properties:          83 bytes\n```\n我们可以看到那两个key是从kubectl指定的目录中的文件名。这些key的内容可能会很大，所以在kubectl describe的输出中，只能够看到键的名字和他们的大小。 如果想要看到键的值的话，可以使用kubectl get：\n\n```\n$ kubectl get configmaps game-config -o yaml\n```\n### 使用文件创建\n刚才使用目录创建的时候我们—from-file指定的是一个目录，只要指定为一个文件就可以从单个文件中创建ConfigMap。\n\n```\n$ kubectl create configmap game-config-2 --from-file=docs/user-guide/configmap/kubectl/game.properties \n\n$ kubectl get configmaps game-config-2 -o yaml\n```\n—from-file这个参数可以使用多次，你可以使用两次分别指定上个实例中的那两个配置文件，效果就跟指定整个目录是一样的。\n\n### 使用字面值创建\n使用文字值创建，利用—from-literal参数传递配置信息，该参数可以使用多次，格式如下；\n\n```\n$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm\n\n$ kubectl get configmaps special-config -o yaml\n```\n\n## Pod中使用ConfigMap\n使用ConfigMap来替代环境变量\nConfigMap可以被用来填入环境变量。看下下面的ConfigMap。\n\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n  special.type: charm\n```\n\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: env-config\n  namespace: default\ndata:\n  log_level: INFO\n```\n我们可以在Pod中这样使用ConfigMap：\n### 使用ConfigMap来替代环境变量\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.how\n        - name: SPECIAL_TYPE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.type\n      envFrom:\n        - configMapRef:\n            name: env-config\n  restartPolicy: Never\n```\n这个Pod运行后会输出如下几行：\n```\nSPECIAL_LEVEL_KEY=very\nSPECIAL_TYPE_KEY=charm\nlog_level=INFO\n```\n### 用ConfigMap设置命令行参数\nConfigMap也可以被使用来设置容器中的命令或者参数值。它使用的是Kubernetes的$(VAR_NAME)替换语法。我们看下下面这个ConfigMap。\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n  special.type: charm\n```\n为了将ConfigMap中的值注入到命令行的参数里面，我们还要像前面那个例子一样使用环境变量替换语法${VAR_NAME)。（其实这个东西就是给Docker容器设置环境变量，以前我创建镜像的时候经常这么玩，通过docker run的时候指定-e参数修改镜像里的环境变量，然后docker的CMD命令再利用该$(VAR_NAME)通过sed来修改配置文件或者作为命令行启动参数。）\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\" ]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.how\n        - name: SPECIAL_TYPE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.type\n  restartPolicy: Never\n```\n\n### 通过数据卷插件使用ConfigMap\n在数据卷里面使用这个ConfigMap，有不同的选项。最基本的就是将文件填入数据卷，在这个文件中，键就是文件名，键值就是文件内容：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"cat /etc/config/special.how\" ]\n      volumeMounts:\n      - name: config-volume\n        mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: special-config\n  restartPolicy: Never\n```\n我们也可以在ConfigMap值被映射的数据卷里控制路径。\n使用前面创建的game-config\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: busybox\n      command: [ \"/bin/sh\",\"-c\",\"ls /etc/config\" ]\n      volumeMounts:\n      - name: config-volume\n        mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: game-config\n  restartPolicy: Never\n```\n\n查看pod日志里面打印出/etc/config目录下的文件\n```\n$ kubectl logs dapi-test-pod test-container\n\ngame.properties\nui.properties\n```","source":"_posts/kubernetes-configmap-usage.md","raw":"---\ntitle: Kubernetes ConfigMap使用\ntags: [kubernetes, configmap]\ndate: 2018-03-29\n---\n摘自：https://jimmysong.io/kubernetes-handbook/concepts/configmap.html\n## ConfigMap\n其实ConfigMap功能在Kubernetes1.2版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与docker image解耦，你总不能每修改一个配置就重做一个image吧？ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。\n\n## ConfigMap概览\nConfigMap API资源用来保存key-value pair配置数据，这个数据可以在pods里使用，或者被用来为像controller一样的系统组件存储配置数据。虽然ConfigMap跟Secrets类似，但是ConfigMap更方便的处理不含敏感信息的字符串。 注意：ConfigMaps不是属性配置文件的替代品。ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的/etc目录，专门用来存储配置文件的目录。下面举个例子，使用ConfigMap配置来创建Kuberntes Volumes，ConfigMap中的每个data项都会成为一个新文件。\n\n```\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  creationTimestamp: 2016-02-18T19:14:38Z\n  name: example-config\n  namespace: default\ndata:\n  example.property.1: hello\n  example.property.2: world\n  example.property.file: |-\n    property.1=value-1\n    property.2=value-2\n    property.3=value-3\n```\n\ndata一栏包括了配置数据，ConfigMap可以被用来保存单个属性，也可以用来保存一个配置文件。 配置数据可以通过很多种方式在Pods里被使用。ConfigMaps可以被用来：\n\n设置环境变量的值\n在容器里设置命令行参数\n在数据卷里面创建config文件\n用户和系统组件两者都可以在ConfigMap里面存储配置数据。\n\n其实不用看下面的文章，直接从kubectl create configmap -h的帮助信息中就可以对ConfigMap究竟如何创建略知一二了。\n\n```\nExamples:\n  # Create a new configmap named my-config based on folder bar\n  kubectl create configmap my-config --from-file=path/to/bar\n\n  # Create a new configmap named my-config with specified keys instead of file basenames on disk\n  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt\n\n  # Create a new configmap named my-config with key1=config1 and key2=config2\n  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2\n```\n\n## 创建ConfigMaps\n可以使用该命令，用给定值、文件或目录来创建ConfigMap。\n```\nkubectl create configmap\n```\n\n### 使用目录创建\n比如我们已经有个了包含一些配置文件，其中包含了我们想要设置的ConfigMap的值：\n```\n$ ls docs/user-guide/configmap/kubectl/\ngame.properties\nui.properties\n\n$ cat docs/user-guide/configmap/kubectl/game.properties\nenemies=aliens\nlives=3\nenemies.cheat=true\nenemies.cheat.level=noGoodRotten\nsecret.code.passphrase=UUDDLRLRBABAS\nsecret.code.allowed=true\nsecret.code.lives=30\n\n$ cat docs/user-guide/configmap/kubectl/ui.properties\ncolor.good=purple\ncolor.bad=yellow\nallow.textmode=true\nhow.nice.to.look=fairlyNice\n```\n使用下面的命令可以创建一个包含目录中所有文件的ConfigMap。\n\n```\n$ kubectl create configmap game-config --from-file=docs/user-guide/configmap/kubectl\n```\n—from-file指定在目录下的所有文件都会被用在ConfigMap里面创建一个键值对，键的名字就是文件名，值就是文件的内容。\n\n让我们来看一下这个命令创建的ConfigMap：\n```\n$ kubectl describe configmaps game-config\nName:           game-config\nNamespace:      default\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\ngame.properties:        158 bytes\nui.properties:          83 bytes\n```\n我们可以看到那两个key是从kubectl指定的目录中的文件名。这些key的内容可能会很大，所以在kubectl describe的输出中，只能够看到键的名字和他们的大小。 如果想要看到键的值的话，可以使用kubectl get：\n\n```\n$ kubectl get configmaps game-config -o yaml\n```\n### 使用文件创建\n刚才使用目录创建的时候我们—from-file指定的是一个目录，只要指定为一个文件就可以从单个文件中创建ConfigMap。\n\n```\n$ kubectl create configmap game-config-2 --from-file=docs/user-guide/configmap/kubectl/game.properties \n\n$ kubectl get configmaps game-config-2 -o yaml\n```\n—from-file这个参数可以使用多次，你可以使用两次分别指定上个实例中的那两个配置文件，效果就跟指定整个目录是一样的。\n\n### 使用字面值创建\n使用文字值创建，利用—from-literal参数传递配置信息，该参数可以使用多次，格式如下；\n\n```\n$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm\n\n$ kubectl get configmaps special-config -o yaml\n```\n\n## Pod中使用ConfigMap\n使用ConfigMap来替代环境变量\nConfigMap可以被用来填入环境变量。看下下面的ConfigMap。\n\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n  special.type: charm\n```\n\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: env-config\n  namespace: default\ndata:\n  log_level: INFO\n```\n我们可以在Pod中这样使用ConfigMap：\n### 使用ConfigMap来替代环境变量\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.how\n        - name: SPECIAL_TYPE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.type\n      envFrom:\n        - configMapRef:\n            name: env-config\n  restartPolicy: Never\n```\n这个Pod运行后会输出如下几行：\n```\nSPECIAL_LEVEL_KEY=very\nSPECIAL_TYPE_KEY=charm\nlog_level=INFO\n```\n### 用ConfigMap设置命令行参数\nConfigMap也可以被使用来设置容器中的命令或者参数值。它使用的是Kubernetes的$(VAR_NAME)替换语法。我们看下下面这个ConfigMap。\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n  special.type: charm\n```\n为了将ConfigMap中的值注入到命令行的参数里面，我们还要像前面那个例子一样使用环境变量替换语法${VAR_NAME)。（其实这个东西就是给Docker容器设置环境变量，以前我创建镜像的时候经常这么玩，通过docker run的时候指定-e参数修改镜像里的环境变量，然后docker的CMD命令再利用该$(VAR_NAME)通过sed来修改配置文件或者作为命令行启动参数。）\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\" ]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.how\n        - name: SPECIAL_TYPE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.type\n  restartPolicy: Never\n```\n\n### 通过数据卷插件使用ConfigMap\n在数据卷里面使用这个ConfigMap，有不同的选项。最基本的就是将文件填入数据卷，在这个文件中，键就是文件名，键值就是文件内容：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"cat /etc/config/special.how\" ]\n      volumeMounts:\n      - name: config-volume\n        mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: special-config\n  restartPolicy: Never\n```\n我们也可以在ConfigMap值被映射的数据卷里控制路径。\n使用前面创建的game-config\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: busybox\n      command: [ \"/bin/sh\",\"-c\",\"ls /etc/config\" ]\n      volumeMounts:\n      - name: config-volume\n        mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: game-config\n  restartPolicy: Never\n```\n\n查看pod日志里面打印出/etc/config目录下的文件\n```\n$ kubectl logs dapi-test-pod test-container\n\ngame.properties\nui.properties\n```","slug":"kubernetes-configmap-usage","published":1,"updated":"2018-05-23T02:14:24.373Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjncsum4t00087gvr6pgk9aux","content":"<p>摘自：<a href=\"https://jimmysong.io/kubernetes-handbook/concepts/configmap.html\" target=\"_blank\" rel=\"noopener\">https://jimmysong.io/kubernetes-handbook/concepts/configmap.html</a></p>\n<h2 id=\"ConfigMap\"><a href=\"#ConfigMap\" class=\"headerlink\" title=\"ConfigMap\"></a>ConfigMap</h2><p>其实ConfigMap功能在Kubernetes1.2版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与docker image解耦，你总不能每修改一个配置就重做一个image吧？ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。</p>\n<h2 id=\"ConfigMap概览\"><a href=\"#ConfigMap概览\" class=\"headerlink\" title=\"ConfigMap概览\"></a>ConfigMap概览</h2><p>ConfigMap API资源用来保存key-value pair配置数据，这个数据可以在pods里使用，或者被用来为像controller一样的系统组件存储配置数据。虽然ConfigMap跟Secrets类似，但是ConfigMap更方便的处理不含敏感信息的字符串。 注意：ConfigMaps不是属性配置文件的替代品。ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的/etc目录，专门用来存储配置文件的目录。下面举个例子，使用ConfigMap配置来创建Kuberntes Volumes，ConfigMap中的每个data项都会成为一个新文件。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  creationTimestamp: 2016-02-18T19:14:38Z</span><br><span class=\"line\">  name: example-config</span><br><span class=\"line\">  namespace: default</span><br><span class=\"line\">data:</span><br><span class=\"line\">  example.property.1: hello</span><br><span class=\"line\">  example.property.2: world</span><br><span class=\"line\">  example.property.file: |-</span><br><span class=\"line\">    property.1=value-1</span><br><span class=\"line\">    property.2=value-2</span><br><span class=\"line\">    property.3=value-3</span><br></pre></td></tr></table></figure>\n<p>data一栏包括了配置数据，ConfigMap可以被用来保存单个属性，也可以用来保存一个配置文件。 配置数据可以通过很多种方式在Pods里被使用。ConfigMaps可以被用来：</p>\n<p>设置环境变量的值<br>在容器里设置命令行参数<br>在数据卷里面创建config文件<br>用户和系统组件两者都可以在ConfigMap里面存储配置数据。</p>\n<p>其实不用看下面的文章，直接从kubectl create configmap -h的帮助信息中就可以对ConfigMap究竟如何创建略知一二了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Examples:</span><br><span class=\"line\">  # Create a new configmap named my-config based on folder bar</span><br><span class=\"line\">  kubectl create configmap my-config --from-file=path/to/bar</span><br><span class=\"line\"></span><br><span class=\"line\">  # Create a new configmap named my-config with specified keys instead of file basenames on disk</span><br><span class=\"line\">  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt</span><br><span class=\"line\"></span><br><span class=\"line\">  # Create a new configmap named my-config with key1=config1 and key2=config2</span><br><span class=\"line\">  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2</span><br></pre></td></tr></table></figure>\n<h2 id=\"创建ConfigMaps\"><a href=\"#创建ConfigMaps\" class=\"headerlink\" title=\"创建ConfigMaps\"></a>创建ConfigMaps</h2><p>可以使用该命令，用给定值、文件或目录来创建ConfigMap。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl create configmap</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用目录创建\"><a href=\"#使用目录创建\" class=\"headerlink\" title=\"使用目录创建\"></a>使用目录创建</h3><p>比如我们已经有个了包含一些配置文件，其中包含了我们想要设置的ConfigMap的值：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ls docs/user-guide/configmap/kubectl/</span><br><span class=\"line\">game.properties</span><br><span class=\"line\">ui.properties</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat docs/user-guide/configmap/kubectl/game.properties</span><br><span class=\"line\">enemies=aliens</span><br><span class=\"line\">lives=3</span><br><span class=\"line\">enemies.cheat=true</span><br><span class=\"line\">enemies.cheat.level=noGoodRotten</span><br><span class=\"line\">secret.code.passphrase=UUDDLRLRBABAS</span><br><span class=\"line\">secret.code.allowed=true</span><br><span class=\"line\">secret.code.lives=30</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat docs/user-guide/configmap/kubectl/ui.properties</span><br><span class=\"line\">color.good=purple</span><br><span class=\"line\">color.bad=yellow</span><br><span class=\"line\">allow.textmode=true</span><br><span class=\"line\">how.nice.to.look=fairlyNice</span><br></pre></td></tr></table></figure></p>\n<p>使用下面的命令可以创建一个包含目录中所有文件的ConfigMap。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl create configmap game-config --from-file=docs/user-guide/configmap/kubectl</span><br></pre></td></tr></table></figure>\n<p>—from-file指定在目录下的所有文件都会被用在ConfigMap里面创建一个键值对，键的名字就是文件名，值就是文件的内容。</p>\n<p>让我们来看一下这个命令创建的ConfigMap：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl describe configmaps game-config</span><br><span class=\"line\">Name:           game-config</span><br><span class=\"line\">Namespace:      default</span><br><span class=\"line\">Labels:         &lt;none&gt;</span><br><span class=\"line\">Annotations:    &lt;none&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">Data</span><br><span class=\"line\">====</span><br><span class=\"line\">game.properties:        158 bytes</span><br><span class=\"line\">ui.properties:          83 bytes</span><br></pre></td></tr></table></figure></p>\n<p>我们可以看到那两个key是从kubectl指定的目录中的文件名。这些key的内容可能会很大，所以在kubectl describe的输出中，只能够看到键的名字和他们的大小。 如果想要看到键的值的话，可以使用kubectl get：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get configmaps game-config -o yaml</span><br></pre></td></tr></table></figure>\n<h3 id=\"使用文件创建\"><a href=\"#使用文件创建\" class=\"headerlink\" title=\"使用文件创建\"></a>使用文件创建</h3><p>刚才使用目录创建的时候我们—from-file指定的是一个目录，只要指定为一个文件就可以从单个文件中创建ConfigMap。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl create configmap game-config-2 --from-file=docs/user-guide/configmap/kubectl/game.properties </span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get configmaps game-config-2 -o yaml</span><br></pre></td></tr></table></figure>\n<p>—from-file这个参数可以使用多次，你可以使用两次分别指定上个实例中的那两个配置文件，效果就跟指定整个目录是一样的。</p>\n<h3 id=\"使用字面值创建\"><a href=\"#使用字面值创建\" class=\"headerlink\" title=\"使用字面值创建\"></a>使用字面值创建</h3><p>使用文字值创建，利用—from-literal参数传递配置信息，该参数可以使用多次，格式如下；</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get configmaps special-config -o yaml</span><br></pre></td></tr></table></figure>\n<h2 id=\"Pod中使用ConfigMap\"><a href=\"#Pod中使用ConfigMap\" class=\"headerlink\" title=\"Pod中使用ConfigMap\"></a>Pod中使用ConfigMap</h2><p>使用ConfigMap来替代环境变量<br>ConfigMap可以被用来填入环境变量。看下下面的ConfigMap。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: special-config</span><br><span class=\"line\">  namespace: default</span><br><span class=\"line\">data:</span><br><span class=\"line\">  special.how: very</span><br><span class=\"line\">  special.type: charm</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: env-config</span><br><span class=\"line\">  namespace: default</span><br><span class=\"line\">data:</span><br><span class=\"line\">  log_level: INFO</span><br></pre></td></tr></table></figure>\n<p>我们可以在Pod中这样使用ConfigMap：</p>\n<h3 id=\"使用ConfigMap来替代环境变量\"><a href=\"#使用ConfigMap来替代环境变量\" class=\"headerlink\" title=\"使用ConfigMap来替代环境变量\"></a>使用ConfigMap来替代环境变量</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dapi-test-pod</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">    - name: test-container</span><br><span class=\"line\">      image: gcr.io/google_containers/busybox</span><br><span class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</span><br><span class=\"line\">      env:</span><br><span class=\"line\">        - name: SPECIAL_LEVEL_KEY</span><br><span class=\"line\">          valueFrom:</span><br><span class=\"line\">            configMapKeyRef:</span><br><span class=\"line\">              name: special-config</span><br><span class=\"line\">              key: special.how</span><br><span class=\"line\">        - name: SPECIAL_TYPE_KEY</span><br><span class=\"line\">          valueFrom:</span><br><span class=\"line\">            configMapKeyRef:</span><br><span class=\"line\">              name: special-config</span><br><span class=\"line\">              key: special.type</span><br><span class=\"line\">      envFrom:</span><br><span class=\"line\">        - configMapRef:</span><br><span class=\"line\">            name: env-config</span><br><span class=\"line\">  restartPolicy: Never</span><br></pre></td></tr></table></figure>\n<p>这个Pod运行后会输出如下几行：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SPECIAL_LEVEL_KEY=very</span><br><span class=\"line\">SPECIAL_TYPE_KEY=charm</span><br><span class=\"line\">log_level=INFO</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"用ConfigMap设置命令行参数\"><a href=\"#用ConfigMap设置命令行参数\" class=\"headerlink\" title=\"用ConfigMap设置命令行参数\"></a>用ConfigMap设置命令行参数</h3><p>ConfigMap也可以被使用来设置容器中的命令或者参数值。它使用的是Kubernetes的$(VAR_NAME)替换语法。我们看下下面这个ConfigMap。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: special-config</span><br><span class=\"line\">  namespace: default</span><br><span class=\"line\">data:</span><br><span class=\"line\">  special.how: very</span><br><span class=\"line\">  special.type: charm</span><br></pre></td></tr></table></figure></p>\n<p>为了将ConfigMap中的值注入到命令行的参数里面，我们还要像前面那个例子一样使用环境变量替换语法${VAR_NAME)。（其实这个东西就是给Docker容器设置环境变量，以前我创建镜像的时候经常这么玩，通过docker run的时候指定-e参数修改镜像里的环境变量，然后docker的CMD命令再利用该$(VAR_NAME)通过sed来修改配置文件或者作为命令行启动参数。）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dapi-test-pod</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">    - name: test-container</span><br><span class=\"line\">      image: gcr.io/google_containers/busybox</span><br><span class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&quot; ]</span><br><span class=\"line\">      env:</span><br><span class=\"line\">        - name: SPECIAL_LEVEL_KEY</span><br><span class=\"line\">          valueFrom:</span><br><span class=\"line\">            configMapKeyRef:</span><br><span class=\"line\">              name: special-config</span><br><span class=\"line\">              key: special.how</span><br><span class=\"line\">        - name: SPECIAL_TYPE_KEY</span><br><span class=\"line\">          valueFrom:</span><br><span class=\"line\">            configMapKeyRef:</span><br><span class=\"line\">              name: special-config</span><br><span class=\"line\">              key: special.type</span><br><span class=\"line\">  restartPolicy: Never</span><br></pre></td></tr></table></figure>\n<h3 id=\"通过数据卷插件使用ConfigMap\"><a href=\"#通过数据卷插件使用ConfigMap\" class=\"headerlink\" title=\"通过数据卷插件使用ConfigMap\"></a>通过数据卷插件使用ConfigMap</h3><p>在数据卷里面使用这个ConfigMap，有不同的选项。最基本的就是将文件填入数据卷，在这个文件中，键就是文件名，键值就是文件内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dapi-test-pod</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">    - name: test-container</span><br><span class=\"line\">      image: gcr.io/google_containers/busybox</span><br><span class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cat /etc/config/special.how&quot; ]</span><br><span class=\"line\">      volumeMounts:</span><br><span class=\"line\">      - name: config-volume</span><br><span class=\"line\">        mountPath: /etc/config</span><br><span class=\"line\">  volumes:</span><br><span class=\"line\">    - name: config-volume</span><br><span class=\"line\">      configMap:</span><br><span class=\"line\">        name: special-config</span><br><span class=\"line\">  restartPolicy: Never</span><br></pre></td></tr></table></figure></p>\n<p>我们也可以在ConfigMap值被映射的数据卷里控制路径。<br>使用前面创建的game-config<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dapi-test-pod</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">    - name: test-container</span><br><span class=\"line\">      image: busybox</span><br><span class=\"line\">      command: [ &quot;/bin/sh&quot;,&quot;-c&quot;,&quot;ls /etc/config&quot; ]</span><br><span class=\"line\">      volumeMounts:</span><br><span class=\"line\">      - name: config-volume</span><br><span class=\"line\">        mountPath: /etc/config</span><br><span class=\"line\">  volumes:</span><br><span class=\"line\">    - name: config-volume</span><br><span class=\"line\">      configMap:</span><br><span class=\"line\">        name: game-config</span><br><span class=\"line\">  restartPolicy: Never</span><br></pre></td></tr></table></figure></p>\n<p>查看pod日志里面打印出/etc/config目录下的文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl logs dapi-test-pod test-container</span><br><span class=\"line\"></span><br><span class=\"line\">game.properties</span><br><span class=\"line\">ui.properties</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<p>摘自：<a href=\"https://jimmysong.io/kubernetes-handbook/concepts/configmap.html\" target=\"_blank\" rel=\"noopener\">https://jimmysong.io/kubernetes-handbook/concepts/configmap.html</a></p>\n<h2 id=\"ConfigMap\"><a href=\"#ConfigMap\" class=\"headerlink\" title=\"ConfigMap\"></a>ConfigMap</h2><p>其实ConfigMap功能在Kubernetes1.2版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与docker image解耦，你总不能每修改一个配置就重做一个image吧？ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。</p>\n<h2 id=\"ConfigMap概览\"><a href=\"#ConfigMap概览\" class=\"headerlink\" title=\"ConfigMap概览\"></a>ConfigMap概览</h2><p>ConfigMap API资源用来保存key-value pair配置数据，这个数据可以在pods里使用，或者被用来为像controller一样的系统组件存储配置数据。虽然ConfigMap跟Secrets类似，但是ConfigMap更方便的处理不含敏感信息的字符串。 注意：ConfigMaps不是属性配置文件的替代品。ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的/etc目录，专门用来存储配置文件的目录。下面举个例子，使用ConfigMap配置来创建Kuberntes Volumes，ConfigMap中的每个data项都会成为一个新文件。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  creationTimestamp: 2016-02-18T19:14:38Z</span><br><span class=\"line\">  name: example-config</span><br><span class=\"line\">  namespace: default</span><br><span class=\"line\">data:</span><br><span class=\"line\">  example.property.1: hello</span><br><span class=\"line\">  example.property.2: world</span><br><span class=\"line\">  example.property.file: |-</span><br><span class=\"line\">    property.1=value-1</span><br><span class=\"line\">    property.2=value-2</span><br><span class=\"line\">    property.3=value-3</span><br></pre></td></tr></table></figure>\n<p>data一栏包括了配置数据，ConfigMap可以被用来保存单个属性，也可以用来保存一个配置文件。 配置数据可以通过很多种方式在Pods里被使用。ConfigMaps可以被用来：</p>\n<p>设置环境变量的值<br>在容器里设置命令行参数<br>在数据卷里面创建config文件<br>用户和系统组件两者都可以在ConfigMap里面存储配置数据。</p>\n<p>其实不用看下面的文章，直接从kubectl create configmap -h的帮助信息中就可以对ConfigMap究竟如何创建略知一二了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Examples:</span><br><span class=\"line\">  # Create a new configmap named my-config based on folder bar</span><br><span class=\"line\">  kubectl create configmap my-config --from-file=path/to/bar</span><br><span class=\"line\"></span><br><span class=\"line\">  # Create a new configmap named my-config with specified keys instead of file basenames on disk</span><br><span class=\"line\">  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt</span><br><span class=\"line\"></span><br><span class=\"line\">  # Create a new configmap named my-config with key1=config1 and key2=config2</span><br><span class=\"line\">  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2</span><br></pre></td></tr></table></figure>\n<h2 id=\"创建ConfigMaps\"><a href=\"#创建ConfigMaps\" class=\"headerlink\" title=\"创建ConfigMaps\"></a>创建ConfigMaps</h2><p>可以使用该命令，用给定值、文件或目录来创建ConfigMap。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl create configmap</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用目录创建\"><a href=\"#使用目录创建\" class=\"headerlink\" title=\"使用目录创建\"></a>使用目录创建</h3><p>比如我们已经有个了包含一些配置文件，其中包含了我们想要设置的ConfigMap的值：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ls docs/user-guide/configmap/kubectl/</span><br><span class=\"line\">game.properties</span><br><span class=\"line\">ui.properties</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat docs/user-guide/configmap/kubectl/game.properties</span><br><span class=\"line\">enemies=aliens</span><br><span class=\"line\">lives=3</span><br><span class=\"line\">enemies.cheat=true</span><br><span class=\"line\">enemies.cheat.level=noGoodRotten</span><br><span class=\"line\">secret.code.passphrase=UUDDLRLRBABAS</span><br><span class=\"line\">secret.code.allowed=true</span><br><span class=\"line\">secret.code.lives=30</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat docs/user-guide/configmap/kubectl/ui.properties</span><br><span class=\"line\">color.good=purple</span><br><span class=\"line\">color.bad=yellow</span><br><span class=\"line\">allow.textmode=true</span><br><span class=\"line\">how.nice.to.look=fairlyNice</span><br></pre></td></tr></table></figure></p>\n<p>使用下面的命令可以创建一个包含目录中所有文件的ConfigMap。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl create configmap game-config --from-file=docs/user-guide/configmap/kubectl</span><br></pre></td></tr></table></figure>\n<p>—from-file指定在目录下的所有文件都会被用在ConfigMap里面创建一个键值对，键的名字就是文件名，值就是文件的内容。</p>\n<p>让我们来看一下这个命令创建的ConfigMap：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl describe configmaps game-config</span><br><span class=\"line\">Name:           game-config</span><br><span class=\"line\">Namespace:      default</span><br><span class=\"line\">Labels:         &lt;none&gt;</span><br><span class=\"line\">Annotations:    &lt;none&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">Data</span><br><span class=\"line\">====</span><br><span class=\"line\">game.properties:        158 bytes</span><br><span class=\"line\">ui.properties:          83 bytes</span><br></pre></td></tr></table></figure></p>\n<p>我们可以看到那两个key是从kubectl指定的目录中的文件名。这些key的内容可能会很大，所以在kubectl describe的输出中，只能够看到键的名字和他们的大小。 如果想要看到键的值的话，可以使用kubectl get：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get configmaps game-config -o yaml</span><br></pre></td></tr></table></figure>\n<h3 id=\"使用文件创建\"><a href=\"#使用文件创建\" class=\"headerlink\" title=\"使用文件创建\"></a>使用文件创建</h3><p>刚才使用目录创建的时候我们—from-file指定的是一个目录，只要指定为一个文件就可以从单个文件中创建ConfigMap。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl create configmap game-config-2 --from-file=docs/user-guide/configmap/kubectl/game.properties </span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get configmaps game-config-2 -o yaml</span><br></pre></td></tr></table></figure>\n<p>—from-file这个参数可以使用多次，你可以使用两次分别指定上个实例中的那两个配置文件，效果就跟指定整个目录是一样的。</p>\n<h3 id=\"使用字面值创建\"><a href=\"#使用字面值创建\" class=\"headerlink\" title=\"使用字面值创建\"></a>使用字面值创建</h3><p>使用文字值创建，利用—from-literal参数传递配置信息，该参数可以使用多次，格式如下；</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get configmaps special-config -o yaml</span><br></pre></td></tr></table></figure>\n<h2 id=\"Pod中使用ConfigMap\"><a href=\"#Pod中使用ConfigMap\" class=\"headerlink\" title=\"Pod中使用ConfigMap\"></a>Pod中使用ConfigMap</h2><p>使用ConfigMap来替代环境变量<br>ConfigMap可以被用来填入环境变量。看下下面的ConfigMap。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: special-config</span><br><span class=\"line\">  namespace: default</span><br><span class=\"line\">data:</span><br><span class=\"line\">  special.how: very</span><br><span class=\"line\">  special.type: charm</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: env-config</span><br><span class=\"line\">  namespace: default</span><br><span class=\"line\">data:</span><br><span class=\"line\">  log_level: INFO</span><br></pre></td></tr></table></figure>\n<p>我们可以在Pod中这样使用ConfigMap：</p>\n<h3 id=\"使用ConfigMap来替代环境变量\"><a href=\"#使用ConfigMap来替代环境变量\" class=\"headerlink\" title=\"使用ConfigMap来替代环境变量\"></a>使用ConfigMap来替代环境变量</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dapi-test-pod</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">    - name: test-container</span><br><span class=\"line\">      image: gcr.io/google_containers/busybox</span><br><span class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</span><br><span class=\"line\">      env:</span><br><span class=\"line\">        - name: SPECIAL_LEVEL_KEY</span><br><span class=\"line\">          valueFrom:</span><br><span class=\"line\">            configMapKeyRef:</span><br><span class=\"line\">              name: special-config</span><br><span class=\"line\">              key: special.how</span><br><span class=\"line\">        - name: SPECIAL_TYPE_KEY</span><br><span class=\"line\">          valueFrom:</span><br><span class=\"line\">            configMapKeyRef:</span><br><span class=\"line\">              name: special-config</span><br><span class=\"line\">              key: special.type</span><br><span class=\"line\">      envFrom:</span><br><span class=\"line\">        - configMapRef:</span><br><span class=\"line\">            name: env-config</span><br><span class=\"line\">  restartPolicy: Never</span><br></pre></td></tr></table></figure>\n<p>这个Pod运行后会输出如下几行：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SPECIAL_LEVEL_KEY=very</span><br><span class=\"line\">SPECIAL_TYPE_KEY=charm</span><br><span class=\"line\">log_level=INFO</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"用ConfigMap设置命令行参数\"><a href=\"#用ConfigMap设置命令行参数\" class=\"headerlink\" title=\"用ConfigMap设置命令行参数\"></a>用ConfigMap设置命令行参数</h3><p>ConfigMap也可以被使用来设置容器中的命令或者参数值。它使用的是Kubernetes的$(VAR_NAME)替换语法。我们看下下面这个ConfigMap。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: special-config</span><br><span class=\"line\">  namespace: default</span><br><span class=\"line\">data:</span><br><span class=\"line\">  special.how: very</span><br><span class=\"line\">  special.type: charm</span><br></pre></td></tr></table></figure></p>\n<p>为了将ConfigMap中的值注入到命令行的参数里面，我们还要像前面那个例子一样使用环境变量替换语法${VAR_NAME)。（其实这个东西就是给Docker容器设置环境变量，以前我创建镜像的时候经常这么玩，通过docker run的时候指定-e参数修改镜像里的环境变量，然后docker的CMD命令再利用该$(VAR_NAME)通过sed来修改配置文件或者作为命令行启动参数。）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dapi-test-pod</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">    - name: test-container</span><br><span class=\"line\">      image: gcr.io/google_containers/busybox</span><br><span class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&quot; ]</span><br><span class=\"line\">      env:</span><br><span class=\"line\">        - name: SPECIAL_LEVEL_KEY</span><br><span class=\"line\">          valueFrom:</span><br><span class=\"line\">            configMapKeyRef:</span><br><span class=\"line\">              name: special-config</span><br><span class=\"line\">              key: special.how</span><br><span class=\"line\">        - name: SPECIAL_TYPE_KEY</span><br><span class=\"line\">          valueFrom:</span><br><span class=\"line\">            configMapKeyRef:</span><br><span class=\"line\">              name: special-config</span><br><span class=\"line\">              key: special.type</span><br><span class=\"line\">  restartPolicy: Never</span><br></pre></td></tr></table></figure>\n<h3 id=\"通过数据卷插件使用ConfigMap\"><a href=\"#通过数据卷插件使用ConfigMap\" class=\"headerlink\" title=\"通过数据卷插件使用ConfigMap\"></a>通过数据卷插件使用ConfigMap</h3><p>在数据卷里面使用这个ConfigMap，有不同的选项。最基本的就是将文件填入数据卷，在这个文件中，键就是文件名，键值就是文件内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dapi-test-pod</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">    - name: test-container</span><br><span class=\"line\">      image: gcr.io/google_containers/busybox</span><br><span class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cat /etc/config/special.how&quot; ]</span><br><span class=\"line\">      volumeMounts:</span><br><span class=\"line\">      - name: config-volume</span><br><span class=\"line\">        mountPath: /etc/config</span><br><span class=\"line\">  volumes:</span><br><span class=\"line\">    - name: config-volume</span><br><span class=\"line\">      configMap:</span><br><span class=\"line\">        name: special-config</span><br><span class=\"line\">  restartPolicy: Never</span><br></pre></td></tr></table></figure></p>\n<p>我们也可以在ConfigMap值被映射的数据卷里控制路径。<br>使用前面创建的game-config<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dapi-test-pod</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">    - name: test-container</span><br><span class=\"line\">      image: busybox</span><br><span class=\"line\">      command: [ &quot;/bin/sh&quot;,&quot;-c&quot;,&quot;ls /etc/config&quot; ]</span><br><span class=\"line\">      volumeMounts:</span><br><span class=\"line\">      - name: config-volume</span><br><span class=\"line\">        mountPath: /etc/config</span><br><span class=\"line\">  volumes:</span><br><span class=\"line\">    - name: config-volume</span><br><span class=\"line\">      configMap:</span><br><span class=\"line\">        name: game-config</span><br><span class=\"line\">  restartPolicy: Never</span><br></pre></td></tr></table></figure></p>\n<p>查看pod日志里面打印出/etc/config目录下的文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl logs dapi-test-pod test-container</span><br><span class=\"line\"></span><br><span class=\"line\">game.properties</span><br><span class=\"line\">ui.properties</span><br></pre></td></tr></table></figure></p>\n"},{"title":"使用Docker快速搭建Kafka集群","date":"2018-06-25T16:00:00.000Z","_content":"\n## 前言\n工作项目中使用到了MeessageQueue，原来是使用阿里云的MQ，是按使用量收费的，用了一段时间感觉数据量还是蛮大的，所以打算自己搭建消息队列服务器，先前测试过RabbitMQ，使用rabbitmq:3.7-management在docker环境下，消费者关掉后，容器CPU一直占用很高，不知什么情况，最后选择了Kafka，下面来讲下怎么快速搭建Kafka集群。\n\n## 准备工作\n两台服务器，\n服务器A(192.168.10.1)：部署zookeeper集群、kafka\n服务器B(192.168.10.2)：部署kafka\n\n各服务器创建一个kafka网络\n```\n$ docker network create kafka\n```\n\n[下载docker-compose](https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64), 复制到/usr/bin/下\n```\n$ wget -O docker-compose https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64\n$ mv ./docker-compose /usr/bin\n$ chmod +x /usr/bin/docker-compose\n$ docker-compose version\ndocker-compose version 1.21.2, build a133471\ndocker-py version: 3.3.0\nCPython version: 3.6.5\nOpenSSL version: OpenSSL 1.0.1t  3 May 2016\n\n```\n\n## 服务器A\n该服务器部署zookeeper，和kafka，为了方便zookeeper和kafka通信，指定了同一个docker network，\n同时kafka开启了JMX，方便使用kafka-manager做监控。\n\nzookeeper镜像使用说明：\nZOO_MY_ID参数指定zooid，每个zooid不能重复\nZOO_SERVERS指定其他zookeeper服务器地址\n更多说明请参阅https://hub.docker.com/_/zookeeper/\n\nkafka镜像使用说明：\nKAFKA_ADVERTISED_HOST_NAME 监听地址，指向服务器外网ip地址\nKAFKA_ZOOKEEPER_CONNECT zookeeper服务器ip:port多个使用,分隔\nKAFKA_JMX_OPTS jmx参数\nKAFKA_BROKER_ID brokerid,不填写会自动生成\nKAFKA_ADVERTISED_PORT 监听端口默认是9092，可以按需求修改\njava.rmi.server.hostname 需要指向服务器的ip\n更多说明请参阅https://hub.docker.com/r/wurstmeister/kafka/\n\n<font color='red'>注意: zookeeper版本和kafka zookeeper的版本要一致，之前试过使用zookeeper 3.5启动kafka 1.1.0会出现kafka will not attempt to authenticate using sasl错误</font>\nkafka-zookeeper.yaml文件\n```\nversion: '3.1'\n\nservices:\n  zoo1:\n    image: zookeeper:3.4\n    restart: always\n    hostname: zoo1\n    ports:\n      - 2181:2181\n    environment:\n      ZOO_MY_ID: 1\n      ZOO_SERVERS: server.1=0.0.0.0:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888\n    networks:\n      - kafka\n    restart: always\n  zoo2:\n    image: zookeeper:3.4\n    restart: always\n    hostname: zoo2\n    ports:\n      - 2182:2181\n    environment:\n      ZOO_MY_ID: 2\n      ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=0.0.0.0:2888:3888 server.3=zoo3:2888:3888\n    networks:\n      - kafka\n    restart: always  \n  zoo3:\n    image: zookeeper:3.4\n    restart: always\n    hostname: zoo3\n    ports:\n      - 2183:2181\n    environment:\n      ZOO_MY_ID: 3\n      ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=0.0.0.0:2888:3888\n    networks:\n      - kafka\n    restart: always\n  kafka:\n    image: wurstmeister/kafka:1.1.0\n    ports:\n      - \"9092:9092\"\n      - \"1099:1099\"\n    environment:\n      #指向服务器ip地址\n      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.1\n      KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2182,zoo3:2183\n      KAFKA_JMX_OPTS: \"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.1 -Dcom.sun.management.jmxremote.rmi.port=1099\"\n      KAFKA_ADVERTISED_PORT: 9092\n      KAFKA_BROKER_ID: 1\n      JMX_PORT: 1099\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /etc/localtime:/etc/localtime\n    networks:\n    - kafka\n    restart: always\nnetworks:\n  kafka:\n    external: true\n```\n### 部署\n```\n$ docker-compose -f kafka-zookeeper.yaml up -d\nCreating kafka_zoo2_1 ... done\nCreating kafka_zoo2_2 ... done\nCreating kafka_zoo2_3 ... done\nCreating kafka_kafka_1 ... done\n```\n\n## 服务器B\n\nkafka.yaml\n```\nversion: '3.1'\n\nservices:\n  kafka2:\n    image: wurstmeister/kafka:1.1.0\n    ports:\n      - \"9092:9092\"\n      - \"1099:1099\"\n    environment:\n      #指向服务器ip地址\n      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.2\n      KAFKA_ZOOKEEPER_CONNECT: 192.168.10.2:2181,192.168.10.2:2182,192.168.10.2:2183\n      KAFKA_JMX_OPTS: \"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.2 -Dcom.sun.management.jmxremote.rmi.port=1099\"\n      KAFKA_ADVERTISED_PORT: 9092\n      KAFKA_BROKER_ID: 2\n      JMX_PORT: 1099\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /etc/localtime:/etc/localtime\n    networks:\n    - kafka\n    restart: always\n  kafka3:\n    image: wurstmeister/kafka:1.1.0\n    ports:\n      - \"9093:9092\"\n      - \"2099:2099\"\n    environment:\n      #指向服务器ip地址\n      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.2\n      KAFKA_ZOOKEEPER_CONNECT: 192.168.10.2:2181,192.168.10.2:2182,192.168.10.2:2183\n      KAFKA_JMX_OPTS: \"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.2 -Dcom.sun.management.jmxremote.rmi.port=2099\"\n      KAFKA_ADVERTISED_PORT: 9093\n      KAFKA_BROKER_ID: 3\n      JMX_PORT: 2099\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /etc/localtime:/etc/localtime\n    networks:\n    - kafka\n    restart: always    \nnetworks:\n  kafka:\n    external: true\n```\n\n### 部署\n```\n$ docker-compose -f kafka.yaml up -d\nCreating kafka_kafka3_1 ... done\nCreating kafka_kafka2_1 ... done\n```\n\n## 部署kafka-manager\nyahoo开源的kafka监控管理工具https://github.com/yahoo/kafka-manager\n\n在服务器A上部署kafka-manager\n```\ndocker run -d \\\n     --name kafka-manager \\\n     -p 9000:9000  \\\n     --network kafka \\\n     -e ZK_HOSTS=\"zoo1:2181,zoo2:2181,zoo3:2183\" \\\n     hlebalbau/kafka-manager:latest \\\n     -Dpidfile.path=/dev/null\n```\n\n访问http://192.168.10.1:9000 就可以访问啦，怎么玩，稍微看看就知道了。\n\n## 项目集成kafka\n\nspring-boot版本:1.5.10.RELEASE\nspring-kafka版本:1.3.5.RELEASE\n\nhttps://docs.spring.io/spring-kafka/docs/1.3.5.RELEASE/reference/html/","source":"_posts/kafka-cluster.md","raw":"---\ntitle: 使用Docker快速搭建Kafka集群\ntags: [docker,kafka]\ndate: 2018-06-26\n---\n\n## 前言\n工作项目中使用到了MeessageQueue，原来是使用阿里云的MQ，是按使用量收费的，用了一段时间感觉数据量还是蛮大的，所以打算自己搭建消息队列服务器，先前测试过RabbitMQ，使用rabbitmq:3.7-management在docker环境下，消费者关掉后，容器CPU一直占用很高，不知什么情况，最后选择了Kafka，下面来讲下怎么快速搭建Kafka集群。\n\n## 准备工作\n两台服务器，\n服务器A(192.168.10.1)：部署zookeeper集群、kafka\n服务器B(192.168.10.2)：部署kafka\n\n各服务器创建一个kafka网络\n```\n$ docker network create kafka\n```\n\n[下载docker-compose](https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64), 复制到/usr/bin/下\n```\n$ wget -O docker-compose https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64\n$ mv ./docker-compose /usr/bin\n$ chmod +x /usr/bin/docker-compose\n$ docker-compose version\ndocker-compose version 1.21.2, build a133471\ndocker-py version: 3.3.0\nCPython version: 3.6.5\nOpenSSL version: OpenSSL 1.0.1t  3 May 2016\n\n```\n\n## 服务器A\n该服务器部署zookeeper，和kafka，为了方便zookeeper和kafka通信，指定了同一个docker network，\n同时kafka开启了JMX，方便使用kafka-manager做监控。\n\nzookeeper镜像使用说明：\nZOO_MY_ID参数指定zooid，每个zooid不能重复\nZOO_SERVERS指定其他zookeeper服务器地址\n更多说明请参阅https://hub.docker.com/_/zookeeper/\n\nkafka镜像使用说明：\nKAFKA_ADVERTISED_HOST_NAME 监听地址，指向服务器外网ip地址\nKAFKA_ZOOKEEPER_CONNECT zookeeper服务器ip:port多个使用,分隔\nKAFKA_JMX_OPTS jmx参数\nKAFKA_BROKER_ID brokerid,不填写会自动生成\nKAFKA_ADVERTISED_PORT 监听端口默认是9092，可以按需求修改\njava.rmi.server.hostname 需要指向服务器的ip\n更多说明请参阅https://hub.docker.com/r/wurstmeister/kafka/\n\n<font color='red'>注意: zookeeper版本和kafka zookeeper的版本要一致，之前试过使用zookeeper 3.5启动kafka 1.1.0会出现kafka will not attempt to authenticate using sasl错误</font>\nkafka-zookeeper.yaml文件\n```\nversion: '3.1'\n\nservices:\n  zoo1:\n    image: zookeeper:3.4\n    restart: always\n    hostname: zoo1\n    ports:\n      - 2181:2181\n    environment:\n      ZOO_MY_ID: 1\n      ZOO_SERVERS: server.1=0.0.0.0:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888\n    networks:\n      - kafka\n    restart: always\n  zoo2:\n    image: zookeeper:3.4\n    restart: always\n    hostname: zoo2\n    ports:\n      - 2182:2181\n    environment:\n      ZOO_MY_ID: 2\n      ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=0.0.0.0:2888:3888 server.3=zoo3:2888:3888\n    networks:\n      - kafka\n    restart: always  \n  zoo3:\n    image: zookeeper:3.4\n    restart: always\n    hostname: zoo3\n    ports:\n      - 2183:2181\n    environment:\n      ZOO_MY_ID: 3\n      ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=0.0.0.0:2888:3888\n    networks:\n      - kafka\n    restart: always\n  kafka:\n    image: wurstmeister/kafka:1.1.0\n    ports:\n      - \"9092:9092\"\n      - \"1099:1099\"\n    environment:\n      #指向服务器ip地址\n      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.1\n      KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2182,zoo3:2183\n      KAFKA_JMX_OPTS: \"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.1 -Dcom.sun.management.jmxremote.rmi.port=1099\"\n      KAFKA_ADVERTISED_PORT: 9092\n      KAFKA_BROKER_ID: 1\n      JMX_PORT: 1099\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /etc/localtime:/etc/localtime\n    networks:\n    - kafka\n    restart: always\nnetworks:\n  kafka:\n    external: true\n```\n### 部署\n```\n$ docker-compose -f kafka-zookeeper.yaml up -d\nCreating kafka_zoo2_1 ... done\nCreating kafka_zoo2_2 ... done\nCreating kafka_zoo2_3 ... done\nCreating kafka_kafka_1 ... done\n```\n\n## 服务器B\n\nkafka.yaml\n```\nversion: '3.1'\n\nservices:\n  kafka2:\n    image: wurstmeister/kafka:1.1.0\n    ports:\n      - \"9092:9092\"\n      - \"1099:1099\"\n    environment:\n      #指向服务器ip地址\n      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.2\n      KAFKA_ZOOKEEPER_CONNECT: 192.168.10.2:2181,192.168.10.2:2182,192.168.10.2:2183\n      KAFKA_JMX_OPTS: \"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.2 -Dcom.sun.management.jmxremote.rmi.port=1099\"\n      KAFKA_ADVERTISED_PORT: 9092\n      KAFKA_BROKER_ID: 2\n      JMX_PORT: 1099\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /etc/localtime:/etc/localtime\n    networks:\n    - kafka\n    restart: always\n  kafka3:\n    image: wurstmeister/kafka:1.1.0\n    ports:\n      - \"9093:9092\"\n      - \"2099:2099\"\n    environment:\n      #指向服务器ip地址\n      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.2\n      KAFKA_ZOOKEEPER_CONNECT: 192.168.10.2:2181,192.168.10.2:2182,192.168.10.2:2183\n      KAFKA_JMX_OPTS: \"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.2 -Dcom.sun.management.jmxremote.rmi.port=2099\"\n      KAFKA_ADVERTISED_PORT: 9093\n      KAFKA_BROKER_ID: 3\n      JMX_PORT: 2099\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /etc/localtime:/etc/localtime\n    networks:\n    - kafka\n    restart: always    \nnetworks:\n  kafka:\n    external: true\n```\n\n### 部署\n```\n$ docker-compose -f kafka.yaml up -d\nCreating kafka_kafka3_1 ... done\nCreating kafka_kafka2_1 ... done\n```\n\n## 部署kafka-manager\nyahoo开源的kafka监控管理工具https://github.com/yahoo/kafka-manager\n\n在服务器A上部署kafka-manager\n```\ndocker run -d \\\n     --name kafka-manager \\\n     -p 9000:9000  \\\n     --network kafka \\\n     -e ZK_HOSTS=\"zoo1:2181,zoo2:2181,zoo3:2183\" \\\n     hlebalbau/kafka-manager:latest \\\n     -Dpidfile.path=/dev/null\n```\n\n访问http://192.168.10.1:9000 就可以访问啦，怎么玩，稍微看看就知道了。\n\n## 项目集成kafka\n\nspring-boot版本:1.5.10.RELEASE\nspring-kafka版本:1.3.5.RELEASE\n\nhttps://docs.spring.io/spring-kafka/docs/1.3.5.RELEASE/reference/html/","slug":"kafka-cluster","published":1,"updated":"2018-06-26T08:46:32.135Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjncsum4v000a7gvrnddhgays","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>工作项目中使用到了MeessageQueue，原来是使用阿里云的MQ，是按使用量收费的，用了一段时间感觉数据量还是蛮大的，所以打算自己搭建消息队列服务器，先前测试过RabbitMQ，使用rabbitmq:3.7-management在docker环境下，消费者关掉后，容器CPU一直占用很高，不知什么情况，最后选择了Kafka，下面来讲下怎么快速搭建Kafka集群。</p>\n<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><p>两台服务器，<br>服务器A(192.168.10.1)：部署zookeeper集群、kafka<br>服务器B(192.168.10.2)：部署kafka</p>\n<p>各服务器创建一个kafka网络<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network create kafka</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64\" target=\"_blank\" rel=\"noopener\">下载docker-compose</a>, 复制到/usr/bin/下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ wget -O docker-compose https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64</span><br><span class=\"line\">$ mv ./docker-compose /usr/bin</span><br><span class=\"line\">$ chmod +x /usr/bin/docker-compose</span><br><span class=\"line\">$ docker-compose version</span><br><span class=\"line\">docker-compose version 1.21.2, build a133471</span><br><span class=\"line\">docker-py version: 3.3.0</span><br><span class=\"line\">CPython version: 3.6.5</span><br><span class=\"line\">OpenSSL version: OpenSSL 1.0.1t  3 May 2016</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"服务器A\"><a href=\"#服务器A\" class=\"headerlink\" title=\"服务器A\"></a>服务器A</h2><p>该服务器部署zookeeper，和kafka，为了方便zookeeper和kafka通信，指定了同一个docker network，<br>同时kafka开启了JMX，方便使用kafka-manager做监控。</p>\n<p>zookeeper镜像使用说明：<br>ZOO_MY_ID参数指定zooid，每个zooid不能重复<br>ZOO_SERVERS指定其他zookeeper服务器地址<br>更多说明请参阅<a href=\"https://hub.docker.com/_/zookeeper/\" target=\"_blank\" rel=\"noopener\">https://hub.docker.com/_/zookeeper/</a></p>\n<p>kafka镜像使用说明：<br>KAFKA_ADVERTISED_HOST_NAME 监听地址，指向服务器外网ip地址<br>KAFKA_ZOOKEEPER_CONNECT zookeeper服务器ip:port多个使用,分隔<br>KAFKA_JMX_OPTS jmx参数<br>KAFKA_BROKER_ID brokerid,不填写会自动生成<br>KAFKA_ADVERTISED_PORT 监听端口默认是9092，可以按需求修改<br>java.rmi.server.hostname 需要指向服务器的ip<br>更多说明请参阅<a href=\"https://hub.docker.com/r/wurstmeister/kafka/\" target=\"_blank\" rel=\"noopener\">https://hub.docker.com/r/wurstmeister/kafka/</a></p>\n<p><font color=\"red\">注意: zookeeper版本和kafka zookeeper的版本要一致，之前试过使用zookeeper 3.5启动kafka 1.1.0会出现kafka will not attempt to authenticate using sasl错误</font><br>kafka-zookeeper.yaml文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3.1&apos;</span><br><span class=\"line\"></span><br><span class=\"line\">services:</span><br><span class=\"line\">  zoo1:</span><br><span class=\"line\">    image: zookeeper:3.4</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">    hostname: zoo1</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - 2181:2181</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      ZOO_MY_ID: 1</span><br><span class=\"line\">      ZOO_SERVERS: server.1=0.0.0.0:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">  zoo2:</span><br><span class=\"line\">    image: zookeeper:3.4</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">    hostname: zoo2</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - 2182:2181</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      ZOO_MY_ID: 2</span><br><span class=\"line\">      ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=0.0.0.0:2888:3888 server.3=zoo3:2888:3888</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">    restart: always  </span><br><span class=\"line\">  zoo3:</span><br><span class=\"line\">    image: zookeeper:3.4</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">    hostname: zoo3</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - 2183:2181</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      ZOO_MY_ID: 3</span><br><span class=\"line\">      ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=0.0.0.0:2888:3888</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">  kafka:</span><br><span class=\"line\">    image: wurstmeister/kafka:1.1.0</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;9092:9092&quot;</span><br><span class=\"line\">      - &quot;1099:1099&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      #指向服务器ip地址</span><br><span class=\"line\">      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.1</span><br><span class=\"line\">      KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2182,zoo3:2183</span><br><span class=\"line\">      KAFKA_JMX_OPTS: &quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.1 -Dcom.sun.management.jmxremote.rmi.port=1099&quot;</span><br><span class=\"line\">      KAFKA_ADVERTISED_PORT: 9092</span><br><span class=\"line\">      KAFKA_BROKER_ID: 1</span><br><span class=\"line\">      JMX_PORT: 1099</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - /var/run/docker.sock:/var/run/docker.sock</span><br><span class=\"line\">      - /etc/localtime:/etc/localtime</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">    - kafka</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  kafka:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker-compose -f kafka-zookeeper.yaml up -d</span><br><span class=\"line\">Creating kafka_zoo2_1 ... done</span><br><span class=\"line\">Creating kafka_zoo2_2 ... done</span><br><span class=\"line\">Creating kafka_zoo2_3 ... done</span><br><span class=\"line\">Creating kafka_kafka_1 ... done</span><br></pre></td></tr></table></figure>\n<h2 id=\"服务器B\"><a href=\"#服务器B\" class=\"headerlink\" title=\"服务器B\"></a>服务器B</h2><p>kafka.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3.1&apos;</span><br><span class=\"line\"></span><br><span class=\"line\">services:</span><br><span class=\"line\">  kafka2:</span><br><span class=\"line\">    image: wurstmeister/kafka:1.1.0</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;9092:9092&quot;</span><br><span class=\"line\">      - &quot;1099:1099&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      #指向服务器ip地址</span><br><span class=\"line\">      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.2</span><br><span class=\"line\">      KAFKA_ZOOKEEPER_CONNECT: 192.168.10.2:2181,192.168.10.2:2182,192.168.10.2:2183</span><br><span class=\"line\">      KAFKA_JMX_OPTS: &quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.2 -Dcom.sun.management.jmxremote.rmi.port=1099&quot;</span><br><span class=\"line\">      KAFKA_ADVERTISED_PORT: 9092</span><br><span class=\"line\">      KAFKA_BROKER_ID: 2</span><br><span class=\"line\">      JMX_PORT: 1099</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - /var/run/docker.sock:/var/run/docker.sock</span><br><span class=\"line\">      - /etc/localtime:/etc/localtime</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">    - kafka</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">  kafka3:</span><br><span class=\"line\">    image: wurstmeister/kafka:1.1.0</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;9093:9092&quot;</span><br><span class=\"line\">      - &quot;2099:2099&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      #指向服务器ip地址</span><br><span class=\"line\">      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.2</span><br><span class=\"line\">      KAFKA_ZOOKEEPER_CONNECT: 192.168.10.2:2181,192.168.10.2:2182,192.168.10.2:2183</span><br><span class=\"line\">      KAFKA_JMX_OPTS: &quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.2 -Dcom.sun.management.jmxremote.rmi.port=2099&quot;</span><br><span class=\"line\">      KAFKA_ADVERTISED_PORT: 9093</span><br><span class=\"line\">      KAFKA_BROKER_ID: 3</span><br><span class=\"line\">      JMX_PORT: 2099</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - /var/run/docker.sock:/var/run/docker.sock</span><br><span class=\"line\">      - /etc/localtime:/etc/localtime</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">    - kafka</span><br><span class=\"line\">    restart: always    </span><br><span class=\"line\">networks:</span><br><span class=\"line\">  kafka:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"部署-1\"><a href=\"#部署-1\" class=\"headerlink\" title=\"部署\"></a>部署</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker-compose -f kafka.yaml up -d</span><br><span class=\"line\">Creating kafka_kafka3_1 ... done</span><br><span class=\"line\">Creating kafka_kafka2_1 ... done</span><br></pre></td></tr></table></figure>\n<h2 id=\"部署kafka-manager\"><a href=\"#部署kafka-manager\" class=\"headerlink\" title=\"部署kafka-manager\"></a>部署kafka-manager</h2><p>yahoo开源的kafka监控管理工具<a href=\"https://github.com/yahoo/kafka-manager\" target=\"_blank\" rel=\"noopener\">https://github.com/yahoo/kafka-manager</a></p>\n<p>在服务器A上部署kafka-manager<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d \\</span><br><span class=\"line\">     --name kafka-manager \\</span><br><span class=\"line\">     -p 9000:9000  \\</span><br><span class=\"line\">     --network kafka \\</span><br><span class=\"line\">     -e ZK_HOSTS=&quot;zoo1:2181,zoo2:2181,zoo3:2183&quot; \\</span><br><span class=\"line\">     hlebalbau/kafka-manager:latest \\</span><br><span class=\"line\">     -Dpidfile.path=/dev/null</span><br></pre></td></tr></table></figure></p>\n<p>访问<a href=\"http://192.168.10.1:9000\" target=\"_blank\" rel=\"noopener\">http://192.168.10.1:9000</a> 就可以访问啦，怎么玩，稍微看看就知道了。</p>\n<h2 id=\"项目集成kafka\"><a href=\"#项目集成kafka\" class=\"headerlink\" title=\"项目集成kafka\"></a>项目集成kafka</h2><p>spring-boot版本:1.5.10.RELEASE<br>spring-kafka版本:1.3.5.RELEASE</p>\n<p><a href=\"https://docs.spring.io/spring-kafka/docs/1.3.5.RELEASE/reference/html/\" target=\"_blank\" rel=\"noopener\">https://docs.spring.io/spring-kafka/docs/1.3.5.RELEASE/reference/html/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>工作项目中使用到了MeessageQueue，原来是使用阿里云的MQ，是按使用量收费的，用了一段时间感觉数据量还是蛮大的，所以打算自己搭建消息队列服务器，先前测试过RabbitMQ，使用rabbitmq:3.7-management在docker环境下，消费者关掉后，容器CPU一直占用很高，不知什么情况，最后选择了Kafka，下面来讲下怎么快速搭建Kafka集群。</p>\n<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><p>两台服务器，<br>服务器A(192.168.10.1)：部署zookeeper集群、kafka<br>服务器B(192.168.10.2)：部署kafka</p>\n<p>各服务器创建一个kafka网络<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network create kafka</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64\" target=\"_blank\" rel=\"noopener\">下载docker-compose</a>, 复制到/usr/bin/下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ wget -O docker-compose https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64</span><br><span class=\"line\">$ mv ./docker-compose /usr/bin</span><br><span class=\"line\">$ chmod +x /usr/bin/docker-compose</span><br><span class=\"line\">$ docker-compose version</span><br><span class=\"line\">docker-compose version 1.21.2, build a133471</span><br><span class=\"line\">docker-py version: 3.3.0</span><br><span class=\"line\">CPython version: 3.6.5</span><br><span class=\"line\">OpenSSL version: OpenSSL 1.0.1t  3 May 2016</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"服务器A\"><a href=\"#服务器A\" class=\"headerlink\" title=\"服务器A\"></a>服务器A</h2><p>该服务器部署zookeeper，和kafka，为了方便zookeeper和kafka通信，指定了同一个docker network，<br>同时kafka开启了JMX，方便使用kafka-manager做监控。</p>\n<p>zookeeper镜像使用说明：<br>ZOO_MY_ID参数指定zooid，每个zooid不能重复<br>ZOO_SERVERS指定其他zookeeper服务器地址<br>更多说明请参阅<a href=\"https://hub.docker.com/_/zookeeper/\" target=\"_blank\" rel=\"noopener\">https://hub.docker.com/_/zookeeper/</a></p>\n<p>kafka镜像使用说明：<br>KAFKA_ADVERTISED_HOST_NAME 监听地址，指向服务器外网ip地址<br>KAFKA_ZOOKEEPER_CONNECT zookeeper服务器ip:port多个使用,分隔<br>KAFKA_JMX_OPTS jmx参数<br>KAFKA_BROKER_ID brokerid,不填写会自动生成<br>KAFKA_ADVERTISED_PORT 监听端口默认是9092，可以按需求修改<br>java.rmi.server.hostname 需要指向服务器的ip<br>更多说明请参阅<a href=\"https://hub.docker.com/r/wurstmeister/kafka/\" target=\"_blank\" rel=\"noopener\">https://hub.docker.com/r/wurstmeister/kafka/</a></p>\n<p><font color=\"red\">注意: zookeeper版本和kafka zookeeper的版本要一致，之前试过使用zookeeper 3.5启动kafka 1.1.0会出现kafka will not attempt to authenticate using sasl错误</font><br>kafka-zookeeper.yaml文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3.1&apos;</span><br><span class=\"line\"></span><br><span class=\"line\">services:</span><br><span class=\"line\">  zoo1:</span><br><span class=\"line\">    image: zookeeper:3.4</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">    hostname: zoo1</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - 2181:2181</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      ZOO_MY_ID: 1</span><br><span class=\"line\">      ZOO_SERVERS: server.1=0.0.0.0:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">  zoo2:</span><br><span class=\"line\">    image: zookeeper:3.4</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">    hostname: zoo2</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - 2182:2181</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      ZOO_MY_ID: 2</span><br><span class=\"line\">      ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=0.0.0.0:2888:3888 server.3=zoo3:2888:3888</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">    restart: always  </span><br><span class=\"line\">  zoo3:</span><br><span class=\"line\">    image: zookeeper:3.4</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">    hostname: zoo3</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - 2183:2181</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      ZOO_MY_ID: 3</span><br><span class=\"line\">      ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=0.0.0.0:2888:3888</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">  kafka:</span><br><span class=\"line\">    image: wurstmeister/kafka:1.1.0</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;9092:9092&quot;</span><br><span class=\"line\">      - &quot;1099:1099&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      #指向服务器ip地址</span><br><span class=\"line\">      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.1</span><br><span class=\"line\">      KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2182,zoo3:2183</span><br><span class=\"line\">      KAFKA_JMX_OPTS: &quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.1 -Dcom.sun.management.jmxremote.rmi.port=1099&quot;</span><br><span class=\"line\">      KAFKA_ADVERTISED_PORT: 9092</span><br><span class=\"line\">      KAFKA_BROKER_ID: 1</span><br><span class=\"line\">      JMX_PORT: 1099</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - /var/run/docker.sock:/var/run/docker.sock</span><br><span class=\"line\">      - /etc/localtime:/etc/localtime</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">    - kafka</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">networks:</span><br><span class=\"line\">  kafka:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker-compose -f kafka-zookeeper.yaml up -d</span><br><span class=\"line\">Creating kafka_zoo2_1 ... done</span><br><span class=\"line\">Creating kafka_zoo2_2 ... done</span><br><span class=\"line\">Creating kafka_zoo2_3 ... done</span><br><span class=\"line\">Creating kafka_kafka_1 ... done</span><br></pre></td></tr></table></figure>\n<h2 id=\"服务器B\"><a href=\"#服务器B\" class=\"headerlink\" title=\"服务器B\"></a>服务器B</h2><p>kafka.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &apos;3.1&apos;</span><br><span class=\"line\"></span><br><span class=\"line\">services:</span><br><span class=\"line\">  kafka2:</span><br><span class=\"line\">    image: wurstmeister/kafka:1.1.0</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;9092:9092&quot;</span><br><span class=\"line\">      - &quot;1099:1099&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      #指向服务器ip地址</span><br><span class=\"line\">      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.2</span><br><span class=\"line\">      KAFKA_ZOOKEEPER_CONNECT: 192.168.10.2:2181,192.168.10.2:2182,192.168.10.2:2183</span><br><span class=\"line\">      KAFKA_JMX_OPTS: &quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.2 -Dcom.sun.management.jmxremote.rmi.port=1099&quot;</span><br><span class=\"line\">      KAFKA_ADVERTISED_PORT: 9092</span><br><span class=\"line\">      KAFKA_BROKER_ID: 2</span><br><span class=\"line\">      JMX_PORT: 1099</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - /var/run/docker.sock:/var/run/docker.sock</span><br><span class=\"line\">      - /etc/localtime:/etc/localtime</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">    - kafka</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">  kafka3:</span><br><span class=\"line\">    image: wurstmeister/kafka:1.1.0</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &quot;9093:9092&quot;</span><br><span class=\"line\">      - &quot;2099:2099&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      #指向服务器ip地址</span><br><span class=\"line\">      KAFKA_ADVERTISED_HOST_NAME: 192.168.10.2</span><br><span class=\"line\">      KAFKA_ZOOKEEPER_CONNECT: 192.168.10.2:2181,192.168.10.2:2182,192.168.10.2:2183</span><br><span class=\"line\">      KAFKA_JMX_OPTS: &quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.10.2 -Dcom.sun.management.jmxremote.rmi.port=2099&quot;</span><br><span class=\"line\">      KAFKA_ADVERTISED_PORT: 9093</span><br><span class=\"line\">      KAFKA_BROKER_ID: 3</span><br><span class=\"line\">      JMX_PORT: 2099</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">      - /var/run/docker.sock:/var/run/docker.sock</span><br><span class=\"line\">      - /etc/localtime:/etc/localtime</span><br><span class=\"line\">    networks:</span><br><span class=\"line\">    - kafka</span><br><span class=\"line\">    restart: always    </span><br><span class=\"line\">networks:</span><br><span class=\"line\">  kafka:</span><br><span class=\"line\">    external: true</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"部署-1\"><a href=\"#部署-1\" class=\"headerlink\" title=\"部署\"></a>部署</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker-compose -f kafka.yaml up -d</span><br><span class=\"line\">Creating kafka_kafka3_1 ... done</span><br><span class=\"line\">Creating kafka_kafka2_1 ... done</span><br></pre></td></tr></table></figure>\n<h2 id=\"部署kafka-manager\"><a href=\"#部署kafka-manager\" class=\"headerlink\" title=\"部署kafka-manager\"></a>部署kafka-manager</h2><p>yahoo开源的kafka监控管理工具<a href=\"https://github.com/yahoo/kafka-manager\" target=\"_blank\" rel=\"noopener\">https://github.com/yahoo/kafka-manager</a></p>\n<p>在服务器A上部署kafka-manager<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d \\</span><br><span class=\"line\">     --name kafka-manager \\</span><br><span class=\"line\">     -p 9000:9000  \\</span><br><span class=\"line\">     --network kafka \\</span><br><span class=\"line\">     -e ZK_HOSTS=&quot;zoo1:2181,zoo2:2181,zoo3:2183&quot; \\</span><br><span class=\"line\">     hlebalbau/kafka-manager:latest \\</span><br><span class=\"line\">     -Dpidfile.path=/dev/null</span><br></pre></td></tr></table></figure></p>\n<p>访问<a href=\"http://192.168.10.1:9000\" target=\"_blank\" rel=\"noopener\">http://192.168.10.1:9000</a> 就可以访问啦，怎么玩，稍微看看就知道了。</p>\n<h2 id=\"项目集成kafka\"><a href=\"#项目集成kafka\" class=\"headerlink\" title=\"项目集成kafka\"></a>项目集成kafka</h2><p>spring-boot版本:1.5.10.RELEASE<br>spring-kafka版本:1.3.5.RELEASE</p>\n<p><a href=\"https://docs.spring.io/spring-kafka/docs/1.3.5.RELEASE/reference/html/\" target=\"_blank\" rel=\"noopener\">https://docs.spring.io/spring-kafka/docs/1.3.5.RELEASE/reference/html/</a></p>\n"},{"title":"Kubernetes Java Client使用","date":"2018-03-28T16:00:00.000Z","_content":"## 前言\n使用Kubernetes 加Jenkins CICD，那么问题来了，Jenkins里面怎么部署Kubernetes应用呢。曾经在Jenkins上搜索过几个插件，其中Kubernetes Continuous Deploy没搞懂怎么用，咨询了一技术哥们说：本质就是配置认证， 调用k8s接口实现。在他的指点下，使用Kubernetes client去调用Kubernetes接口z\nk8s java client github地址：https://github.com/kubernetes-client/java。\n\n\n下面使用springboot + k8s client搭建的一个k8s-api项目\n\n## 创建接口调用token\n使用RBAC（基于角色的访问控制)调用接口，关于RBAC阅读以下文档\n官网文档：https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/\n中文翻译文档：https://jimmysong.io/kubernetes-handbook/concepts/rbac.html\n\n首先创建一个ServiceAccount，再绑定ClusterRole角色的cluster-admin权限，获得token，备client调接口用。\ndeployuser-token.yaml文件\n```\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: deploy-user\n annotations:\n  rbac.authorization.kubernetes.io/autoupdate: \"true\"\nroleRef:\n kind: ClusterRole\n name: cluster-admin\n apiGroup: rbac.authorization.k8s.io\nsubjects:\n - kind: ServiceAccount\n   name: deploy-user\n   namespace: kube-system\n   \n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n name: deploy-user\n namespace: kube-system\n labels:\n  kubernetes.io/cluster-service: \"true\"\n  addonmanager.kubernetes.io/mode: Reconcile\n```\n\n```\n$ kubectl apply -f cat deployuser-token.yaml\n```\n\n查看用户秘钥\n```\n$ kubectl get secret -n kube-system|grep deploy-user\nNAME                                     TYPE                                  DATA      AGE\ndeploy-user-token-5g6w6                  kubernetes.io/service-account-token   3         1d\n```\n\n#查看deploy-user账户秘钥详情\n```\n$ kubectl describe secret deploy-user-token-5g6w6  -n kube-system\n\nName:         deploy-user-token-5g6w6\nNamespace:    kube-system\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name=deploy-user\n              kubernetes.io/service-account.uid=0de0540e-2b24-11e8-841c-080027381e88\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nca.crt:     1025 bytes\nnamespace:  11 bytes\ntoken:  TOKEN\n```\n\n## 新建一个Maven project\npom.xml文件\n```\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\txsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\t<modelVersion>4.0.0</modelVersion>\n\n\t<groupId>org.kevin</groupId>\n\t<artifactId>k8s-api</artifactId>\n\t<version>0.0.1-SNAPSHOT</version>\n\t<packaging>jar</packaging>\n\n\t<name>k8s-api</name>\n\t<url>http://maven.apache.org</url>\n\n\t<properties>\n\t\t<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n\t</properties>\n\t<parent>\n\t    <groupId>org.springframework.boot</groupId>\n\t    <artifactId>spring-boot-starter-parent</artifactId>\n\t    <version>1.5.10.RELEASE</version>\n\t</parent>\n \n\n\t<dependencies>\n\t\t<dependency>\n\t\t\t<groupId>io.kubernetes</groupId>\n\t\t\t<artifactId>client-java</artifactId>\n\t\t\t<version>1.0.0-beta1</version>\n\t\t</dependency>\n\t\t\n\t\t<dependency>\n\t        <groupId>org.springframework.boot</groupId>\n\t        <artifactId>spring-boot-starter-web</artifactId>\n\t    </dependency>\n\t</dependencies>\n\t\n\t<build>\n\t\t<finalName>app</finalName>\n\t\t<resources>\n\t\t\t<resource>\n\t\t\t\t<directory>src/main/resources</directory>\n\t\t\t\t<includes>\n\t\t\t\t\t<include>**/*</include>\n\t\t\t\t\t<include>**/*/*</include>\n\t\t\t\t</includes>\n\t\t\t</resource>\n\t\t</resources>\n\t\t<plugins>\n\t\t\t<plugin>\n\t\t\t\t<groupId>org.apache.maven.plugins</groupId>\n\t\t\t\t<artifactId>maven-compiler-plugin</artifactId>\n\t\t\t\t<configuration>\n\t\t\t\t\t<source>1.7</source>\n\t\t\t\t\t<target>1.7</target>\n\t\t\t\t</configuration>\n\t\t\t</plugin>\n\t\n\t\t\t<plugin>\n\t\t\t\t<groupId>org.springframework.boot</groupId>\n\t\t\t\t<artifactId>spring-boot-maven-plugin</artifactId>\n\t\t\t</plugin>\n\t\t</plugins>\n\t</build>\n</project>\n\n```\n重点在\n```\n\t\t<dependency>\n\t\t\t<groupId>io.kubernetes</groupId>\n\t\t\t<artifactId>client-java</artifactId>\n\t\t\t<version>1.0.0-beta1</version>\n\t\t</dependency>\n```\n因为集群是k8s 1.8的，client需要使用1.0.0-beta1版本\n\n### 配置\n新建application.properties配置以下内容\n```\nserver.port=8089\n\n#deploy-user的token，可以配置在配置文件，也可以在声明pod的设置env引用secret\n#token=TOKEN\n```\n\n### springboot启动类\nspringboot启动类，包括初始化ApiClient。\nKUBERNETES_SERVICE_HOST 使用k8s部署后pod的env环境变量，k8s apiserver地址\nKUBERNETES_SERVICE_PORT k8s apiserver端口\ntoken 是调接口凭证。\n```\n@SpringBootApplication\npublic class App {\n\tpublic static void main(String[] args) throws IOException, ApiException {\n\t\tSpringApplication.run(App.class, args);\n\t}\n\t\n\t@Value(\"${KUBERNETES_SERVICE_HOST}\")\n\tprivate String host;\n\t@Value(\"${KUBERNETES_SERVICE_PORT}\")\n\tprivate String port;\n\t@Value(\"${token}\")\n\tprivate String token;\n\t\n\t@Bean\n\tpublic ApiClient getClient() {\n\t\ttry {\n\t\t\tApiClient client = new ApiClient();\n\t\t\tclient.setBasePath(\"https://\" + host + \":\" + port);\n\t\t\tclient.setApiKeyPrefix(\"bearer\");\n\t\t\tclient.setApiKey(token);\n\t\t\t//忽略ssl验证，不然java会报错。\n\t\t\tclient.setVerifyingSsl(false);\n\t\t\tConfiguration.setDefaultApiClient(client);\n\t\t\treturn client;\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tSystem.out.println(\"初始化ApiClient错误\");\n\t\t\tSystem.exit(1);\n\t\t\treturn null;\n\t\t}\n\t}\n}\n```\n\n### 使用client调用k8s api\n详细API接口文档：https://github.com/kubernetes-client/java/blob/master/kubernetes/README.md\nController.java\n```\n@RestController\npublic class Controller {\n\n\t@RequestMapping(method = RequestMethod.GET, path = \"list_pod_for_all_namespace\")\n\t@ResponseBody\n\tpublic RespObject listPod() {\n\t\tCoreV1Api api = new CoreV1Api();\n\t\tV1PodList list;\n\t\ttry {\n\t\t\tlist = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null);\n\t\t\tList<String> names = new ArrayList<>();\n\t\t\tfor (V1Pod item : list.getItems()) {\n\t\t\t\tnames.add(item.getMetadata().getName());\n\t\t\t}\n\t\t\treturn new RespObject(names);\n\t\t} catch (ApiException e) {\n\t\t\te.printStackTrace();\n\t\t\treturn new RespObject(e.getMessage(), 1);\n\t\t}\n\t}\n}\n```\n\nRespObject.java\n```\npublic class RespObject {\n\tprivate Object result;\n\tprivate int code;\n\t\n\tpublic RespObject(Object res) {\n\t\tthis.code = 0;\n\t\tthis.result = res;\n\t}\n\t\n\tpublic RespObject(Object res, int code) {\n\t\tthis.code = code;\n\t\tthis.result = res;\n\t}\n\n\tpublic Object getResult() {\n\t\treturn result;\n\t}\n\n\tpublic void setResult(Object result) {\n\t\tthis.result = result;\n\t}\n\n\tpublic int getCode() {\n\t\treturn code;\n\t}\n\n\tpublic void setCode(int code) {\n\t\tthis.code = code;\n\t}\n}\n```\n\n## 编译、部署到k8s\n\n### 制作docker镜像\nk8s-api.dockerfile文件\n```\nFROM java:8-jre-alpine\nWORKDIR /opt/\nCOPY app.jar /opt/app.jar\nCMD [\"/bin/sh\", \"-c\", \"java -jar app.jar\"]\n```\n```\n$ docker build -t k8s-api-demo -f k8s-api.dockerfile .\n```\n\n### 部署到k8s\n至此Java代码差不多了，打成jar包，docker build成镜像，用k8s部署，最后测试下\n\nk8s-api.yaml文件\n```\n---\nkind: Deployment\napiVersion: apps/v1beta2\nmetadata:\n name: k8s-api-demo\n labels:\n  app: k8s-api-demo\n namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k8s-api-demo\n  template:\n    metadata:\n      labels:\n        app: k8s-api-demo\n    spec:\n      containers:\n      - name: k8s-api-demo\n        image: k8s-api-demo\n        #配置env\n        env: \n        - name: token\n          valueFrom: \n            #token的值引用自secret\n            secretKeyRef: \n              # 通过kubectl get secret -n kube-system|grep deploy-user 得到名称\n              name: deploy-user-token-5g6w6\n              key: token\n              \n---\nkind: Service\napiVersion: v1\nmetadata:\n name: k8s-api-demo\n labels:\n  app: k8s-api-demo\n namespace: kube-system\nspec:\n ports:\n - port: 8099\n   targetPort: 8089\n   nodePort: 32180\n selector:\n  app: k8s-api-demo\n type: NodePort \n\n```\n\n```\n$ kubectl apply -f k8s-api.yaml\n$ kubectl get svc -o wide -n kube-system |grep k8s-api-demo\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE       SELECTOR\nk8s-api-demo       NodePort    10.100.62.191    <none>        8099:32180/TCP      1d        app=k8s-api-demo\n```\n\n通过访问master/node:32180端口即可访问到该项目\n访问http://master/node:32180/list_pod_for_all_namespace\n就可以返回所有namespace的pod name列表，结果如下：\n```\n{\n    \"result\": [\n        \"jenkins-74bbcdfd9-84hbr\",\n        \"adminserver-7c8c47cb95-ljf6z\",\n        \"dapi-test-pod\",\n        \"jobservice-7d584c99fb-2g2xq\",\n        \"k8s-api-6ccdbcb947-k6cwm\",\n        \"mysql-c87996b55-dhnjl\",\n        \"nfs-7b689fb468-p8pfm\",\n        \"nginx-75f4785b7-2t4kr\",\n        \"registry-c5f6f84dd-kk46k\",\n        \"ui-78599c46-8xpx5\",\n        \"calico-etcd-68mzw\",\n        \"calico-kube-controllers-6ff88bf6d4-57bqw\",\n        \"calico-node-6q2r7\",\n        \"calico-node-dx6cm\",\n        \"default-http-backend-n9st8\",\n        \"etcd-ubuntu-k8s\",\n        \"kube-apiserver-ubuntu-k8s\",\n        \"kube-controller-manager-ubuntu-k8s\",\n        \"kube-dns-545bc4bfd4-bgzg8\",\n        \"kube-proxy-77rlm\",\n        \"kube-proxy-nffh7\",\n        \"kube-scheduler-ubuntu-k8s\",\n        \"kubernetes-dashboard-76894548fd-kkhrj\",\n        \"nginx-ingress-controller-4kkph\",\n        \"nginx-ingress-controller-h9cpd\",\n        \"nginx-75f4785b7-lq4p7\",\n        \"nginx-75f4785b7-qt5sx\",\n        \"nginx-75f4785b7-v4lxg\"\n    ],\n    \"code\": 0\n}\n```\n\n## 在jenkins里面使用\n可以用pipeline用HttpRequest调用\n```\ndef resp;\ndef url = \"http://192.168.10.93:32180/list_pod_for_all_namespace\"\ntry {\n    resp = httpRequest httpMode: 'GET', url: \"$url\"\n    println resp.result\n}catch(e){\n}\n\n```\n\n文章Java代码：https://github.com/dongamp1990/k8s-api-demo.git","source":"_posts/kubernetes-java-client.md","raw":"---\ntitle: Kubernetes Java Client使用\ntags: [kubernetes]\ndate: 2018-03-29\n---\n## 前言\n使用Kubernetes 加Jenkins CICD，那么问题来了，Jenkins里面怎么部署Kubernetes应用呢。曾经在Jenkins上搜索过几个插件，其中Kubernetes Continuous Deploy没搞懂怎么用，咨询了一技术哥们说：本质就是配置认证， 调用k8s接口实现。在他的指点下，使用Kubernetes client去调用Kubernetes接口z\nk8s java client github地址：https://github.com/kubernetes-client/java。\n\n\n下面使用springboot + k8s client搭建的一个k8s-api项目\n\n## 创建接口调用token\n使用RBAC（基于角色的访问控制)调用接口，关于RBAC阅读以下文档\n官网文档：https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/\n中文翻译文档：https://jimmysong.io/kubernetes-handbook/concepts/rbac.html\n\n首先创建一个ServiceAccount，再绑定ClusterRole角色的cluster-admin权限，获得token，备client调接口用。\ndeployuser-token.yaml文件\n```\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: deploy-user\n annotations:\n  rbac.authorization.kubernetes.io/autoupdate: \"true\"\nroleRef:\n kind: ClusterRole\n name: cluster-admin\n apiGroup: rbac.authorization.k8s.io\nsubjects:\n - kind: ServiceAccount\n   name: deploy-user\n   namespace: kube-system\n   \n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n name: deploy-user\n namespace: kube-system\n labels:\n  kubernetes.io/cluster-service: \"true\"\n  addonmanager.kubernetes.io/mode: Reconcile\n```\n\n```\n$ kubectl apply -f cat deployuser-token.yaml\n```\n\n查看用户秘钥\n```\n$ kubectl get secret -n kube-system|grep deploy-user\nNAME                                     TYPE                                  DATA      AGE\ndeploy-user-token-5g6w6                  kubernetes.io/service-account-token   3         1d\n```\n\n#查看deploy-user账户秘钥详情\n```\n$ kubectl describe secret deploy-user-token-5g6w6  -n kube-system\n\nName:         deploy-user-token-5g6w6\nNamespace:    kube-system\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name=deploy-user\n              kubernetes.io/service-account.uid=0de0540e-2b24-11e8-841c-080027381e88\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nca.crt:     1025 bytes\nnamespace:  11 bytes\ntoken:  TOKEN\n```\n\n## 新建一个Maven project\npom.xml文件\n```\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\txsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\t<modelVersion>4.0.0</modelVersion>\n\n\t<groupId>org.kevin</groupId>\n\t<artifactId>k8s-api</artifactId>\n\t<version>0.0.1-SNAPSHOT</version>\n\t<packaging>jar</packaging>\n\n\t<name>k8s-api</name>\n\t<url>http://maven.apache.org</url>\n\n\t<properties>\n\t\t<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n\t</properties>\n\t<parent>\n\t    <groupId>org.springframework.boot</groupId>\n\t    <artifactId>spring-boot-starter-parent</artifactId>\n\t    <version>1.5.10.RELEASE</version>\n\t</parent>\n \n\n\t<dependencies>\n\t\t<dependency>\n\t\t\t<groupId>io.kubernetes</groupId>\n\t\t\t<artifactId>client-java</artifactId>\n\t\t\t<version>1.0.0-beta1</version>\n\t\t</dependency>\n\t\t\n\t\t<dependency>\n\t        <groupId>org.springframework.boot</groupId>\n\t        <artifactId>spring-boot-starter-web</artifactId>\n\t    </dependency>\n\t</dependencies>\n\t\n\t<build>\n\t\t<finalName>app</finalName>\n\t\t<resources>\n\t\t\t<resource>\n\t\t\t\t<directory>src/main/resources</directory>\n\t\t\t\t<includes>\n\t\t\t\t\t<include>**/*</include>\n\t\t\t\t\t<include>**/*/*</include>\n\t\t\t\t</includes>\n\t\t\t</resource>\n\t\t</resources>\n\t\t<plugins>\n\t\t\t<plugin>\n\t\t\t\t<groupId>org.apache.maven.plugins</groupId>\n\t\t\t\t<artifactId>maven-compiler-plugin</artifactId>\n\t\t\t\t<configuration>\n\t\t\t\t\t<source>1.7</source>\n\t\t\t\t\t<target>1.7</target>\n\t\t\t\t</configuration>\n\t\t\t</plugin>\n\t\n\t\t\t<plugin>\n\t\t\t\t<groupId>org.springframework.boot</groupId>\n\t\t\t\t<artifactId>spring-boot-maven-plugin</artifactId>\n\t\t\t</plugin>\n\t\t</plugins>\n\t</build>\n</project>\n\n```\n重点在\n```\n\t\t<dependency>\n\t\t\t<groupId>io.kubernetes</groupId>\n\t\t\t<artifactId>client-java</artifactId>\n\t\t\t<version>1.0.0-beta1</version>\n\t\t</dependency>\n```\n因为集群是k8s 1.8的，client需要使用1.0.0-beta1版本\n\n### 配置\n新建application.properties配置以下内容\n```\nserver.port=8089\n\n#deploy-user的token，可以配置在配置文件，也可以在声明pod的设置env引用secret\n#token=TOKEN\n```\n\n### springboot启动类\nspringboot启动类，包括初始化ApiClient。\nKUBERNETES_SERVICE_HOST 使用k8s部署后pod的env环境变量，k8s apiserver地址\nKUBERNETES_SERVICE_PORT k8s apiserver端口\ntoken 是调接口凭证。\n```\n@SpringBootApplication\npublic class App {\n\tpublic static void main(String[] args) throws IOException, ApiException {\n\t\tSpringApplication.run(App.class, args);\n\t}\n\t\n\t@Value(\"${KUBERNETES_SERVICE_HOST}\")\n\tprivate String host;\n\t@Value(\"${KUBERNETES_SERVICE_PORT}\")\n\tprivate String port;\n\t@Value(\"${token}\")\n\tprivate String token;\n\t\n\t@Bean\n\tpublic ApiClient getClient() {\n\t\ttry {\n\t\t\tApiClient client = new ApiClient();\n\t\t\tclient.setBasePath(\"https://\" + host + \":\" + port);\n\t\t\tclient.setApiKeyPrefix(\"bearer\");\n\t\t\tclient.setApiKey(token);\n\t\t\t//忽略ssl验证，不然java会报错。\n\t\t\tclient.setVerifyingSsl(false);\n\t\t\tConfiguration.setDefaultApiClient(client);\n\t\t\treturn client;\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tSystem.out.println(\"初始化ApiClient错误\");\n\t\t\tSystem.exit(1);\n\t\t\treturn null;\n\t\t}\n\t}\n}\n```\n\n### 使用client调用k8s api\n详细API接口文档：https://github.com/kubernetes-client/java/blob/master/kubernetes/README.md\nController.java\n```\n@RestController\npublic class Controller {\n\n\t@RequestMapping(method = RequestMethod.GET, path = \"list_pod_for_all_namespace\")\n\t@ResponseBody\n\tpublic RespObject listPod() {\n\t\tCoreV1Api api = new CoreV1Api();\n\t\tV1PodList list;\n\t\ttry {\n\t\t\tlist = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null);\n\t\t\tList<String> names = new ArrayList<>();\n\t\t\tfor (V1Pod item : list.getItems()) {\n\t\t\t\tnames.add(item.getMetadata().getName());\n\t\t\t}\n\t\t\treturn new RespObject(names);\n\t\t} catch (ApiException e) {\n\t\t\te.printStackTrace();\n\t\t\treturn new RespObject(e.getMessage(), 1);\n\t\t}\n\t}\n}\n```\n\nRespObject.java\n```\npublic class RespObject {\n\tprivate Object result;\n\tprivate int code;\n\t\n\tpublic RespObject(Object res) {\n\t\tthis.code = 0;\n\t\tthis.result = res;\n\t}\n\t\n\tpublic RespObject(Object res, int code) {\n\t\tthis.code = code;\n\t\tthis.result = res;\n\t}\n\n\tpublic Object getResult() {\n\t\treturn result;\n\t}\n\n\tpublic void setResult(Object result) {\n\t\tthis.result = result;\n\t}\n\n\tpublic int getCode() {\n\t\treturn code;\n\t}\n\n\tpublic void setCode(int code) {\n\t\tthis.code = code;\n\t}\n}\n```\n\n## 编译、部署到k8s\n\n### 制作docker镜像\nk8s-api.dockerfile文件\n```\nFROM java:8-jre-alpine\nWORKDIR /opt/\nCOPY app.jar /opt/app.jar\nCMD [\"/bin/sh\", \"-c\", \"java -jar app.jar\"]\n```\n```\n$ docker build -t k8s-api-demo -f k8s-api.dockerfile .\n```\n\n### 部署到k8s\n至此Java代码差不多了，打成jar包，docker build成镜像，用k8s部署，最后测试下\n\nk8s-api.yaml文件\n```\n---\nkind: Deployment\napiVersion: apps/v1beta2\nmetadata:\n name: k8s-api-demo\n labels:\n  app: k8s-api-demo\n namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k8s-api-demo\n  template:\n    metadata:\n      labels:\n        app: k8s-api-demo\n    spec:\n      containers:\n      - name: k8s-api-demo\n        image: k8s-api-demo\n        #配置env\n        env: \n        - name: token\n          valueFrom: \n            #token的值引用自secret\n            secretKeyRef: \n              # 通过kubectl get secret -n kube-system|grep deploy-user 得到名称\n              name: deploy-user-token-5g6w6\n              key: token\n              \n---\nkind: Service\napiVersion: v1\nmetadata:\n name: k8s-api-demo\n labels:\n  app: k8s-api-demo\n namespace: kube-system\nspec:\n ports:\n - port: 8099\n   targetPort: 8089\n   nodePort: 32180\n selector:\n  app: k8s-api-demo\n type: NodePort \n\n```\n\n```\n$ kubectl apply -f k8s-api.yaml\n$ kubectl get svc -o wide -n kube-system |grep k8s-api-demo\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE       SELECTOR\nk8s-api-demo       NodePort    10.100.62.191    <none>        8099:32180/TCP      1d        app=k8s-api-demo\n```\n\n通过访问master/node:32180端口即可访问到该项目\n访问http://master/node:32180/list_pod_for_all_namespace\n就可以返回所有namespace的pod name列表，结果如下：\n```\n{\n    \"result\": [\n        \"jenkins-74bbcdfd9-84hbr\",\n        \"adminserver-7c8c47cb95-ljf6z\",\n        \"dapi-test-pod\",\n        \"jobservice-7d584c99fb-2g2xq\",\n        \"k8s-api-6ccdbcb947-k6cwm\",\n        \"mysql-c87996b55-dhnjl\",\n        \"nfs-7b689fb468-p8pfm\",\n        \"nginx-75f4785b7-2t4kr\",\n        \"registry-c5f6f84dd-kk46k\",\n        \"ui-78599c46-8xpx5\",\n        \"calico-etcd-68mzw\",\n        \"calico-kube-controllers-6ff88bf6d4-57bqw\",\n        \"calico-node-6q2r7\",\n        \"calico-node-dx6cm\",\n        \"default-http-backend-n9st8\",\n        \"etcd-ubuntu-k8s\",\n        \"kube-apiserver-ubuntu-k8s\",\n        \"kube-controller-manager-ubuntu-k8s\",\n        \"kube-dns-545bc4bfd4-bgzg8\",\n        \"kube-proxy-77rlm\",\n        \"kube-proxy-nffh7\",\n        \"kube-scheduler-ubuntu-k8s\",\n        \"kubernetes-dashboard-76894548fd-kkhrj\",\n        \"nginx-ingress-controller-4kkph\",\n        \"nginx-ingress-controller-h9cpd\",\n        \"nginx-75f4785b7-lq4p7\",\n        \"nginx-75f4785b7-qt5sx\",\n        \"nginx-75f4785b7-v4lxg\"\n    ],\n    \"code\": 0\n}\n```\n\n## 在jenkins里面使用\n可以用pipeline用HttpRequest调用\n```\ndef resp;\ndef url = \"http://192.168.10.93:32180/list_pod_for_all_namespace\"\ntry {\n    resp = httpRequest httpMode: 'GET', url: \"$url\"\n    println resp.result\n}catch(e){\n}\n\n```\n\n文章Java代码：https://github.com/dongamp1990/k8s-api-demo.git","slug":"kubernetes-java-client","published":1,"updated":"2018-05-23T02:11:01.361Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjncsum4w000c7gvrcsfma5lr","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>使用Kubernetes 加Jenkins CICD，那么问题来了，Jenkins里面怎么部署Kubernetes应用呢。曾经在Jenkins上搜索过几个插件，其中Kubernetes Continuous Deploy没搞懂怎么用，咨询了一技术哥们说：本质就是配置认证， 调用k8s接口实现。在他的指点下，使用Kubernetes client去调用Kubernetes接口z<br>k8s java client github地址：<a href=\"https://github.com/kubernetes-client/java。\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes-client/java。</a></p>\n<p>下面使用springboot + k8s client搭建的一个k8s-api项目</p>\n<h2 id=\"创建接口调用token\"><a href=\"#创建接口调用token\" class=\"headerlink\" title=\"创建接口调用token\"></a>创建接口调用token</h2><p>使用RBAC（基于角色的访问控制)调用接口，关于RBAC阅读以下文档<br>官网文档：<a href=\"https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/\" target=\"_blank\" rel=\"noopener\">https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/</a><br>中文翻译文档：<a href=\"https://jimmysong.io/kubernetes-handbook/concepts/rbac.html\" target=\"_blank\" rel=\"noopener\">https://jimmysong.io/kubernetes-handbook/concepts/rbac.html</a></p>\n<p>首先创建一个ServiceAccount，再绑定ClusterRole角色的cluster-admin权限，获得token，备client调接口用。<br>deployuser-token.yaml文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\"> name: deploy-user</span><br><span class=\"line\"> annotations:</span><br><span class=\"line\">  rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\"> kind: ClusterRole</span><br><span class=\"line\"> name: cluster-admin</span><br><span class=\"line\"> apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">subjects:</span><br><span class=\"line\"> - kind: ServiceAccount</span><br><span class=\"line\">   name: deploy-user</span><br><span class=\"line\">   namespace: kube-system</span><br><span class=\"line\">   </span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\"> name: deploy-user</span><br><span class=\"line\"> namespace: kube-system</span><br><span class=\"line\"> labels:</span><br><span class=\"line\">  kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class=\"line\">  addonmanager.kubernetes.io/mode: Reconcile</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f cat deployuser-token.yaml</span><br></pre></td></tr></table></figure>\n<p>查看用户秘钥<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get secret -n kube-system|grep deploy-user</span><br><span class=\"line\">NAME                                     TYPE                                  DATA      AGE</span><br><span class=\"line\">deploy-user-token-5g6w6                  kubernetes.io/service-account-token   3         1d</span><br></pre></td></tr></table></figure></p>\n<p>#查看deploy-user账户秘钥详情<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl describe secret deploy-user-token-5g6w6  -n kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">Name:         deploy-user-token-5g6w6</span><br><span class=\"line\">Namespace:    kube-system</span><br><span class=\"line\">Labels:       &lt;none&gt;</span><br><span class=\"line\">Annotations:  kubernetes.io/service-account.name=deploy-user</span><br><span class=\"line\">              kubernetes.io/service-account.uid=0de0540e-2b24-11e8-841c-080027381e88</span><br><span class=\"line\"></span><br><span class=\"line\">Type:  kubernetes.io/service-account-token</span><br><span class=\"line\"></span><br><span class=\"line\">Data</span><br><span class=\"line\">====</span><br><span class=\"line\">ca.crt:     1025 bytes</span><br><span class=\"line\">namespace:  11 bytes</span><br><span class=\"line\">token:  TOKEN</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"新建一个Maven-project\"><a href=\"#新建一个Maven-project\" class=\"headerlink\" title=\"新建一个Maven project\"></a>新建一个Maven project</h2><p>pom.xml文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class=\"line\">\txsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class=\"line\">\t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;groupId&gt;org.kevin&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;k8s-api&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">\t&lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;name&gt;k8s-api&lt;/name&gt;</span><br><span class=\"line\">\t&lt;url&gt;http://maven.apache.org&lt;/url&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;properties&gt;</span><br><span class=\"line\">\t\t&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class=\"line\">\t&lt;/properties&gt;</span><br><span class=\"line\">\t&lt;parent&gt;</span><br><span class=\"line\">\t    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;</span><br><span class=\"line\">\t    &lt;version&gt;1.5.10.RELEASE&lt;/version&gt;</span><br><span class=\"line\">\t&lt;/parent&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;dependencies&gt;</span><br><span class=\"line\">\t\t&lt;dependency&gt;</span><br><span class=\"line\">\t\t\t&lt;groupId&gt;io.kubernetes&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t&lt;artifactId&gt;client-java&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t&lt;version&gt;1.0.0-beta1&lt;/version&gt;</span><br><span class=\"line\">\t\t&lt;/dependency&gt;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t&lt;dependency&gt;</span><br><span class=\"line\">\t        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;</span><br><span class=\"line\">\t    &lt;/dependency&gt;</span><br><span class=\"line\">\t&lt;/dependencies&gt;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t&lt;build&gt;</span><br><span class=\"line\">\t\t&lt;finalName&gt;app&lt;/finalName&gt;</span><br><span class=\"line\">\t\t&lt;resources&gt;</span><br><span class=\"line\">\t\t\t&lt;resource&gt;</span><br><span class=\"line\">\t\t\t\t&lt;directory&gt;src/main/resources&lt;/directory&gt;</span><br><span class=\"line\">\t\t\t\t&lt;includes&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;include&gt;**/*&lt;/include&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;include&gt;**/*/*&lt;/include&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/includes&gt;</span><br><span class=\"line\">\t\t\t&lt;/resource&gt;</span><br><span class=\"line\">\t\t&lt;/resources&gt;</span><br><span class=\"line\">\t\t&lt;plugins&gt;</span><br><span class=\"line\">\t\t\t&lt;plugin&gt;</span><br><span class=\"line\">\t\t\t\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;configuration&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;source&gt;1.7&lt;/source&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;target&gt;1.7&lt;/target&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/configuration&gt;</span><br><span class=\"line\">\t\t\t&lt;/plugin&gt;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t\t&lt;plugin&gt;</span><br><span class=\"line\">\t\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t&lt;/plugin&gt;</span><br><span class=\"line\">\t\t&lt;/plugins&gt;</span><br><span class=\"line\">\t&lt;/build&gt;</span><br><span class=\"line\">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p>\n<p>重点在<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;io.kubernetes&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;client-java&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;1.0.0-beta1&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n<p>因为集群是k8s 1.8的，client需要使用1.0.0-beta1版本</p>\n<h3 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h3><p>新建application.properties配置以下内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">server.port=8089</span><br><span class=\"line\"></span><br><span class=\"line\">#deploy-user的token，可以配置在配置文件，也可以在声明pod的设置env引用secret</span><br><span class=\"line\">#token=TOKEN</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"springboot启动类\"><a href=\"#springboot启动类\" class=\"headerlink\" title=\"springboot启动类\"></a>springboot启动类</h3><p>springboot启动类，包括初始化ApiClient。<br>KUBERNETES_SERVICE_HOST 使用k8s部署后pod的env环境变量，k8s apiserver地址<br>KUBERNETES_SERVICE_PORT k8s apiserver端口<br>token 是调接口凭证。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@SpringBootApplication</span><br><span class=\"line\">public class App &#123;</span><br><span class=\"line\">\tpublic static void main(String[] args) throws IOException, ApiException &#123;</span><br><span class=\"line\">\t\tSpringApplication.run(App.class, args);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t@Value(&quot;$&#123;KUBERNETES_SERVICE_HOST&#125;&quot;)</span><br><span class=\"line\">\tprivate String host;</span><br><span class=\"line\">\t@Value(&quot;$&#123;KUBERNETES_SERVICE_PORT&#125;&quot;)</span><br><span class=\"line\">\tprivate String port;</span><br><span class=\"line\">\t@Value(&quot;$&#123;token&#125;&quot;)</span><br><span class=\"line\">\tprivate String token;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t@Bean</span><br><span class=\"line\">\tpublic ApiClient getClient() &#123;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tApiClient client = new ApiClient();</span><br><span class=\"line\">\t\t\tclient.setBasePath(&quot;https://&quot; + host + &quot;:&quot; + port);</span><br><span class=\"line\">\t\t\tclient.setApiKeyPrefix(&quot;bearer&quot;);</span><br><span class=\"line\">\t\t\tclient.setApiKey(token);</span><br><span class=\"line\">\t\t\t//忽略ssl验证，不然java会报错。</span><br><span class=\"line\">\t\t\tclient.setVerifyingSsl(false);</span><br><span class=\"line\">\t\t\tConfiguration.setDefaultApiClient(client);</span><br><span class=\"line\">\t\t\treturn client;</span><br><span class=\"line\">\t\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\tSystem.out.println(&quot;初始化ApiClient错误&quot;);</span><br><span class=\"line\">\t\t\tSystem.exit(1);</span><br><span class=\"line\">\t\t\treturn null;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用client调用k8s-api\"><a href=\"#使用client调用k8s-api\" class=\"headerlink\" title=\"使用client调用k8s api\"></a>使用client调用k8s api</h3><p>详细API接口文档：<a href=\"https://github.com/kubernetes-client/java/blob/master/kubernetes/README.md\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes-client/java/blob/master/kubernetes/README.md</a><br>Controller.java<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@RestController</span><br><span class=\"line\">public class Controller &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t@RequestMapping(method = RequestMethod.GET, path = &quot;list_pod_for_all_namespace&quot;)</span><br><span class=\"line\">\t@ResponseBody</span><br><span class=\"line\">\tpublic RespObject listPod() &#123;</span><br><span class=\"line\">\t\tCoreV1Api api = new CoreV1Api();</span><br><span class=\"line\">\t\tV1PodList list;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tlist = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null);</span><br><span class=\"line\">\t\t\tList&lt;String&gt; names = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\t\t\tfor (V1Pod item : list.getItems()) &#123;</span><br><span class=\"line\">\t\t\t\tnames.add(item.getMetadata().getName());</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\treturn new RespObject(names);</span><br><span class=\"line\">\t\t&#125; catch (ApiException e) &#123;</span><br><span class=\"line\">\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\treturn new RespObject(e.getMessage(), 1);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>RespObject.java<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class RespObject &#123;</span><br><span class=\"line\">\tprivate Object result;</span><br><span class=\"line\">\tprivate int code;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tpublic RespObject(Object res) &#123;</span><br><span class=\"line\">\t\tthis.code = 0;</span><br><span class=\"line\">\t\tthis.result = res;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tpublic RespObject(Object res, int code) &#123;</span><br><span class=\"line\">\t\tthis.code = code;</span><br><span class=\"line\">\t\tthis.result = res;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic Object getResult() &#123;</span><br><span class=\"line\">\t\treturn result;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic void setResult(Object result) &#123;</span><br><span class=\"line\">\t\tthis.result = result;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic int getCode() &#123;</span><br><span class=\"line\">\t\treturn code;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic void setCode(int code) &#123;</span><br><span class=\"line\">\t\tthis.code = code;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"编译、部署到k8s\"><a href=\"#编译、部署到k8s\" class=\"headerlink\" title=\"编译、部署到k8s\"></a>编译、部署到k8s</h2><h3 id=\"制作docker镜像\"><a href=\"#制作docker镜像\" class=\"headerlink\" title=\"制作docker镜像\"></a>制作docker镜像</h3><p>k8s-api.dockerfile文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM java:8-jre-alpine</span><br><span class=\"line\">WORKDIR /opt/</span><br><span class=\"line\">COPY app.jar /opt/app.jar</span><br><span class=\"line\">CMD [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -jar app.jar&quot;]</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker build -t k8s-api-demo -f k8s-api.dockerfile .</span><br></pre></td></tr></table></figure>\n<h3 id=\"部署到k8s\"><a href=\"#部署到k8s\" class=\"headerlink\" title=\"部署到k8s\"></a>部署到k8s</h3><p>至此Java代码差不多了，打成jar包，docker build成镜像，用k8s部署，最后测试下</p>\n<p>k8s-api.yaml文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">apiVersion: apps/v1beta2</span><br><span class=\"line\">metadata:</span><br><span class=\"line\"> name: k8s-api-demo</span><br><span class=\"line\"> labels:</span><br><span class=\"line\">  app: k8s-api-demo</span><br><span class=\"line\"> namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      app: k8s-api-demo</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        app: k8s-api-demo</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: k8s-api-demo</span><br><span class=\"line\">        image: k8s-api-demo</span><br><span class=\"line\">        #配置env</span><br><span class=\"line\">        env: </span><br><span class=\"line\">        - name: token</span><br><span class=\"line\">          valueFrom: </span><br><span class=\"line\">            #token的值引用自secret</span><br><span class=\"line\">            secretKeyRef: </span><br><span class=\"line\">              # 通过kubectl get secret -n kube-system|grep deploy-user 得到名称</span><br><span class=\"line\">              name: deploy-user-token-5g6w6</span><br><span class=\"line\">              key: token</span><br><span class=\"line\">              </span><br><span class=\"line\">---</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\"> name: k8s-api-demo</span><br><span class=\"line\"> labels:</span><br><span class=\"line\">  app: k8s-api-demo</span><br><span class=\"line\"> namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\"> ports:</span><br><span class=\"line\"> - port: 8099</span><br><span class=\"line\">   targetPort: 8089</span><br><span class=\"line\">   nodePort: 32180</span><br><span class=\"line\"> selector:</span><br><span class=\"line\">  app: k8s-api-demo</span><br><span class=\"line\"> type: NodePort</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f k8s-api.yaml</span><br><span class=\"line\">$ kubectl get svc -o wide -n kube-system |grep k8s-api-demo</span><br><span class=\"line\">NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE       SELECTOR</span><br><span class=\"line\">k8s-api-demo       NodePort    10.100.62.191    &lt;none&gt;        8099:32180/TCP      1d        app=k8s-api-demo</span><br></pre></td></tr></table></figure>\n<p>通过访问master/node:32180端口即可访问到该项目<br>访问<a href=\"http://master/node:32180/list_pod_for_all_namespace\" target=\"_blank\" rel=\"noopener\">http://master/node:32180/list_pod_for_all_namespace</a><br>就可以返回所有namespace的pod name列表，结果如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;result&quot;: [</span><br><span class=\"line\">        &quot;jenkins-74bbcdfd9-84hbr&quot;,</span><br><span class=\"line\">        &quot;adminserver-7c8c47cb95-ljf6z&quot;,</span><br><span class=\"line\">        &quot;dapi-test-pod&quot;,</span><br><span class=\"line\">        &quot;jobservice-7d584c99fb-2g2xq&quot;,</span><br><span class=\"line\">        &quot;k8s-api-6ccdbcb947-k6cwm&quot;,</span><br><span class=\"line\">        &quot;mysql-c87996b55-dhnjl&quot;,</span><br><span class=\"line\">        &quot;nfs-7b689fb468-p8pfm&quot;,</span><br><span class=\"line\">        &quot;nginx-75f4785b7-2t4kr&quot;,</span><br><span class=\"line\">        &quot;registry-c5f6f84dd-kk46k&quot;,</span><br><span class=\"line\">        &quot;ui-78599c46-8xpx5&quot;,</span><br><span class=\"line\">        &quot;calico-etcd-68mzw&quot;,</span><br><span class=\"line\">        &quot;calico-kube-controllers-6ff88bf6d4-57bqw&quot;,</span><br><span class=\"line\">        &quot;calico-node-6q2r7&quot;,</span><br><span class=\"line\">        &quot;calico-node-dx6cm&quot;,</span><br><span class=\"line\">        &quot;default-http-backend-n9st8&quot;,</span><br><span class=\"line\">        &quot;etcd-ubuntu-k8s&quot;,</span><br><span class=\"line\">        &quot;kube-apiserver-ubuntu-k8s&quot;,</span><br><span class=\"line\">        &quot;kube-controller-manager-ubuntu-k8s&quot;,</span><br><span class=\"line\">        &quot;kube-dns-545bc4bfd4-bgzg8&quot;,</span><br><span class=\"line\">        &quot;kube-proxy-77rlm&quot;,</span><br><span class=\"line\">        &quot;kube-proxy-nffh7&quot;,</span><br><span class=\"line\">        &quot;kube-scheduler-ubuntu-k8s&quot;,</span><br><span class=\"line\">        &quot;kubernetes-dashboard-76894548fd-kkhrj&quot;,</span><br><span class=\"line\">        &quot;nginx-ingress-controller-4kkph&quot;,</span><br><span class=\"line\">        &quot;nginx-ingress-controller-h9cpd&quot;,</span><br><span class=\"line\">        &quot;nginx-75f4785b7-lq4p7&quot;,</span><br><span class=\"line\">        &quot;nginx-75f4785b7-qt5sx&quot;,</span><br><span class=\"line\">        &quot;nginx-75f4785b7-v4lxg&quot;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;code&quot;: 0</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"在jenkins里面使用\"><a href=\"#在jenkins里面使用\" class=\"headerlink\" title=\"在jenkins里面使用\"></a>在jenkins里面使用</h2><p>可以用pipeline用HttpRequest调用<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def resp;</span><br><span class=\"line\">def url = &quot;http://192.168.10.93:32180/list_pod_for_all_namespace&quot;</span><br><span class=\"line\">try &#123;</span><br><span class=\"line\">    resp = httpRequest httpMode: &apos;GET&apos;, url: &quot;$url&quot;</span><br><span class=\"line\">    println resp.result</span><br><span class=\"line\">&#125;catch(e)&#123;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>文章Java代码：<a href=\"https://github.com/dongamp1990/k8s-api-demo.git\" target=\"_blank\" rel=\"noopener\">https://github.com/dongamp1990/k8s-api-demo.git</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>使用Kubernetes 加Jenkins CICD，那么问题来了，Jenkins里面怎么部署Kubernetes应用呢。曾经在Jenkins上搜索过几个插件，其中Kubernetes Continuous Deploy没搞懂怎么用，咨询了一技术哥们说：本质就是配置认证， 调用k8s接口实现。在他的指点下，使用Kubernetes client去调用Kubernetes接口z<br>k8s java client github地址：<a href=\"https://github.com/kubernetes-client/java。\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes-client/java。</a></p>\n<p>下面使用springboot + k8s client搭建的一个k8s-api项目</p>\n<h2 id=\"创建接口调用token\"><a href=\"#创建接口调用token\" class=\"headerlink\" title=\"创建接口调用token\"></a>创建接口调用token</h2><p>使用RBAC（基于角色的访问控制)调用接口，关于RBAC阅读以下文档<br>官网文档：<a href=\"https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/\" target=\"_blank\" rel=\"noopener\">https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/</a><br>中文翻译文档：<a href=\"https://jimmysong.io/kubernetes-handbook/concepts/rbac.html\" target=\"_blank\" rel=\"noopener\">https://jimmysong.io/kubernetes-handbook/concepts/rbac.html</a></p>\n<p>首先创建一个ServiceAccount，再绑定ClusterRole角色的cluster-admin权限，获得token，备client调接口用。<br>deployuser-token.yaml文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\"> name: deploy-user</span><br><span class=\"line\"> annotations:</span><br><span class=\"line\">  rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\"> kind: ClusterRole</span><br><span class=\"line\"> name: cluster-admin</span><br><span class=\"line\"> apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">subjects:</span><br><span class=\"line\"> - kind: ServiceAccount</span><br><span class=\"line\">   name: deploy-user</span><br><span class=\"line\">   namespace: kube-system</span><br><span class=\"line\">   </span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\"> name: deploy-user</span><br><span class=\"line\"> namespace: kube-system</span><br><span class=\"line\"> labels:</span><br><span class=\"line\">  kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class=\"line\">  addonmanager.kubernetes.io/mode: Reconcile</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f cat deployuser-token.yaml</span><br></pre></td></tr></table></figure>\n<p>查看用户秘钥<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get secret -n kube-system|grep deploy-user</span><br><span class=\"line\">NAME                                     TYPE                                  DATA      AGE</span><br><span class=\"line\">deploy-user-token-5g6w6                  kubernetes.io/service-account-token   3         1d</span><br></pre></td></tr></table></figure></p>\n<p>#查看deploy-user账户秘钥详情<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl describe secret deploy-user-token-5g6w6  -n kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">Name:         deploy-user-token-5g6w6</span><br><span class=\"line\">Namespace:    kube-system</span><br><span class=\"line\">Labels:       &lt;none&gt;</span><br><span class=\"line\">Annotations:  kubernetes.io/service-account.name=deploy-user</span><br><span class=\"line\">              kubernetes.io/service-account.uid=0de0540e-2b24-11e8-841c-080027381e88</span><br><span class=\"line\"></span><br><span class=\"line\">Type:  kubernetes.io/service-account-token</span><br><span class=\"line\"></span><br><span class=\"line\">Data</span><br><span class=\"line\">====</span><br><span class=\"line\">ca.crt:     1025 bytes</span><br><span class=\"line\">namespace:  11 bytes</span><br><span class=\"line\">token:  TOKEN</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"新建一个Maven-project\"><a href=\"#新建一个Maven-project\" class=\"headerlink\" title=\"新建一个Maven project\"></a>新建一个Maven project</h2><p>pom.xml文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class=\"line\">\txsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class=\"line\">\t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;groupId&gt;org.kevin&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;k8s-api&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\">\t&lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;name&gt;k8s-api&lt;/name&gt;</span><br><span class=\"line\">\t&lt;url&gt;http://maven.apache.org&lt;/url&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;properties&gt;</span><br><span class=\"line\">\t\t&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class=\"line\">\t&lt;/properties&gt;</span><br><span class=\"line\">\t&lt;parent&gt;</span><br><span class=\"line\">\t    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;</span><br><span class=\"line\">\t    &lt;version&gt;1.5.10.RELEASE&lt;/version&gt;</span><br><span class=\"line\">\t&lt;/parent&gt;</span><br><span class=\"line\"> </span><br><span class=\"line\"></span><br><span class=\"line\">\t&lt;dependencies&gt;</span><br><span class=\"line\">\t\t&lt;dependency&gt;</span><br><span class=\"line\">\t\t\t&lt;groupId&gt;io.kubernetes&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t&lt;artifactId&gt;client-java&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t&lt;version&gt;1.0.0-beta1&lt;/version&gt;</span><br><span class=\"line\">\t\t&lt;/dependency&gt;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t&lt;dependency&gt;</span><br><span class=\"line\">\t        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;</span><br><span class=\"line\">\t    &lt;/dependency&gt;</span><br><span class=\"line\">\t&lt;/dependencies&gt;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t&lt;build&gt;</span><br><span class=\"line\">\t\t&lt;finalName&gt;app&lt;/finalName&gt;</span><br><span class=\"line\">\t\t&lt;resources&gt;</span><br><span class=\"line\">\t\t\t&lt;resource&gt;</span><br><span class=\"line\">\t\t\t\t&lt;directory&gt;src/main/resources&lt;/directory&gt;</span><br><span class=\"line\">\t\t\t\t&lt;includes&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;include&gt;**/*&lt;/include&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;include&gt;**/*/*&lt;/include&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/includes&gt;</span><br><span class=\"line\">\t\t\t&lt;/resource&gt;</span><br><span class=\"line\">\t\t&lt;/resources&gt;</span><br><span class=\"line\">\t\t&lt;plugins&gt;</span><br><span class=\"line\">\t\t\t&lt;plugin&gt;</span><br><span class=\"line\">\t\t\t\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;configuration&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;source&gt;1.7&lt;/source&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;target&gt;1.7&lt;/target&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/configuration&gt;</span><br><span class=\"line\">\t\t\t&lt;/plugin&gt;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t\t&lt;plugin&gt;</span><br><span class=\"line\">\t\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t&lt;/plugin&gt;</span><br><span class=\"line\">\t\t&lt;/plugins&gt;</span><br><span class=\"line\">\t&lt;/build&gt;</span><br><span class=\"line\">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p>\n<p>重点在<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;io.kubernetes&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;client-java&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;1.0.0-beta1&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n<p>因为集群是k8s 1.8的，client需要使用1.0.0-beta1版本</p>\n<h3 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h3><p>新建application.properties配置以下内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">server.port=8089</span><br><span class=\"line\"></span><br><span class=\"line\">#deploy-user的token，可以配置在配置文件，也可以在声明pod的设置env引用secret</span><br><span class=\"line\">#token=TOKEN</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"springboot启动类\"><a href=\"#springboot启动类\" class=\"headerlink\" title=\"springboot启动类\"></a>springboot启动类</h3><p>springboot启动类，包括初始化ApiClient。<br>KUBERNETES_SERVICE_HOST 使用k8s部署后pod的env环境变量，k8s apiserver地址<br>KUBERNETES_SERVICE_PORT k8s apiserver端口<br>token 是调接口凭证。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@SpringBootApplication</span><br><span class=\"line\">public class App &#123;</span><br><span class=\"line\">\tpublic static void main(String[] args) throws IOException, ApiException &#123;</span><br><span class=\"line\">\t\tSpringApplication.run(App.class, args);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t@Value(&quot;$&#123;KUBERNETES_SERVICE_HOST&#125;&quot;)</span><br><span class=\"line\">\tprivate String host;</span><br><span class=\"line\">\t@Value(&quot;$&#123;KUBERNETES_SERVICE_PORT&#125;&quot;)</span><br><span class=\"line\">\tprivate String port;</span><br><span class=\"line\">\t@Value(&quot;$&#123;token&#125;&quot;)</span><br><span class=\"line\">\tprivate String token;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t@Bean</span><br><span class=\"line\">\tpublic ApiClient getClient() &#123;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tApiClient client = new ApiClient();</span><br><span class=\"line\">\t\t\tclient.setBasePath(&quot;https://&quot; + host + &quot;:&quot; + port);</span><br><span class=\"line\">\t\t\tclient.setApiKeyPrefix(&quot;bearer&quot;);</span><br><span class=\"line\">\t\t\tclient.setApiKey(token);</span><br><span class=\"line\">\t\t\t//忽略ssl验证，不然java会报错。</span><br><span class=\"line\">\t\t\tclient.setVerifyingSsl(false);</span><br><span class=\"line\">\t\t\tConfiguration.setDefaultApiClient(client);</span><br><span class=\"line\">\t\t\treturn client;</span><br><span class=\"line\">\t\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\tSystem.out.println(&quot;初始化ApiClient错误&quot;);</span><br><span class=\"line\">\t\t\tSystem.exit(1);</span><br><span class=\"line\">\t\t\treturn null;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用client调用k8s-api\"><a href=\"#使用client调用k8s-api\" class=\"headerlink\" title=\"使用client调用k8s api\"></a>使用client调用k8s api</h3><p>详细API接口文档：<a href=\"https://github.com/kubernetes-client/java/blob/master/kubernetes/README.md\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes-client/java/blob/master/kubernetes/README.md</a><br>Controller.java<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@RestController</span><br><span class=\"line\">public class Controller &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t@RequestMapping(method = RequestMethod.GET, path = &quot;list_pod_for_all_namespace&quot;)</span><br><span class=\"line\">\t@ResponseBody</span><br><span class=\"line\">\tpublic RespObject listPod() &#123;</span><br><span class=\"line\">\t\tCoreV1Api api = new CoreV1Api();</span><br><span class=\"line\">\t\tV1PodList list;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tlist = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null);</span><br><span class=\"line\">\t\t\tList&lt;String&gt; names = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\t\t\tfor (V1Pod item : list.getItems()) &#123;</span><br><span class=\"line\">\t\t\t\tnames.add(item.getMetadata().getName());</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\treturn new RespObject(names);</span><br><span class=\"line\">\t\t&#125; catch (ApiException e) &#123;</span><br><span class=\"line\">\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\treturn new RespObject(e.getMessage(), 1);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>RespObject.java<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class RespObject &#123;</span><br><span class=\"line\">\tprivate Object result;</span><br><span class=\"line\">\tprivate int code;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tpublic RespObject(Object res) &#123;</span><br><span class=\"line\">\t\tthis.code = 0;</span><br><span class=\"line\">\t\tthis.result = res;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tpublic RespObject(Object res, int code) &#123;</span><br><span class=\"line\">\t\tthis.code = code;</span><br><span class=\"line\">\t\tthis.result = res;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic Object getResult() &#123;</span><br><span class=\"line\">\t\treturn result;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic void setResult(Object result) &#123;</span><br><span class=\"line\">\t\tthis.result = result;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic int getCode() &#123;</span><br><span class=\"line\">\t\treturn code;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic void setCode(int code) &#123;</span><br><span class=\"line\">\t\tthis.code = code;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"编译、部署到k8s\"><a href=\"#编译、部署到k8s\" class=\"headerlink\" title=\"编译、部署到k8s\"></a>编译、部署到k8s</h2><h3 id=\"制作docker镜像\"><a href=\"#制作docker镜像\" class=\"headerlink\" title=\"制作docker镜像\"></a>制作docker镜像</h3><p>k8s-api.dockerfile文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM java:8-jre-alpine</span><br><span class=\"line\">WORKDIR /opt/</span><br><span class=\"line\">COPY app.jar /opt/app.jar</span><br><span class=\"line\">CMD [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -jar app.jar&quot;]</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker build -t k8s-api-demo -f k8s-api.dockerfile .</span><br></pre></td></tr></table></figure>\n<h3 id=\"部署到k8s\"><a href=\"#部署到k8s\" class=\"headerlink\" title=\"部署到k8s\"></a>部署到k8s</h3><p>至此Java代码差不多了，打成jar包，docker build成镜像，用k8s部署，最后测试下</p>\n<p>k8s-api.yaml文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">apiVersion: apps/v1beta2</span><br><span class=\"line\">metadata:</span><br><span class=\"line\"> name: k8s-api-demo</span><br><span class=\"line\"> labels:</span><br><span class=\"line\">  app: k8s-api-demo</span><br><span class=\"line\"> namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      app: k8s-api-demo</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        app: k8s-api-demo</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: k8s-api-demo</span><br><span class=\"line\">        image: k8s-api-demo</span><br><span class=\"line\">        #配置env</span><br><span class=\"line\">        env: </span><br><span class=\"line\">        - name: token</span><br><span class=\"line\">          valueFrom: </span><br><span class=\"line\">            #token的值引用自secret</span><br><span class=\"line\">            secretKeyRef: </span><br><span class=\"line\">              # 通过kubectl get secret -n kube-system|grep deploy-user 得到名称</span><br><span class=\"line\">              name: deploy-user-token-5g6w6</span><br><span class=\"line\">              key: token</span><br><span class=\"line\">              </span><br><span class=\"line\">---</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\"> name: k8s-api-demo</span><br><span class=\"line\"> labels:</span><br><span class=\"line\">  app: k8s-api-demo</span><br><span class=\"line\"> namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\"> ports:</span><br><span class=\"line\"> - port: 8099</span><br><span class=\"line\">   targetPort: 8089</span><br><span class=\"line\">   nodePort: 32180</span><br><span class=\"line\"> selector:</span><br><span class=\"line\">  app: k8s-api-demo</span><br><span class=\"line\"> type: NodePort</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f k8s-api.yaml</span><br><span class=\"line\">$ kubectl get svc -o wide -n kube-system |grep k8s-api-demo</span><br><span class=\"line\">NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE       SELECTOR</span><br><span class=\"line\">k8s-api-demo       NodePort    10.100.62.191    &lt;none&gt;        8099:32180/TCP      1d        app=k8s-api-demo</span><br></pre></td></tr></table></figure>\n<p>通过访问master/node:32180端口即可访问到该项目<br>访问<a href=\"http://master/node:32180/list_pod_for_all_namespace\" target=\"_blank\" rel=\"noopener\">http://master/node:32180/list_pod_for_all_namespace</a><br>就可以返回所有namespace的pod name列表，结果如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;result&quot;: [</span><br><span class=\"line\">        &quot;jenkins-74bbcdfd9-84hbr&quot;,</span><br><span class=\"line\">        &quot;adminserver-7c8c47cb95-ljf6z&quot;,</span><br><span class=\"line\">        &quot;dapi-test-pod&quot;,</span><br><span class=\"line\">        &quot;jobservice-7d584c99fb-2g2xq&quot;,</span><br><span class=\"line\">        &quot;k8s-api-6ccdbcb947-k6cwm&quot;,</span><br><span class=\"line\">        &quot;mysql-c87996b55-dhnjl&quot;,</span><br><span class=\"line\">        &quot;nfs-7b689fb468-p8pfm&quot;,</span><br><span class=\"line\">        &quot;nginx-75f4785b7-2t4kr&quot;,</span><br><span class=\"line\">        &quot;registry-c5f6f84dd-kk46k&quot;,</span><br><span class=\"line\">        &quot;ui-78599c46-8xpx5&quot;,</span><br><span class=\"line\">        &quot;calico-etcd-68mzw&quot;,</span><br><span class=\"line\">        &quot;calico-kube-controllers-6ff88bf6d4-57bqw&quot;,</span><br><span class=\"line\">        &quot;calico-node-6q2r7&quot;,</span><br><span class=\"line\">        &quot;calico-node-dx6cm&quot;,</span><br><span class=\"line\">        &quot;default-http-backend-n9st8&quot;,</span><br><span class=\"line\">        &quot;etcd-ubuntu-k8s&quot;,</span><br><span class=\"line\">        &quot;kube-apiserver-ubuntu-k8s&quot;,</span><br><span class=\"line\">        &quot;kube-controller-manager-ubuntu-k8s&quot;,</span><br><span class=\"line\">        &quot;kube-dns-545bc4bfd4-bgzg8&quot;,</span><br><span class=\"line\">        &quot;kube-proxy-77rlm&quot;,</span><br><span class=\"line\">        &quot;kube-proxy-nffh7&quot;,</span><br><span class=\"line\">        &quot;kube-scheduler-ubuntu-k8s&quot;,</span><br><span class=\"line\">        &quot;kubernetes-dashboard-76894548fd-kkhrj&quot;,</span><br><span class=\"line\">        &quot;nginx-ingress-controller-4kkph&quot;,</span><br><span class=\"line\">        &quot;nginx-ingress-controller-h9cpd&quot;,</span><br><span class=\"line\">        &quot;nginx-75f4785b7-lq4p7&quot;,</span><br><span class=\"line\">        &quot;nginx-75f4785b7-qt5sx&quot;,</span><br><span class=\"line\">        &quot;nginx-75f4785b7-v4lxg&quot;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;code&quot;: 0</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"在jenkins里面使用\"><a href=\"#在jenkins里面使用\" class=\"headerlink\" title=\"在jenkins里面使用\"></a>在jenkins里面使用</h2><p>可以用pipeline用HttpRequest调用<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def resp;</span><br><span class=\"line\">def url = &quot;http://192.168.10.93:32180/list_pod_for_all_namespace&quot;</span><br><span class=\"line\">try &#123;</span><br><span class=\"line\">    resp = httpRequest httpMode: &apos;GET&apos;, url: &quot;$url&quot;</span><br><span class=\"line\">    println resp.result</span><br><span class=\"line\">&#125;catch(e)&#123;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>文章Java代码：<a href=\"https://github.com/dongamp1990/k8s-api-demo.git\" target=\"_blank\" rel=\"noopener\">https://github.com/dongamp1990/k8s-api-demo.git</a></p>\n"},{"title":"使用kubeadm快速构建集群","date":"2018-06-14T16:00:00.000Z","_content":"## 前言\n本文将介绍如何在Ubuntu server 16.03版本上安装kubeadm，构建一个kubernetes的基础的测试集群，用来做学习和测试用途，当前最新的版本是1.10.4\n使用CoreDNS替换kubeDNS，使用calico网络，部署dashboard，部署heapster+grafana+influxdb监控\n\n本次安装2台集群，1个master，1个node，可以根据自己需求增加node机器，每台服务器4G内存，2个CPU核心以上\n所有服务器使用Ubuntu server 16.03\n所有节点提前安装好Docker[安装示例](/2018/01/11/docker-install-note/)\n\n## 安装kubeadm\n在所有节点上安装kubeadm\n\n查看apt安装源如下配置，\n```\n$ cat /etc/apt/sources.list\n\n# kubeadm及kubernetes组件安装源\ndeb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main\n```\n\n更新源，可以不理会gpg的报错信息。\n```\n$ apt-get update\n\nIgn:4 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease\nFetched 8,993 B in 0s (20.7 kB/s)\nReading package lists... Done\nW: GPG error: https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nW: The repository 'https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease' is not signed.\nN: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.\nN: See apt-secure(8) manpage for repository creation and user configuration details.\n```\n\n强制安装kubeadm，kubectl，kubelet软件包。\n\n```\n$ apt-get install -y kubelet kubeadm kubectl kubernetes-cni --allow-unauthenticated\n```\nkubeadm安装完以后，就可以使用它来快速安装部署Kubernetes集群了。\n\n\n## 下载镜像文件\n因为在墙内的关系，访问不到gcr站点，所以需要先准备好所需镜像,提供的镜像是1.10.1版本的。\n地址: https://pan.baidu.com/s/1CmoDA3jQgD8kkF3QfI90qg 密码: fkxw\n下载kube1.10.tar.gz并解压。\n\nmaster节点load镜像\n```\n$ docker load -i kube-master.tar\n$ docker load -i calico-3.1.3.tar\n$ docker load -i coredns-1.1.3.tar\n```\n\nnode节点load镜像\n```\n$ docker load -i kube-node.tar\n$ docker load -i calico-3.1.3.tar\n$ docker load -i coredns-1.1.3.tar\n```\n\n## 初始化master节点\n--pod-network-cidr这个参数的ip地址可以根据实际情况修改，需要跟calico网络的CALICO_IPV4POOL_CIDR一致\n```\n$ kubeadm init --pod-network-cidr=172.16.1.0/16 --kubernetes-version v1.10.1\n\n[init] Using Kubernetes version: v1.10.1\n[init] Using Authorization modes: [Node RBAC]\n[preflight] Running pre-flight checks.\n\t[WARNING FileExisting-crictl]: crictl not found in system path\nSuggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl\n[preflight] Starting the kubelet service\n[certificates] Generated ca certificate and key.\n[certificates] Generated apiserver certificate and key.\n[certificates] apiserver serving cert is signed for DNS names [ubuntu-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.250]\n[certificates] Generated apiserver-kubelet-client certificate and key.\n[certificates] Generated etcd/ca certificate and key.\n[certificates] Generated etcd/server certificate and key.\n[certificates] etcd/server serving cert is signed for DNS names [localhost] and IPs [127.0.0.1]\n[certificates] Generated etcd/peer certificate and key.\n[certificates] etcd/peer serving cert is signed for DNS names [ubuntu-master] and IPs [192.168.10.250]\n[certificates] Generated etcd/healthcheck-client certificate and key.\n[certificates] Generated apiserver-etcd-client certificate and key.\n[certificates] Generated sa key and public key.\n[certificates] Generated front-proxy-ca certificate and key.\n[certificates] Generated front-proxy-client certificate and key.\n[certificates] Valid certificates and keys now exist in \"/etc/kubernetes/pki\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/controller-manager.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/scheduler.conf\"\n[controlplane] Wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"\n[controlplane] Wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"\n[controlplane] Wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"\n[etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\"\n[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\".\n[init] This might take a minute or longer if the control plane images have to be pulled.\n[apiclient] All control plane components are healthy after 28.003828 seconds\n[uploadconfig] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[markmaster] Will mark node ubuntu-master as master by adding a label and a taint\n[markmaster] Master ubuntu-master tainted and labelled with key/value: node-role.kubernetes.io/master=\"\"\n[bootstraptoken] Using token: rw4enn.mvk547juq7qi2b5f\n[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstraptoken] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[addons] Applied essential addon: kube-dns\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes master has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join 192.168.10.250:6443 --token rw4enn.mvk547juq7qi2b5f --discovery-token-ca-cert-hash sha256:ba260d5191213382a806a9a7d92c9e6bb09061847c7914b1ac584d0c69471579\n```\n\n执行如下命令来配置kubectl\n```\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n这样master的节点就配置好了，并且可以使用kubectl来进行各种操作了，根据上面的提示接着往下做，将slave节点加入到集群。\n\n## node节点加入集群\n在node节点上执行kubeadm join命令\n```\n$ kubeadm join 192.168.10.250:6443 --token rw4enn.mvk547juq7qi2b5f --discovery-token-ca-cert-hash sha256:ba260d5191213382a806a9a7d92c9e6bb09061847c7914b1ac584d0c69471579\n\n[preflight] Running pre-flight checks.\n\t[WARNING FileExisting-crictl]: crictl not found in system path\nSuggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl\n[discovery] Trying to connect to API Server \"192.168.10.250:6443\"\n[discovery] Created cluster-info discovery client, requesting info from \"https://192.168.10.250:6443\"\n[discovery] Requesting info from \"https://192.168.10.250:6443\" again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \"192.168.10.250:6443\"\n[discovery] Successfully established connection with API Server \"192.168.10.250:6443\"\n\nThis node has joined the cluster:\n* Certificate signing request was sent to master and a response\n  was received.\n* The Kubelet was informed of the new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this node join the cluster.\n```\n\n查看节点状态\n```\n$ kubectl get nodes\n\nNAME         STATUS    ROLES     AGE       VERSION\nk8s-master   Ready     master    1h        v1.10.4\nk8s-node1    Ready     <none>    1h        v1.10.4\n```\n\n在master节点查看pod状态\n```\n$ kubectl get pods -n kube-system -o wide\n```\n\n## 安装Calico网络\n\n[calico.yaml文件](https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml)\n```\n$ cat <<EOF > calico.yaml\n# Calico Version v3.1.3\n# https://docs.projectcalico.org/v3.1/releases#v3.1.3\n# This manifest includes the following component versions:\n#   calico/node:v3.1.3\n#   calico/cni:v3.1.3\n#   calico/kube-controllers:v3.1.3\n\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # The location of your etcd cluster.  This uses the Service clusterIP defined below.\n  etcd_endpoints: \"http://10.96.232.136:6666\"\n\n  # Configure the Calico backend to use.\n  calico_backend: \"bird\"\n\n  # The CNI network configuration to install on each node.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.0\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"etcd_endpoints\": \"__ETCD_ENDPOINTS__\",\n          \"log_level\": \"info\",\n          \"mtu\": 1500,\n          \"ipam\": {\n              \"type\": \"calico-ipam\",\n              \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"__KUBECONFIG_FILEPATH__\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n\n---\n\n# This manifest installs the Calico etcd on the kubeadm master.  This uses a DaemonSet\n# to force it to run on the master even when the master isn't schedulable, and uses\n# nodeSelector to ensure it only runs on the master.\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: calico-etcd\n  namespace: kube-system\n  labels:\n    k8s-app: calico-etcd\nspec:\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-etcd\n      annotations:\n        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler\n        # reserves resources for critical add-on pods so that they can be rescheduled after\n        # a failure.  This annotation works in tandem with the toleration below.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      tolerations:\n        # This taint is set by all kubelets running `--cloud-provider=external`\n        # so we should tolerate it to schedule the calico pods\n        - key: node.cloudprovider.kubernetes.io/uninitialized\n          value: \"true\"\n          effect: NoSchedule\n        # Allow this pod to run on the master.\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n        # Allow this pod to be rescheduled while the node is in \"critical add-ons only\" mode.\n        # This, along with the annotation above marks this pod as a critical add-on.\n        - key: CriticalAddonsOnly\n          operator: Exists\n      # Only run this pod on the master.\n      nodeSelector:\n        node-role.kubernetes.io/master: \"\"\n      hostNetwork: true\n      containers:\n        - name: calico-etcd\n          image: quay.io/coreos/etcd:v3.1.10\n          env:\n            - name: CALICO_ETCD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n          command:\n          - /usr/local/bin/etcd\n          args:\n          - --name=calico\n          - --data-dir=/var/etcd/calico-data\n          - --advertise-client-urls=http://$CALICO_ETCD_IP:6666\n          - --listen-client-urls=http://0.0.0.0:6666\n          - --listen-peer-urls=http://0.0.0.0:6667\n          - --auto-compaction-retention=1\n          volumeMounts:\n            - name: var-etcd\n              mountPath: /var/etcd\n      volumes:\n        - name: var-etcd\n          hostPath:\n            path: /var/etcd\n\n---\n\n# This manifest installs the Service which gets traffic to the Calico\n# etcd.\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: calico-etcd\n  name: calico-etcd\n  namespace: kube-system\nspec:\n  # Select the calico-etcd pod running on the master.\n  selector:\n    k8s-app: calico-etcd\n  # This ClusterIP needs to be known in advance, since we cannot rely\n  # on DNS to get access to etcd.\n  clusterIP: 10.96.232.136\n  ports:\n    - port: 6666\n\n---\n\n# This manifest installs the calico/node container, as well\n# as the Calico CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler\n        # reserves resources for critical add-on pods so that they can be rescheduled after\n        # a failure.  This annotation works in tandem with the toleration below.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      hostNetwork: true\n      tolerations:\n        # Make sure calico/node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      serviceAccountName: calico-cni-plugin\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      containers:\n        # Runs calico/node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: quay.io/calico/node:v3.1.3\n          env:\n            # The location of the Calico etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_endpoints\n            # Enable BGP.  Disable to enforce policy only.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"kubeadm,bgp\"\n            # Disable file logging so `kubectl logs` works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set noderef for node controller.\n            - name: CALICO_K8S_NODE_REF\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within `--cluster-cidr`.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"172.16.1.0/16\"\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              value: \"1440\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"info\"\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n        # This container installs the Calico CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: quay.io/calico/cni:v3.1.3\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The location of the Calico etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_endpoints\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n      volumes:\n        # Used by calico/node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n\n---\n\n# This manifest deploys the Calico Kubernetes controllers.\n# See https://github.com/projectcalico/kube-controllers\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\nspec:\n  # The controllers can only have a single active instance.\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n      annotations:\n        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler\n        # reserves resources for critical add-on pods so that they can be rescheduled after\n        # a failure.  This annotation works in tandem with the toleration below.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      # The controllers must run in the host network namespace so that\n      # it isn't governed by policy that would prevent it from working.\n      hostNetwork: true\n      tolerations:\n        # Allow this pod to be rescheduled while the node is in \"critical add-ons only\" mode.\n        # This, along with the annotation above marks this pod as a critical add-on.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n      serviceAccountName: calico-kube-controllers\n      containers:\n        - name: calico-kube-controllers\n          image: quay.io/calico/kube-controllers:v3.1.3\n          env:\n            # The location of the Calico etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_endpoints\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: policy,profile,workloadendpoint,node\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-cni-plugin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-cni-plugin\nsubjects:\n- kind: ServiceAccount\n  name: calico-cni-plugin\n  namespace: kube-system\n\n---\n\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-cni-plugin\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n    verbs:\n      - get\n\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-cni-plugin\n  namespace: kube-system\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n\n---\n\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  - apiGroups:\n    - \"\"\n    - extensions\n    resources:\n      - pods\n      - namespaces\n      - networkpolicies\n      - nodes\n    verbs:\n      - watch\n      - list\n  - apiGroups:\n    - networking.k8s.io\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n\nEOF\n```\n\n部署calico\n```\nkubectl apply -f calico.yaml\n```\n\n## 安装CoreDNS\n在/etc/kubernetes/manifests/下创建coredns目录,\n需要把官方提供的两个文件deploy.sh和coredns.yaml.sed放到conredns目录\n\n### 准备配置文件\n```\n$ mkdir -p /etc/kubernetes/manifests/coredns\n```\n\n[deploy.sh文件](https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh)\n```\n$ cat <<EOF > /etc/kubernetes/manifests/coredns/deploy.sh\n#!/bin/bash\n\n# Deploys CoreDNS to a cluster currently running Kube-DNS.\n\nshow_help () {\ncat << USAGE\nusage: $0 [ -r REVERSE-CIDR ] [ -i DNS-IP ] [ -d CLUSTER-DOMAIN ] [ -t YAML-TEMPLATE ]\n    -r : Define a reverse zone for the given CIDR. You may specifcy this option more\n         than once to add multiple reverse zones. If no reverse CIDRs are defined,\n         then the default is to handle all reverse zones (i.e. in-addr.arpa and ip6.arpa)\n    -i : Specify the cluster DNS IP address. If not specificed, the IP address of\n         the existing \"kube-dns\" service is used, if present.\nUSAGE\nexit 0\n}\n\n# Simple Defaults\nCLUSTER_DOMAIN=cluster.local\nYAML_TEMPLATE=`pwd`/coredns.yaml.sed\n\n\n# Get Opts\nwhile getopts \"hr:i:d:t:\" opt; do\n    case \"$opt\" in\n    h)  show_help\n        ;;\n    r)  REVERSE_CIDRS=\"$REVERSE_CIDRS $OPTARG\"\n        ;;\n    i)  CLUSTER_DNS_IP=$OPTARG\n        ;;\n    d)  CLUSTER_DOMAIN=$OPTARG\n        ;;\n    t)  YAML_TEMPLATE=$OPTARG\n        ;;\n    esac\ndone\n\n# Conditional Defaults\nif [[ -z $REVERSE_CIDRS ]]; then\n  REVERSE_CIDRS=\"in-addr.arpa ip6.arpa\"\nfi\nif [[ -z $CLUSTER_DNS_IP ]]; then\n  # Default IP to kube-dns IP\n  CLUSTER_DNS_IP=$(kubectl get service --namespace kube-system kube-dns -o jsonpath=\"{.spec.clusterIP}\")\n  if [ $? -ne 0 ]; then\n      >&2 echo \"Error! The IP address for DNS service couldn't be determined automatically. Please specify the DNS-IP with the '-i' option.\"\n      exit 2\n  fi\nfi\n\nsed -e s/CLUSTER_DNS_IP/$CLUSTER_DNS_IP/g -e s/CLUSTER_DOMAIN/$CLUSTER_DOMAIN/g -e \"s?REVERSE_CIDRS?$REVERSE_CIDRS?g\" $YAML_TEMPLATE\nEOF\n```\n\n[coredns.yaml.sed文件](https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed)\n```\n$ cat <<EOF > /etc/kubernetes/manifests/coredns/coredns.yaml.sed\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:coredns\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - endpoints\n  - services\n  - pods\n  - namespaces\n  verbs:\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:coredns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:coredns\nsubjects:\n- kind: ServiceAccount\n  name: coredns\n  namespace: kube-system\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health\n        kubernetes CLUSTER_DOMAIN REVERSE_CIDRS {\n          pods insecure\n          upstream\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n        proxy . /etc/resolv.conf\n        cache 30\n        reload\n    }\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n    spec:\n      serviceAccountName: coredns\n      tolerations:\n        - key: \"CriticalAddonsOnly\"\n          operator: \"Exists\"\n      containers:\n      - name: coredns\n        image: coredns/coredns:1.1.3\n        imagePullPolicy: IfNotPresent\n        args: [ \"-conf\", \"/etc/coredns/Corefile\" ]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n          readOnly: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - all\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n      dnsPolicy: Default\n      volumes:\n        - name: config-volume\n          configMap:\n            name: coredns\n            items:\n            - key: Corefile\n              path: Corefile\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  annotations:\n    prometheus.io/scrape: \"true\"\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: CLUSTER_DNS_IP\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\nEOF\n\n```\n\n### 使用CoreDNS替换Kube-DNS\n最佳的案例场景中，使用CoreDNS替换Kube-DNS只需要使用下面的两个命令：\n```\n$ ./deploy.sh | kubectl apply -f -\n$ kubectl delete --namespace=kube-system deployment kube-dns\n```\n\n查看pod情况\n```\n$ kubectl get pods -n kube-system -o wide\n\nNAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE\ncalico-etcd-2fn6s                          1/1       Running   0          7m        192.168.10.250   k8s-master\ncalico-kube-controllers-679568f47c-k7q4w   1/1       Running   0          7m        192.168.10.250   k8s-master\ncalico-node-8l2mf                          2/2       Running   0          7m        192.168.10.250   k8s-master\ncalico-node-gx4x8                          2/2       Running   0          5m        192.168.10.57    k8s-node1\ncoredns-6cc44759b8-8p8cb                   1/1       Running   1          6h        172.16.1.193     k8s-master\ncoredns-6cc44759b8-xzmfr                   1/1       Running   1          6h        172.16.1.194     k8s-master\netcd-k8s-master                            1/1       Running   3          7h        192.168.10.250   k8s-master\nheapster-77b9c5bd7b-cdr7d                  1/1       Running   0          4m        172.16.1.68      k8s-node1\nkube-apiserver-k8s-master                  1/1       Running   3          7h        192.168.10.250   k8s-master\nkube-controller-manager-k8s-master         1/1       Running   3          7h        192.168.10.250   k8s-master\nkube-proxy-7nvtx                           1/1       Running   3          7h        192.168.10.250   k8s-master\nkube-proxy-nh6jf                           1/1       Running   0          5m        192.168.10.57    k8s-node1\nkube-scheduler-k8s-master                  1/1       Running   3          7h        192.168.10.250   k8s-master\n```\n\n至此，node集群已经部署好了。\n\n## 部署Dashboard\nnode节点load dashboard镜像\n```\ndocker load -i kube-dashboard-1.8.3.tar\n```\n\ndashboard.yaml\n```\ncat <<EOF > dashboard.yaml\n# Copyright 2017 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Configuration to deploy release version of the Dashboard UI compatible with\n# Kubernetes 1.8.\n#\n# Example usage: kubectl create -f <this_file>\n\n# ------------------- Dashboard Secret ------------------- #\n\napiVersion: v1\nkind: Secret\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-certs\n  namespace: kube-system\ntype: Opaque\n\n---\n# ------------------- Dashboard Service Account ------------------- #\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\n\n---\n# ------------------- Dashboard Role & Role Binding ------------------- #\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: kubernetes-dashboard-minimal\n  namespace: kube-system\nrules:\n  # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"create\"]\n  # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"create\"]\n  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\"]\n  verbs: [\"get\", \"update\", \"delete\"]\n  # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"kubernetes-dashboard-settings\"]\n  verbs: [\"get\", \"update\"]\n  # Allow Dashboard to get metrics from heapster.\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  resourceNames: [\"heapster\"]\n  verbs: [\"proxy\"]\n- apiGroups: [\"\"]\n  resources: [\"services/proxy\"]\n  resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\"]\n  verbs: [\"get\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kubernetes-dashboard-minimal\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: kubernetes-dashboard-minimal\nsubjects:\n- kind: ServiceAccount\n  name: kubernetes-dashboard\n  namespace: kube-system\n\n---\n# ------------------- Dashboard Deployment ------------------- #\n\nkind: Deployment\napiVersion: apps/v1beta2\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard\n    spec:\n      containers:\n      - name: kubernetes-dashboard\n        image: reg.qiniu.com/k8s/kubernetes-dashboard-amd64:v1.8.3\n        ports:\n        - containerPort: 8443\n          protocol: TCP\n        args:\n          - --auto-generate-certificates\n          # Uncomment the following line to manually specify Kubernetes API server Host\n          # If not specified, Dashboard will attempt to auto discover the API server and connect\n          # to it. Uncomment only if the default does not work.\n          # - --apiserver-host=http://my-address:port\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n          # Create on-disk volume to store exec logs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /\n            port: 8443\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n      # Comment the following tolerations if Dashboard must not be deployed on master\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n\n---\n# ------------------- Dashboard Service ------------------- #\n\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  ports:\n    - port: 443\n      targetPort: 8443\n      #以https://ip:nodeport访问dashboard\n      nodePort: 32500\n  type: NodePort\n  selector:\n    k8s-app: kubernetes-dashboard\n\nEOF\n```\n\n部署dashboard\n```\n$ kubectl apply -f dashboard.yaml\n```\n部署dashboard后，为了安全默认是不能匿名访问dashboard的，所以要创建一个cluster-admin角色的用户，使用token登录dashboard\n\nadmin-user.yaml\n```\ncat <<EOF > admin-user.yaml\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    k8s-app: kubernetes-dashboard\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kube-system\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kube-system\n  labels:\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n\nEOF\n```\n\n创建admin-user用户\n```\n$ kubectl apply -f admin-user.yaml\n```\n\n查看admin-user用户token， 使用token去登录dashboard\n```\n$kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')\n\nName:         admin-user-token-nfsg4\nNamespace:    kube-system\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name=admin-user\n              kubernetes.io/service-account.uid=6a91db25-704e-11e8-8055-080027c6c772\n\nType:  kubernetes.io/service-account-token\n\nData\n====\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLW5mc2c0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2YTkxZGIyNS03MDRlLTExZTgtODA1NS0wODAwMjdjNmM3NzIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.Ue49ptrBZItvG7f8gavEQSoxtSgbz-bh3jZU9RbjLJh2nkmueTVh5WViauUd5TxsnmefB3DDLgTp_CfwWrV0Uyf7JT6GEDMvjDVhsyHa2qyjLwRfvf1vmnx2P7975yRQniq_a3fgOhdNn0zCL2Er9bDUNabCcR0ubNW6I63kp3-UYGIj9OfAkfqNCTFtjZgvcGvnvkhj2pm_peTJ6H3qmxhcb9WM90Nh77p5qdI8gjk2EdKPAhsOmOxBWSSnHqHPr0gVStsewQEHo0CQOr4MxE3NIg_gwjTbn0KD9vvJeECPAX_zsq7ejD0POoWvSXz8khwvv8yLgJQA-ZkUXJKk7w\nca.crt:     1025 bytes\nnamespace:  11 bytes\n```\n\n访问https://ip:32500即可打开dashboard页面\n\n## 部署heapster、grafana、influxdb\n\n### 配置文件\nheapster.yaml\n```\ncat <<EOF > heapster.yaml\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: heapster\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:heapster\nsubjects:\n- kind: ServiceAccount\n  name: heapster\n  namespace: kube-system\n\n---  \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: heapster\n  namespace: kube-system\n  \n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: mirrorgooglecontainers/heapster-amd64:v1.5.3\n        imagePullPolicy: IfNotPresent\n        #volumeMounts: \n        #- mountPath: /root/.kube\n        #  name: config\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: Heapster\n  name: heapster\n  namespace: kube-system\nspec:\n  ports:\n  - port: 80\n    targetPort: 8082\n  selector:\n    k8s-app: heapster\n  \nEOF\n```\n\ngrafana.yaml\n```\ncat <<EOF > grafana.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: mirrorgooglecontainers/heapster-grafana-amd64:v4.4.3\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /etc/ssl/certs\n          name: ca-certificates\n          readOnly: true\n        - mountPath: /var\n          name: grafana-storage\n        env:\n        - name: INFLUXDB_HOST\n          value: monitoring-influxdb\n        - name: GF_SERVER_HTTP_PORT\n          value: \"3000\"\n          # The following env variables are required to make Grafana accessible via\n          # the kubernetes api-server proxy. On production clusters, we recommend\n          # removing these env variables, setup auth for grafana, and expose the grafana\n          # service using a LoadBalancer or a public IP.\n        - name: GF_AUTH_BASIC_ENABLED\n          value: \"false\"\n        - name: GF_AUTH_ANONYMOUS_ENABLED\n          value: \"true\"\n        - name: GF_AUTH_ANONYMOUS_ORG_ROLE\n          value: Admin\n        - name: GF_SERVER_ROOT_URL\n          # If you're only using the API Server proxy, set this value instead:\n          # value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy\n          value: /\n      volumes:\n      - name: ca-certificates\n        hostPath:\n          path: /etc/ssl/certs\n      - name: grafana-storage\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-grafana\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  # In a production setup, we recommend accessing Grafana through an external Loadbalancer\n  # or through a public IP.\n  # type: LoadBalancer\n  # You could also use NodePort to expose the service at a randomly-generated port\n  # type: NodePort\n  ports:\n  - port: 80\n    targetPort: 3000\n    nodePort: 32600\n  type: NodePort\n  selector:\n    k8s-app: grafana\n\nEOF\n```\n\ninfluxdb.yaml\n```\ncat <<EOF > influxdb.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: influxdb\n    spec:\n      containers:\n      - name: influxdb\n        image: mirrorgooglecontainers/heapster-influxdb-amd64:v1.3.3\n        volumeMounts:\n        - mountPath: /data\n          name: influxdb-storage\n      volumes:\n      - name: influxdb-storage\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-influxdb\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  ports:\n  - port: 8086\n    targetPort: 8086\n  selector:\n    k8s-app: influxdb\nEOF    \n```\n\n### 部署\n```\n$ kubectl create -f grafana.yaml\n$ kubectl create -f heapster.yaml\n$ kubectl create -f influxdb.yaml\n\n```\n访问http://ip:32600即可访问到grafana ui了\n\n\n参考资料:\nhttps://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/calico\nhttps://jimmysong.io/kubernetes-handbook/practice/coredns.html","source":"_posts/kubernetes-kubeadmin-init.md","raw":"---\ntitle: 使用kubeadm快速构建集群\ntags: [kubernetes]\ndate: 2018-06-15\n---\n## 前言\n本文将介绍如何在Ubuntu server 16.03版本上安装kubeadm，构建一个kubernetes的基础的测试集群，用来做学习和测试用途，当前最新的版本是1.10.4\n使用CoreDNS替换kubeDNS，使用calico网络，部署dashboard，部署heapster+grafana+influxdb监控\n\n本次安装2台集群，1个master，1个node，可以根据自己需求增加node机器，每台服务器4G内存，2个CPU核心以上\n所有服务器使用Ubuntu server 16.03\n所有节点提前安装好Docker[安装示例](/2018/01/11/docker-install-note/)\n\n## 安装kubeadm\n在所有节点上安装kubeadm\n\n查看apt安装源如下配置，\n```\n$ cat /etc/apt/sources.list\n\n# kubeadm及kubernetes组件安装源\ndeb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main\n```\n\n更新源，可以不理会gpg的报错信息。\n```\n$ apt-get update\n\nIgn:4 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease\nFetched 8,993 B in 0s (20.7 kB/s)\nReading package lists... Done\nW: GPG error: https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB\nW: The repository 'https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease' is not signed.\nN: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.\nN: See apt-secure(8) manpage for repository creation and user configuration details.\n```\n\n强制安装kubeadm，kubectl，kubelet软件包。\n\n```\n$ apt-get install -y kubelet kubeadm kubectl kubernetes-cni --allow-unauthenticated\n```\nkubeadm安装完以后，就可以使用它来快速安装部署Kubernetes集群了。\n\n\n## 下载镜像文件\n因为在墙内的关系，访问不到gcr站点，所以需要先准备好所需镜像,提供的镜像是1.10.1版本的。\n地址: https://pan.baidu.com/s/1CmoDA3jQgD8kkF3QfI90qg 密码: fkxw\n下载kube1.10.tar.gz并解压。\n\nmaster节点load镜像\n```\n$ docker load -i kube-master.tar\n$ docker load -i calico-3.1.3.tar\n$ docker load -i coredns-1.1.3.tar\n```\n\nnode节点load镜像\n```\n$ docker load -i kube-node.tar\n$ docker load -i calico-3.1.3.tar\n$ docker load -i coredns-1.1.3.tar\n```\n\n## 初始化master节点\n--pod-network-cidr这个参数的ip地址可以根据实际情况修改，需要跟calico网络的CALICO_IPV4POOL_CIDR一致\n```\n$ kubeadm init --pod-network-cidr=172.16.1.0/16 --kubernetes-version v1.10.1\n\n[init] Using Kubernetes version: v1.10.1\n[init] Using Authorization modes: [Node RBAC]\n[preflight] Running pre-flight checks.\n\t[WARNING FileExisting-crictl]: crictl not found in system path\nSuggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl\n[preflight] Starting the kubelet service\n[certificates] Generated ca certificate and key.\n[certificates] Generated apiserver certificate and key.\n[certificates] apiserver serving cert is signed for DNS names [ubuntu-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.250]\n[certificates] Generated apiserver-kubelet-client certificate and key.\n[certificates] Generated etcd/ca certificate and key.\n[certificates] Generated etcd/server certificate and key.\n[certificates] etcd/server serving cert is signed for DNS names [localhost] and IPs [127.0.0.1]\n[certificates] Generated etcd/peer certificate and key.\n[certificates] etcd/peer serving cert is signed for DNS names [ubuntu-master] and IPs [192.168.10.250]\n[certificates] Generated etcd/healthcheck-client certificate and key.\n[certificates] Generated apiserver-etcd-client certificate and key.\n[certificates] Generated sa key and public key.\n[certificates] Generated front-proxy-ca certificate and key.\n[certificates] Generated front-proxy-client certificate and key.\n[certificates] Valid certificates and keys now exist in \"/etc/kubernetes/pki\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/controller-manager.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/scheduler.conf\"\n[controlplane] Wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"\n[controlplane] Wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"\n[controlplane] Wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"\n[etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\"\n[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\".\n[init] This might take a minute or longer if the control plane images have to be pulled.\n[apiclient] All control plane components are healthy after 28.003828 seconds\n[uploadconfig] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[markmaster] Will mark node ubuntu-master as master by adding a label and a taint\n[markmaster] Master ubuntu-master tainted and labelled with key/value: node-role.kubernetes.io/master=\"\"\n[bootstraptoken] Using token: rw4enn.mvk547juq7qi2b5f\n[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstraptoken] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[addons] Applied essential addon: kube-dns\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes master has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join 192.168.10.250:6443 --token rw4enn.mvk547juq7qi2b5f --discovery-token-ca-cert-hash sha256:ba260d5191213382a806a9a7d92c9e6bb09061847c7914b1ac584d0c69471579\n```\n\n执行如下命令来配置kubectl\n```\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n这样master的节点就配置好了，并且可以使用kubectl来进行各种操作了，根据上面的提示接着往下做，将slave节点加入到集群。\n\n## node节点加入集群\n在node节点上执行kubeadm join命令\n```\n$ kubeadm join 192.168.10.250:6443 --token rw4enn.mvk547juq7qi2b5f --discovery-token-ca-cert-hash sha256:ba260d5191213382a806a9a7d92c9e6bb09061847c7914b1ac584d0c69471579\n\n[preflight] Running pre-flight checks.\n\t[WARNING FileExisting-crictl]: crictl not found in system path\nSuggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl\n[discovery] Trying to connect to API Server \"192.168.10.250:6443\"\n[discovery] Created cluster-info discovery client, requesting info from \"https://192.168.10.250:6443\"\n[discovery] Requesting info from \"https://192.168.10.250:6443\" again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \"192.168.10.250:6443\"\n[discovery] Successfully established connection with API Server \"192.168.10.250:6443\"\n\nThis node has joined the cluster:\n* Certificate signing request was sent to master and a response\n  was received.\n* The Kubelet was informed of the new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this node join the cluster.\n```\n\n查看节点状态\n```\n$ kubectl get nodes\n\nNAME         STATUS    ROLES     AGE       VERSION\nk8s-master   Ready     master    1h        v1.10.4\nk8s-node1    Ready     <none>    1h        v1.10.4\n```\n\n在master节点查看pod状态\n```\n$ kubectl get pods -n kube-system -o wide\n```\n\n## 安装Calico网络\n\n[calico.yaml文件](https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml)\n```\n$ cat <<EOF > calico.yaml\n# Calico Version v3.1.3\n# https://docs.projectcalico.org/v3.1/releases#v3.1.3\n# This manifest includes the following component versions:\n#   calico/node:v3.1.3\n#   calico/cni:v3.1.3\n#   calico/kube-controllers:v3.1.3\n\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # The location of your etcd cluster.  This uses the Service clusterIP defined below.\n  etcd_endpoints: \"http://10.96.232.136:6666\"\n\n  # Configure the Calico backend to use.\n  calico_backend: \"bird\"\n\n  # The CNI network configuration to install on each node.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.0\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"etcd_endpoints\": \"__ETCD_ENDPOINTS__\",\n          \"log_level\": \"info\",\n          \"mtu\": 1500,\n          \"ipam\": {\n              \"type\": \"calico-ipam\",\n              \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"__KUBECONFIG_FILEPATH__\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n\n---\n\n# This manifest installs the Calico etcd on the kubeadm master.  This uses a DaemonSet\n# to force it to run on the master even when the master isn't schedulable, and uses\n# nodeSelector to ensure it only runs on the master.\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: calico-etcd\n  namespace: kube-system\n  labels:\n    k8s-app: calico-etcd\nspec:\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-etcd\n      annotations:\n        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler\n        # reserves resources for critical add-on pods so that they can be rescheduled after\n        # a failure.  This annotation works in tandem with the toleration below.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      tolerations:\n        # This taint is set by all kubelets running `--cloud-provider=external`\n        # so we should tolerate it to schedule the calico pods\n        - key: node.cloudprovider.kubernetes.io/uninitialized\n          value: \"true\"\n          effect: NoSchedule\n        # Allow this pod to run on the master.\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n        # Allow this pod to be rescheduled while the node is in \"critical add-ons only\" mode.\n        # This, along with the annotation above marks this pod as a critical add-on.\n        - key: CriticalAddonsOnly\n          operator: Exists\n      # Only run this pod on the master.\n      nodeSelector:\n        node-role.kubernetes.io/master: \"\"\n      hostNetwork: true\n      containers:\n        - name: calico-etcd\n          image: quay.io/coreos/etcd:v3.1.10\n          env:\n            - name: CALICO_ETCD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n          command:\n          - /usr/local/bin/etcd\n          args:\n          - --name=calico\n          - --data-dir=/var/etcd/calico-data\n          - --advertise-client-urls=http://$CALICO_ETCD_IP:6666\n          - --listen-client-urls=http://0.0.0.0:6666\n          - --listen-peer-urls=http://0.0.0.0:6667\n          - --auto-compaction-retention=1\n          volumeMounts:\n            - name: var-etcd\n              mountPath: /var/etcd\n      volumes:\n        - name: var-etcd\n          hostPath:\n            path: /var/etcd\n\n---\n\n# This manifest installs the Service which gets traffic to the Calico\n# etcd.\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: calico-etcd\n  name: calico-etcd\n  namespace: kube-system\nspec:\n  # Select the calico-etcd pod running on the master.\n  selector:\n    k8s-app: calico-etcd\n  # This ClusterIP needs to be known in advance, since we cannot rely\n  # on DNS to get access to etcd.\n  clusterIP: 10.96.232.136\n  ports:\n    - port: 6666\n\n---\n\n# This manifest installs the calico/node container, as well\n# as the Calico CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler\n        # reserves resources for critical add-on pods so that they can be rescheduled after\n        # a failure.  This annotation works in tandem with the toleration below.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      hostNetwork: true\n      tolerations:\n        # Make sure calico/node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      serviceAccountName: calico-cni-plugin\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      containers:\n        # Runs calico/node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: quay.io/calico/node:v3.1.3\n          env:\n            # The location of the Calico etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_endpoints\n            # Enable BGP.  Disable to enforce policy only.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"kubeadm,bgp\"\n            # Disable file logging so `kubectl logs` works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set noderef for node controller.\n            - name: CALICO_K8S_NODE_REF\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within `--cluster-cidr`.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"172.16.1.0/16\"\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              value: \"1440\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"info\"\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n        # This container installs the Calico CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: quay.io/calico/cni:v3.1.3\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The location of the Calico etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_endpoints\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n      volumes:\n        # Used by calico/node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n\n---\n\n# This manifest deploys the Calico Kubernetes controllers.\n# See https://github.com/projectcalico/kube-controllers\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\nspec:\n  # The controllers can only have a single active instance.\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n      annotations:\n        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler\n        # reserves resources for critical add-on pods so that they can be rescheduled after\n        # a failure.  This annotation works in tandem with the toleration below.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      # The controllers must run in the host network namespace so that\n      # it isn't governed by policy that would prevent it from working.\n      hostNetwork: true\n      tolerations:\n        # Allow this pod to be rescheduled while the node is in \"critical add-ons only\" mode.\n        # This, along with the annotation above marks this pod as a critical add-on.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n      serviceAccountName: calico-kube-controllers\n      containers:\n        - name: calico-kube-controllers\n          image: quay.io/calico/kube-controllers:v3.1.3\n          env:\n            # The location of the Calico etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_endpoints\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: policy,profile,workloadendpoint,node\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-cni-plugin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-cni-plugin\nsubjects:\n- kind: ServiceAccount\n  name: calico-cni-plugin\n  namespace: kube-system\n\n---\n\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-cni-plugin\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n    verbs:\n      - get\n\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-cni-plugin\n  namespace: kube-system\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n\n---\n\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  - apiGroups:\n    - \"\"\n    - extensions\n    resources:\n      - pods\n      - namespaces\n      - networkpolicies\n      - nodes\n    verbs:\n      - watch\n      - list\n  - apiGroups:\n    - networking.k8s.io\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n\nEOF\n```\n\n部署calico\n```\nkubectl apply -f calico.yaml\n```\n\n## 安装CoreDNS\n在/etc/kubernetes/manifests/下创建coredns目录,\n需要把官方提供的两个文件deploy.sh和coredns.yaml.sed放到conredns目录\n\n### 准备配置文件\n```\n$ mkdir -p /etc/kubernetes/manifests/coredns\n```\n\n[deploy.sh文件](https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh)\n```\n$ cat <<EOF > /etc/kubernetes/manifests/coredns/deploy.sh\n#!/bin/bash\n\n# Deploys CoreDNS to a cluster currently running Kube-DNS.\n\nshow_help () {\ncat << USAGE\nusage: $0 [ -r REVERSE-CIDR ] [ -i DNS-IP ] [ -d CLUSTER-DOMAIN ] [ -t YAML-TEMPLATE ]\n    -r : Define a reverse zone for the given CIDR. You may specifcy this option more\n         than once to add multiple reverse zones. If no reverse CIDRs are defined,\n         then the default is to handle all reverse zones (i.e. in-addr.arpa and ip6.arpa)\n    -i : Specify the cluster DNS IP address. If not specificed, the IP address of\n         the existing \"kube-dns\" service is used, if present.\nUSAGE\nexit 0\n}\n\n# Simple Defaults\nCLUSTER_DOMAIN=cluster.local\nYAML_TEMPLATE=`pwd`/coredns.yaml.sed\n\n\n# Get Opts\nwhile getopts \"hr:i:d:t:\" opt; do\n    case \"$opt\" in\n    h)  show_help\n        ;;\n    r)  REVERSE_CIDRS=\"$REVERSE_CIDRS $OPTARG\"\n        ;;\n    i)  CLUSTER_DNS_IP=$OPTARG\n        ;;\n    d)  CLUSTER_DOMAIN=$OPTARG\n        ;;\n    t)  YAML_TEMPLATE=$OPTARG\n        ;;\n    esac\ndone\n\n# Conditional Defaults\nif [[ -z $REVERSE_CIDRS ]]; then\n  REVERSE_CIDRS=\"in-addr.arpa ip6.arpa\"\nfi\nif [[ -z $CLUSTER_DNS_IP ]]; then\n  # Default IP to kube-dns IP\n  CLUSTER_DNS_IP=$(kubectl get service --namespace kube-system kube-dns -o jsonpath=\"{.spec.clusterIP}\")\n  if [ $? -ne 0 ]; then\n      >&2 echo \"Error! The IP address for DNS service couldn't be determined automatically. Please specify the DNS-IP with the '-i' option.\"\n      exit 2\n  fi\nfi\n\nsed -e s/CLUSTER_DNS_IP/$CLUSTER_DNS_IP/g -e s/CLUSTER_DOMAIN/$CLUSTER_DOMAIN/g -e \"s?REVERSE_CIDRS?$REVERSE_CIDRS?g\" $YAML_TEMPLATE\nEOF\n```\n\n[coredns.yaml.sed文件](https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed)\n```\n$ cat <<EOF > /etc/kubernetes/manifests/coredns/coredns.yaml.sed\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:coredns\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - endpoints\n  - services\n  - pods\n  - namespaces\n  verbs:\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:coredns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:coredns\nsubjects:\n- kind: ServiceAccount\n  name: coredns\n  namespace: kube-system\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health\n        kubernetes CLUSTER_DOMAIN REVERSE_CIDRS {\n          pods insecure\n          upstream\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n        proxy . /etc/resolv.conf\n        cache 30\n        reload\n    }\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n    spec:\n      serviceAccountName: coredns\n      tolerations:\n        - key: \"CriticalAddonsOnly\"\n          operator: \"Exists\"\n      containers:\n      - name: coredns\n        image: coredns/coredns:1.1.3\n        imagePullPolicy: IfNotPresent\n        args: [ \"-conf\", \"/etc/coredns/Corefile\" ]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n          readOnly: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - all\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n      dnsPolicy: Default\n      volumes:\n        - name: config-volume\n          configMap:\n            name: coredns\n            items:\n            - key: Corefile\n              path: Corefile\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  annotations:\n    prometheus.io/scrape: \"true\"\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: CLUSTER_DNS_IP\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\nEOF\n\n```\n\n### 使用CoreDNS替换Kube-DNS\n最佳的案例场景中，使用CoreDNS替换Kube-DNS只需要使用下面的两个命令：\n```\n$ ./deploy.sh | kubectl apply -f -\n$ kubectl delete --namespace=kube-system deployment kube-dns\n```\n\n查看pod情况\n```\n$ kubectl get pods -n kube-system -o wide\n\nNAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE\ncalico-etcd-2fn6s                          1/1       Running   0          7m        192.168.10.250   k8s-master\ncalico-kube-controllers-679568f47c-k7q4w   1/1       Running   0          7m        192.168.10.250   k8s-master\ncalico-node-8l2mf                          2/2       Running   0          7m        192.168.10.250   k8s-master\ncalico-node-gx4x8                          2/2       Running   0          5m        192.168.10.57    k8s-node1\ncoredns-6cc44759b8-8p8cb                   1/1       Running   1          6h        172.16.1.193     k8s-master\ncoredns-6cc44759b8-xzmfr                   1/1       Running   1          6h        172.16.1.194     k8s-master\netcd-k8s-master                            1/1       Running   3          7h        192.168.10.250   k8s-master\nheapster-77b9c5bd7b-cdr7d                  1/1       Running   0          4m        172.16.1.68      k8s-node1\nkube-apiserver-k8s-master                  1/1       Running   3          7h        192.168.10.250   k8s-master\nkube-controller-manager-k8s-master         1/1       Running   3          7h        192.168.10.250   k8s-master\nkube-proxy-7nvtx                           1/1       Running   3          7h        192.168.10.250   k8s-master\nkube-proxy-nh6jf                           1/1       Running   0          5m        192.168.10.57    k8s-node1\nkube-scheduler-k8s-master                  1/1       Running   3          7h        192.168.10.250   k8s-master\n```\n\n至此，node集群已经部署好了。\n\n## 部署Dashboard\nnode节点load dashboard镜像\n```\ndocker load -i kube-dashboard-1.8.3.tar\n```\n\ndashboard.yaml\n```\ncat <<EOF > dashboard.yaml\n# Copyright 2017 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Configuration to deploy release version of the Dashboard UI compatible with\n# Kubernetes 1.8.\n#\n# Example usage: kubectl create -f <this_file>\n\n# ------------------- Dashboard Secret ------------------- #\n\napiVersion: v1\nkind: Secret\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-certs\n  namespace: kube-system\ntype: Opaque\n\n---\n# ------------------- Dashboard Service Account ------------------- #\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\n\n---\n# ------------------- Dashboard Role & Role Binding ------------------- #\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: kubernetes-dashboard-minimal\n  namespace: kube-system\nrules:\n  # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"create\"]\n  # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"create\"]\n  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\"]\n  verbs: [\"get\", \"update\", \"delete\"]\n  # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"kubernetes-dashboard-settings\"]\n  verbs: [\"get\", \"update\"]\n  # Allow Dashboard to get metrics from heapster.\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  resourceNames: [\"heapster\"]\n  verbs: [\"proxy\"]\n- apiGroups: [\"\"]\n  resources: [\"services/proxy\"]\n  resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\"]\n  verbs: [\"get\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kubernetes-dashboard-minimal\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: kubernetes-dashboard-minimal\nsubjects:\n- kind: ServiceAccount\n  name: kubernetes-dashboard\n  namespace: kube-system\n\n---\n# ------------------- Dashboard Deployment ------------------- #\n\nkind: Deployment\napiVersion: apps/v1beta2\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard\n    spec:\n      containers:\n      - name: kubernetes-dashboard\n        image: reg.qiniu.com/k8s/kubernetes-dashboard-amd64:v1.8.3\n        ports:\n        - containerPort: 8443\n          protocol: TCP\n        args:\n          - --auto-generate-certificates\n          # Uncomment the following line to manually specify Kubernetes API server Host\n          # If not specified, Dashboard will attempt to auto discover the API server and connect\n          # to it. Uncomment only if the default does not work.\n          # - --apiserver-host=http://my-address:port\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n          # Create on-disk volume to store exec logs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /\n            port: 8443\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n      # Comment the following tolerations if Dashboard must not be deployed on master\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n\n---\n# ------------------- Dashboard Service ------------------- #\n\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  ports:\n    - port: 443\n      targetPort: 8443\n      #以https://ip:nodeport访问dashboard\n      nodePort: 32500\n  type: NodePort\n  selector:\n    k8s-app: kubernetes-dashboard\n\nEOF\n```\n\n部署dashboard\n```\n$ kubectl apply -f dashboard.yaml\n```\n部署dashboard后，为了安全默认是不能匿名访问dashboard的，所以要创建一个cluster-admin角色的用户，使用token登录dashboard\n\nadmin-user.yaml\n```\ncat <<EOF > admin-user.yaml\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    k8s-app: kubernetes-dashboard\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kube-system\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kube-system\n  labels:\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n\nEOF\n```\n\n创建admin-user用户\n```\n$ kubectl apply -f admin-user.yaml\n```\n\n查看admin-user用户token， 使用token去登录dashboard\n```\n$kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')\n\nName:         admin-user-token-nfsg4\nNamespace:    kube-system\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name=admin-user\n              kubernetes.io/service-account.uid=6a91db25-704e-11e8-8055-080027c6c772\n\nType:  kubernetes.io/service-account-token\n\nData\n====\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLW5mc2c0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2YTkxZGIyNS03MDRlLTExZTgtODA1NS0wODAwMjdjNmM3NzIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.Ue49ptrBZItvG7f8gavEQSoxtSgbz-bh3jZU9RbjLJh2nkmueTVh5WViauUd5TxsnmefB3DDLgTp_CfwWrV0Uyf7JT6GEDMvjDVhsyHa2qyjLwRfvf1vmnx2P7975yRQniq_a3fgOhdNn0zCL2Er9bDUNabCcR0ubNW6I63kp3-UYGIj9OfAkfqNCTFtjZgvcGvnvkhj2pm_peTJ6H3qmxhcb9WM90Nh77p5qdI8gjk2EdKPAhsOmOxBWSSnHqHPr0gVStsewQEHo0CQOr4MxE3NIg_gwjTbn0KD9vvJeECPAX_zsq7ejD0POoWvSXz8khwvv8yLgJQA-ZkUXJKk7w\nca.crt:     1025 bytes\nnamespace:  11 bytes\n```\n\n访问https://ip:32500即可打开dashboard页面\n\n## 部署heapster、grafana、influxdb\n\n### 配置文件\nheapster.yaml\n```\ncat <<EOF > heapster.yaml\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: heapster\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:heapster\nsubjects:\n- kind: ServiceAccount\n  name: heapster\n  namespace: kube-system\n\n---  \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: heapster\n  namespace: kube-system\n  \n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: mirrorgooglecontainers/heapster-amd64:v1.5.3\n        imagePullPolicy: IfNotPresent\n        #volumeMounts: \n        #- mountPath: /root/.kube\n        #  name: config\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: Heapster\n  name: heapster\n  namespace: kube-system\nspec:\n  ports:\n  - port: 80\n    targetPort: 8082\n  selector:\n    k8s-app: heapster\n  \nEOF\n```\n\ngrafana.yaml\n```\ncat <<EOF > grafana.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: mirrorgooglecontainers/heapster-grafana-amd64:v4.4.3\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /etc/ssl/certs\n          name: ca-certificates\n          readOnly: true\n        - mountPath: /var\n          name: grafana-storage\n        env:\n        - name: INFLUXDB_HOST\n          value: monitoring-influxdb\n        - name: GF_SERVER_HTTP_PORT\n          value: \"3000\"\n          # The following env variables are required to make Grafana accessible via\n          # the kubernetes api-server proxy. On production clusters, we recommend\n          # removing these env variables, setup auth for grafana, and expose the grafana\n          # service using a LoadBalancer or a public IP.\n        - name: GF_AUTH_BASIC_ENABLED\n          value: \"false\"\n        - name: GF_AUTH_ANONYMOUS_ENABLED\n          value: \"true\"\n        - name: GF_AUTH_ANONYMOUS_ORG_ROLE\n          value: Admin\n        - name: GF_SERVER_ROOT_URL\n          # If you're only using the API Server proxy, set this value instead:\n          # value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy\n          value: /\n      volumes:\n      - name: ca-certificates\n        hostPath:\n          path: /etc/ssl/certs\n      - name: grafana-storage\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-grafana\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  # In a production setup, we recommend accessing Grafana through an external Loadbalancer\n  # or through a public IP.\n  # type: LoadBalancer\n  # You could also use NodePort to expose the service at a randomly-generated port\n  # type: NodePort\n  ports:\n  - port: 80\n    targetPort: 3000\n    nodePort: 32600\n  type: NodePort\n  selector:\n    k8s-app: grafana\n\nEOF\n```\n\ninfluxdb.yaml\n```\ncat <<EOF > influxdb.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: influxdb\n    spec:\n      containers:\n      - name: influxdb\n        image: mirrorgooglecontainers/heapster-influxdb-amd64:v1.3.3\n        volumeMounts:\n        - mountPath: /data\n          name: influxdb-storage\n      volumes:\n      - name: influxdb-storage\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-influxdb\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  ports:\n  - port: 8086\n    targetPort: 8086\n  selector:\n    k8s-app: influxdb\nEOF    \n```\n\n### 部署\n```\n$ kubectl create -f grafana.yaml\n$ kubectl create -f heapster.yaml\n$ kubectl create -f influxdb.yaml\n\n```\n访问http://ip:32600即可访问到grafana ui了\n\n\n参考资料:\nhttps://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/calico\nhttps://jimmysong.io/kubernetes-handbook/practice/coredns.html","slug":"kubernetes-kubeadmin-init","published":1,"updated":"2018-06-19T03:53:56.396Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjncsum4x000d7gvriie93tcn","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>本文将介绍如何在Ubuntu server 16.03版本上安装kubeadm，构建一个kubernetes的基础的测试集群，用来做学习和测试用途，当前最新的版本是1.10.4<br>使用CoreDNS替换kubeDNS，使用calico网络，部署dashboard，部署heapster+grafana+influxdb监控</p>\n<p>本次安装2台集群，1个master，1个node，可以根据自己需求增加node机器，每台服务器4G内存，2个CPU核心以上<br>所有服务器使用Ubuntu server 16.03<br>所有节点提前安装好Docker<a href=\"/2018/01/11/docker-install-note/\">安装示例</a></p>\n<h2 id=\"安装kubeadm\"><a href=\"#安装kubeadm\" class=\"headerlink\" title=\"安装kubeadm\"></a>安装kubeadm</h2><p>在所有节点上安装kubeadm</p>\n<p>查看apt安装源如下配置，<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat /etc/apt/sources.list</span><br><span class=\"line\"></span><br><span class=\"line\"># kubeadm及kubernetes组件安装源</span><br><span class=\"line\">deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main</span><br></pre></td></tr></table></figure></p>\n<p>更新源，可以不理会gpg的报错信息。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ apt-get update</span><br><span class=\"line\"></span><br><span class=\"line\">Ign:4 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease</span><br><span class=\"line\">Fetched 8,993 B in 0s (20.7 kB/s)</span><br><span class=\"line\">Reading package lists... Done</span><br><span class=\"line\">W: GPG error: https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease: The following signatures couldn&apos;t be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB</span><br><span class=\"line\">W: The repository &apos;https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease&apos; is not signed.</span><br><span class=\"line\">N: Data from such a repository can&apos;t be authenticated and is therefore potentially dangerous to use.</span><br><span class=\"line\">N: See apt-secure(8) manpage for repository creation and user configuration details.</span><br></pre></td></tr></table></figure></p>\n<p>强制安装kubeadm，kubectl，kubelet软件包。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ apt-get install -y kubelet kubeadm kubectl kubernetes-cni --allow-unauthenticated</span><br></pre></td></tr></table></figure>\n<p>kubeadm安装完以后，就可以使用它来快速安装部署Kubernetes集群了。</p>\n<h2 id=\"下载镜像文件\"><a href=\"#下载镜像文件\" class=\"headerlink\" title=\"下载镜像文件\"></a>下载镜像文件</h2><p>因为在墙内的关系，访问不到gcr站点，所以需要先准备好所需镜像,提供的镜像是1.10.1版本的。<br>地址: <a href=\"https://pan.baidu.com/s/1CmoDA3jQgD8kkF3QfI90qg\" target=\"_blank\" rel=\"noopener\">https://pan.baidu.com/s/1CmoDA3jQgD8kkF3QfI90qg</a> 密码: fkxw<br>下载kube1.10.tar.gz并解压。</p>\n<p>master节点load镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker load -i kube-master.tar</span><br><span class=\"line\">$ docker load -i calico-3.1.3.tar</span><br><span class=\"line\">$ docker load -i coredns-1.1.3.tar</span><br></pre></td></tr></table></figure></p>\n<p>node节点load镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker load -i kube-node.tar</span><br><span class=\"line\">$ docker load -i calico-3.1.3.tar</span><br><span class=\"line\">$ docker load -i coredns-1.1.3.tar</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"初始化master节点\"><a href=\"#初始化master节点\" class=\"headerlink\" title=\"初始化master节点\"></a>初始化master节点</h2><p>–pod-network-cidr这个参数的ip地址可以根据实际情况修改，需要跟calico网络的CALICO_IPV4POOL_CIDR一致<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm init --pod-network-cidr=172.16.1.0/16 --kubernetes-version v1.10.1</span><br><span class=\"line\"></span><br><span class=\"line\">[init] Using Kubernetes version: v1.10.1</span><br><span class=\"line\">[init] Using Authorization modes: [Node RBAC]</span><br><span class=\"line\">[preflight] Running pre-flight checks.</span><br><span class=\"line\">\t[WARNING FileExisting-crictl]: crictl not found in system path</span><br><span class=\"line\">Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl</span><br><span class=\"line\">[preflight] Starting the kubelet service</span><br><span class=\"line\">[certificates] Generated ca certificate and key.</span><br><span class=\"line\">[certificates] Generated apiserver certificate and key.</span><br><span class=\"line\">[certificates] apiserver serving cert is signed for DNS names [ubuntu-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.250]</span><br><span class=\"line\">[certificates] Generated apiserver-kubelet-client certificate and key.</span><br><span class=\"line\">[certificates] Generated etcd/ca certificate and key.</span><br><span class=\"line\">[certificates] Generated etcd/server certificate and key.</span><br><span class=\"line\">[certificates] etcd/server serving cert is signed for DNS names [localhost] and IPs [127.0.0.1]</span><br><span class=\"line\">[certificates] Generated etcd/peer certificate and key.</span><br><span class=\"line\">[certificates] etcd/peer serving cert is signed for DNS names [ubuntu-master] and IPs [192.168.10.250]</span><br><span class=\"line\">[certificates] Generated etcd/healthcheck-client certificate and key.</span><br><span class=\"line\">[certificates] Generated apiserver-etcd-client certificate and key.</span><br><span class=\"line\">[certificates] Generated sa key and public key.</span><br><span class=\"line\">[certificates] Generated front-proxy-ca certificate and key.</span><br><span class=\"line\">[certificates] Generated front-proxy-client certificate and key.</span><br><span class=\"line\">[certificates] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class=\"line\">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class=\"line\">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;</span><br><span class=\"line\">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;</span><br><span class=\"line\">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;</span><br><span class=\"line\">[controlplane] Wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class=\"line\">[controlplane] Wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class=\"line\">[controlplane] Wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class=\"line\">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class=\"line\">[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;.</span><br><span class=\"line\">[init] This might take a minute or longer if the control plane images have to be pulled.</span><br><span class=\"line\">[apiclient] All control plane components are healthy after 28.003828 seconds</span><br><span class=\"line\">[uploadconfig] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class=\"line\">[markmaster] Will mark node ubuntu-master as master by adding a label and a taint</span><br><span class=\"line\">[markmaster] Master ubuntu-master tainted and labelled with key/value: node-role.kubernetes.io/master=&quot;&quot;</span><br><span class=\"line\">[bootstraptoken] Using token: rw4enn.mvk547juq7qi2b5f</span><br><span class=\"line\">[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class=\"line\">[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class=\"line\">[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class=\"line\">[bootstraptoken] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class=\"line\">[addons] Applied essential addon: kube-dns</span><br><span class=\"line\">[addons] Applied essential addon: kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">Your Kubernetes master has initialized successfully!</span><br><span class=\"line\"></span><br><span class=\"line\">To start using your cluster, you need to run the following as a regular user:</span><br><span class=\"line\"></span><br><span class=\"line\">  mkdir -p $HOME/.kube</span><br><span class=\"line\">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class=\"line\"></span><br><span class=\"line\">You should now deploy a pod network to the cluster.</span><br><span class=\"line\">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class=\"line\">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class=\"line\"></span><br><span class=\"line\">You can now join any number of machines by running the following on each node</span><br><span class=\"line\">as root:</span><br><span class=\"line\"></span><br><span class=\"line\">  kubeadm join 192.168.10.250:6443 --token rw4enn.mvk547juq7qi2b5f --discovery-token-ca-cert-hash sha256:ba260d5191213382a806a9a7d92c9e6bb09061847c7914b1ac584d0c69471579</span><br></pre></td></tr></table></figure></p>\n<p>执行如下命令来配置kubectl<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p $HOME/.kube</span><br><span class=\"line\">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure></p>\n<p>这样master的节点就配置好了，并且可以使用kubectl来进行各种操作了，根据上面的提示接着往下做，将slave节点加入到集群。</p>\n<h2 id=\"node节点加入集群\"><a href=\"#node节点加入集群\" class=\"headerlink\" title=\"node节点加入集群\"></a>node节点加入集群</h2><p>在node节点上执行kubeadm join命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm join 192.168.10.250:6443 --token rw4enn.mvk547juq7qi2b5f --discovery-token-ca-cert-hash sha256:ba260d5191213382a806a9a7d92c9e6bb09061847c7914b1ac584d0c69471579</span><br><span class=\"line\"></span><br><span class=\"line\">[preflight] Running pre-flight checks.</span><br><span class=\"line\">\t[WARNING FileExisting-crictl]: crictl not found in system path</span><br><span class=\"line\">Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl</span><br><span class=\"line\">[discovery] Trying to connect to API Server &quot;192.168.10.250:6443&quot;</span><br><span class=\"line\">[discovery] Created cluster-info discovery client, requesting info from &quot;https://192.168.10.250:6443&quot;</span><br><span class=\"line\">[discovery] Requesting info from &quot;https://192.168.10.250:6443&quot; again to validate TLS against the pinned public key</span><br><span class=\"line\">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;192.168.10.250:6443&quot;</span><br><span class=\"line\">[discovery] Successfully established connection with API Server &quot;192.168.10.250:6443&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">This node has joined the cluster:</span><br><span class=\"line\">* Certificate signing request was sent to master and a response</span><br><span class=\"line\">  was received.</span><br><span class=\"line\">* The Kubelet was informed of the new secure connection details.</span><br><span class=\"line\"></span><br><span class=\"line\">Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster.</span><br></pre></td></tr></table></figure></p>\n<p>查看节点状态<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get nodes</span><br><span class=\"line\"></span><br><span class=\"line\">NAME         STATUS    ROLES     AGE       VERSION</span><br><span class=\"line\">k8s-master   Ready     master    1h        v1.10.4</span><br><span class=\"line\">k8s-node1    Ready     &lt;none&gt;    1h        v1.10.4</span><br></pre></td></tr></table></figure></p>\n<p>在master节点查看pod状态<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pods -n kube-system -o wide</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"安装Calico网络\"><a href=\"#安装Calico网络\" class=\"headerlink\" title=\"安装Calico网络\"></a>安装Calico网络</h2><p><a href=\"https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml\" target=\"_blank\" rel=\"noopener\">calico.yaml文件</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br><span class=\"line\">262</span><br><span class=\"line\">263</span><br><span class=\"line\">264</span><br><span class=\"line\">265</span><br><span class=\"line\">266</span><br><span class=\"line\">267</span><br><span class=\"line\">268</span><br><span class=\"line\">269</span><br><span class=\"line\">270</span><br><span class=\"line\">271</span><br><span class=\"line\">272</span><br><span class=\"line\">273</span><br><span class=\"line\">274</span><br><span class=\"line\">275</span><br><span class=\"line\">276</span><br><span class=\"line\">277</span><br><span class=\"line\">278</span><br><span class=\"line\">279</span><br><span class=\"line\">280</span><br><span class=\"line\">281</span><br><span class=\"line\">282</span><br><span class=\"line\">283</span><br><span class=\"line\">284</span><br><span class=\"line\">285</span><br><span class=\"line\">286</span><br><span class=\"line\">287</span><br><span class=\"line\">288</span><br><span class=\"line\">289</span><br><span class=\"line\">290</span><br><span class=\"line\">291</span><br><span class=\"line\">292</span><br><span class=\"line\">293</span><br><span class=\"line\">294</span><br><span class=\"line\">295</span><br><span class=\"line\">296</span><br><span class=\"line\">297</span><br><span class=\"line\">298</span><br><span class=\"line\">299</span><br><span class=\"line\">300</span><br><span class=\"line\">301</span><br><span class=\"line\">302</span><br><span class=\"line\">303</span><br><span class=\"line\">304</span><br><span class=\"line\">305</span><br><span class=\"line\">306</span><br><span class=\"line\">307</span><br><span class=\"line\">308</span><br><span class=\"line\">309</span><br><span class=\"line\">310</span><br><span class=\"line\">311</span><br><span class=\"line\">312</span><br><span class=\"line\">313</span><br><span class=\"line\">314</span><br><span class=\"line\">315</span><br><span class=\"line\">316</span><br><span class=\"line\">317</span><br><span class=\"line\">318</span><br><span class=\"line\">319</span><br><span class=\"line\">320</span><br><span class=\"line\">321</span><br><span class=\"line\">322</span><br><span class=\"line\">323</span><br><span class=\"line\">324</span><br><span class=\"line\">325</span><br><span class=\"line\">326</span><br><span class=\"line\">327</span><br><span class=\"line\">328</span><br><span class=\"line\">329</span><br><span class=\"line\">330</span><br><span class=\"line\">331</span><br><span class=\"line\">332</span><br><span class=\"line\">333</span><br><span class=\"line\">334</span><br><span class=\"line\">335</span><br><span class=\"line\">336</span><br><span class=\"line\">337</span><br><span class=\"line\">338</span><br><span class=\"line\">339</span><br><span class=\"line\">340</span><br><span class=\"line\">341</span><br><span class=\"line\">342</span><br><span class=\"line\">343</span><br><span class=\"line\">344</span><br><span class=\"line\">345</span><br><span class=\"line\">346</span><br><span class=\"line\">347</span><br><span class=\"line\">348</span><br><span class=\"line\">349</span><br><span class=\"line\">350</span><br><span class=\"line\">351</span><br><span class=\"line\">352</span><br><span class=\"line\">353</span><br><span class=\"line\">354</span><br><span class=\"line\">355</span><br><span class=\"line\">356</span><br><span class=\"line\">357</span><br><span class=\"line\">358</span><br><span class=\"line\">359</span><br><span class=\"line\">360</span><br><span class=\"line\">361</span><br><span class=\"line\">362</span><br><span class=\"line\">363</span><br><span class=\"line\">364</span><br><span class=\"line\">365</span><br><span class=\"line\">366</span><br><span class=\"line\">367</span><br><span class=\"line\">368</span><br><span class=\"line\">369</span><br><span class=\"line\">370</span><br><span class=\"line\">371</span><br><span class=\"line\">372</span><br><span class=\"line\">373</span><br><span class=\"line\">374</span><br><span class=\"line\">375</span><br><span class=\"line\">376</span><br><span class=\"line\">377</span><br><span class=\"line\">378</span><br><span class=\"line\">379</span><br><span class=\"line\">380</span><br><span class=\"line\">381</span><br><span class=\"line\">382</span><br><span class=\"line\">383</span><br><span class=\"line\">384</span><br><span class=\"line\">385</span><br><span class=\"line\">386</span><br><span class=\"line\">387</span><br><span class=\"line\">388</span><br><span class=\"line\">389</span><br><span class=\"line\">390</span><br><span class=\"line\">391</span><br><span class=\"line\">392</span><br><span class=\"line\">393</span><br><span class=\"line\">394</span><br><span class=\"line\">395</span><br><span class=\"line\">396</span><br><span class=\"line\">397</span><br><span class=\"line\">398</span><br><span class=\"line\">399</span><br><span class=\"line\">400</span><br><span class=\"line\">401</span><br><span class=\"line\">402</span><br><span class=\"line\">403</span><br><span class=\"line\">404</span><br><span class=\"line\">405</span><br><span class=\"line\">406</span><br><span class=\"line\">407</span><br><span class=\"line\">408</span><br><span class=\"line\">409</span><br><span class=\"line\">410</span><br><span class=\"line\">411</span><br><span class=\"line\">412</span><br><span class=\"line\">413</span><br><span class=\"line\">414</span><br><span class=\"line\">415</span><br><span class=\"line\">416</span><br><span class=\"line\">417</span><br><span class=\"line\">418</span><br><span class=\"line\">419</span><br><span class=\"line\">420</span><br><span class=\"line\">421</span><br><span class=\"line\">422</span><br><span class=\"line\">423</span><br><span class=\"line\">424</span><br><span class=\"line\">425</span><br><span class=\"line\">426</span><br><span class=\"line\">427</span><br><span class=\"line\">428</span><br><span class=\"line\">429</span><br><span class=\"line\">430</span><br><span class=\"line\">431</span><br><span class=\"line\">432</span><br><span class=\"line\">433</span><br><span class=\"line\">434</span><br><span class=\"line\">435</span><br><span class=\"line\">436</span><br><span class=\"line\">437</span><br><span class=\"line\">438</span><br><span class=\"line\">439</span><br><span class=\"line\">440</span><br><span class=\"line\">441</span><br><span class=\"line\">442</span><br><span class=\"line\">443</span><br><span class=\"line\">444</span><br><span class=\"line\">445</span><br><span class=\"line\">446</span><br><span class=\"line\">447</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt;EOF &gt; calico.yaml</span><br><span class=\"line\"># Calico Version v3.1.3</span><br><span class=\"line\"># https://docs.projectcalico.org/v3.1/releases#v3.1.3</span><br><span class=\"line\"># This manifest includes the following component versions:</span><br><span class=\"line\">#   calico/node:v3.1.3</span><br><span class=\"line\">#   calico/cni:v3.1.3</span><br><span class=\"line\">#   calico/kube-controllers:v3.1.3</span><br><span class=\"line\"></span><br><span class=\"line\"># This ConfigMap is used to configure a self-hosted Calico installation.</span><br><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-config</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">data:</span><br><span class=\"line\">  # The location of your etcd cluster.  This uses the Service clusterIP defined below.</span><br><span class=\"line\">  etcd_endpoints: &quot;http://10.96.232.136:6666&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Configure the Calico backend to use.</span><br><span class=\"line\">  calico_backend: &quot;bird&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # The CNI network configuration to install on each node.</span><br><span class=\"line\">  cni_network_config: |-</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;name&quot;: &quot;k8s-pod-network&quot;,</span><br><span class=\"line\">      &quot;cniVersion&quot;: &quot;0.3.0&quot;,</span><br><span class=\"line\">      &quot;plugins&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;calico&quot;,</span><br><span class=\"line\">          &quot;etcd_endpoints&quot;: &quot;__ETCD_ENDPOINTS__&quot;,</span><br><span class=\"line\">          &quot;log_level&quot;: &quot;info&quot;,</span><br><span class=\"line\">          &quot;mtu&quot;: 1500,</span><br><span class=\"line\">          &quot;ipam&quot;: &#123;</span><br><span class=\"line\">              &quot;type&quot;: &quot;calico-ipam&quot;,</span><br><span class=\"line\">              &quot;subnet&quot;: &quot;usePodCidr&quot;</span><br><span class=\"line\">          &#125;,</span><br><span class=\"line\">          &quot;policy&quot;: &#123;</span><br><span class=\"line\">              &quot;type&quot;: &quot;k8s&quot;</span><br><span class=\"line\">          &#125;,</span><br><span class=\"line\">          &quot;kubernetes&quot;: &#123;</span><br><span class=\"line\">              &quot;kubeconfig&quot;: &quot;__KUBECONFIG_FILEPATH__&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;portmap&quot;,</span><br><span class=\"line\">          &quot;snat&quot;: true,</span><br><span class=\"line\">          &quot;capabilities&quot;: &#123;&quot;portMappings&quot;: true&#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\"># This manifest installs the Calico etcd on the kubeadm master.  This uses a DaemonSet</span><br><span class=\"line\"># to force it to run on the master even when the master isn&apos;t schedulable, and uses</span><br><span class=\"line\"># nodeSelector to ensure it only runs on the master.</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: DaemonSet</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-etcd</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: calico-etcd</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        k8s-app: calico-etcd</span><br><span class=\"line\">      annotations:</span><br><span class=\"line\">        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler</span><br><span class=\"line\">        # reserves resources for critical add-on pods so that they can be rescheduled after</span><br><span class=\"line\">        # a failure.  This annotation works in tandem with the toleration below.</span><br><span class=\"line\">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">        # This taint is set by all kubelets running `--cloud-provider=external`</span><br><span class=\"line\">        # so we should tolerate it to schedule the calico pods</span><br><span class=\"line\">        - key: node.cloudprovider.kubernetes.io/uninitialized</span><br><span class=\"line\">          value: &quot;true&quot;</span><br><span class=\"line\">          effect: NoSchedule</span><br><span class=\"line\">        # Allow this pod to run on the master.</span><br><span class=\"line\">        - key: node-role.kubernetes.io/master</span><br><span class=\"line\">          effect: NoSchedule</span><br><span class=\"line\">        # Allow this pod to be rescheduled while the node is in &quot;critical add-ons only&quot; mode.</span><br><span class=\"line\">        # This, along with the annotation above marks this pod as a critical add-on.</span><br><span class=\"line\">        - key: CriticalAddonsOnly</span><br><span class=\"line\">          operator: Exists</span><br><span class=\"line\">      # Only run this pod on the master.</span><br><span class=\"line\">      nodeSelector:</span><br><span class=\"line\">        node-role.kubernetes.io/master: &quot;&quot;</span><br><span class=\"line\">      hostNetwork: true</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">        - name: calico-etcd</span><br><span class=\"line\">          image: quay.io/coreos/etcd:v3.1.10</span><br><span class=\"line\">          env:</span><br><span class=\"line\">            - name: CALICO_ETCD_IP</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                fieldRef:</span><br><span class=\"line\">                  fieldPath: status.podIP</span><br><span class=\"line\">          command:</span><br><span class=\"line\">          - /usr/local/bin/etcd</span><br><span class=\"line\">          args:</span><br><span class=\"line\">          - --name=calico</span><br><span class=\"line\">          - --data-dir=/var/etcd/calico-data</span><br><span class=\"line\">          - --advertise-client-urls=http://$CALICO_ETCD_IP:6666</span><br><span class=\"line\">          - --listen-client-urls=http://0.0.0.0:6666</span><br><span class=\"line\">          - --listen-peer-urls=http://0.0.0.0:6667</span><br><span class=\"line\">          - --auto-compaction-retention=1</span><br><span class=\"line\">          volumeMounts:</span><br><span class=\"line\">            - name: var-etcd</span><br><span class=\"line\">              mountPath: /var/etcd</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">        - name: var-etcd</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /var/etcd</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\"># This manifest installs the Service which gets traffic to the Calico</span><br><span class=\"line\"># etcd.</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: calico-etcd</span><br><span class=\"line\">  name: calico-etcd</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  # Select the calico-etcd pod running on the master.</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: calico-etcd</span><br><span class=\"line\">  # This ClusterIP needs to be known in advance, since we cannot rely</span><br><span class=\"line\">  # on DNS to get access to etcd.</span><br><span class=\"line\">  clusterIP: 10.96.232.136</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">    - port: 6666</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\"># This manifest installs the calico/node container, as well</span><br><span class=\"line\"># as the Calico CNI plugins and network config on</span><br><span class=\"line\"># each master and worker node in a Kubernetes cluster.</span><br><span class=\"line\">kind: DaemonSet</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-node</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: calico-node</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      k8s-app: calico-node</span><br><span class=\"line\">  updateStrategy:</span><br><span class=\"line\">    type: RollingUpdate</span><br><span class=\"line\">    rollingUpdate:</span><br><span class=\"line\">      maxUnavailable: 1</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        k8s-app: calico-node</span><br><span class=\"line\">      annotations:</span><br><span class=\"line\">        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler</span><br><span class=\"line\">        # reserves resources for critical add-on pods so that they can be rescheduled after</span><br><span class=\"line\">        # a failure.  This annotation works in tandem with the toleration below.</span><br><span class=\"line\">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      hostNetwork: true</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">        # Make sure calico/node gets scheduled on all nodes.</span><br><span class=\"line\">        - effect: NoSchedule</span><br><span class=\"line\">          operator: Exists</span><br><span class=\"line\">        # Mark the pod as a critical add-on for rescheduling.</span><br><span class=\"line\">        - key: CriticalAddonsOnly</span><br><span class=\"line\">          operator: Exists</span><br><span class=\"line\">        - effect: NoExecute</span><br><span class=\"line\">          operator: Exists</span><br><span class=\"line\">      serviceAccountName: calico-cni-plugin</span><br><span class=\"line\">      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &quot;force</span><br><span class=\"line\">      # deletion&quot;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.</span><br><span class=\"line\">      terminationGracePeriodSeconds: 0</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">        # Runs calico/node container on each Kubernetes node.  This</span><br><span class=\"line\">        # container programs network policy and routes on each</span><br><span class=\"line\">        # host.</span><br><span class=\"line\">        - name: calico-node</span><br><span class=\"line\">          image: quay.io/calico/node:v3.1.3</span><br><span class=\"line\">          env:</span><br><span class=\"line\">            # The location of the Calico etcd cluster.</span><br><span class=\"line\">            - name: ETCD_ENDPOINTS</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                configMapKeyRef:</span><br><span class=\"line\">                  name: calico-config</span><br><span class=\"line\">                  key: etcd_endpoints</span><br><span class=\"line\">            # Enable BGP.  Disable to enforce policy only.</span><br><span class=\"line\">            - name: CALICO_NETWORKING_BACKEND</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                configMapKeyRef:</span><br><span class=\"line\">                  name: calico-config</span><br><span class=\"line\">                  key: calico_backend</span><br><span class=\"line\">            # Cluster type to identify the deployment type</span><br><span class=\"line\">            - name: CLUSTER_TYPE</span><br><span class=\"line\">              value: &quot;kubeadm,bgp&quot;</span><br><span class=\"line\">            # Disable file logging so `kubectl logs` works.</span><br><span class=\"line\">            - name: CALICO_DISABLE_FILE_LOGGING</span><br><span class=\"line\">              value: &quot;true&quot;</span><br><span class=\"line\">            # Set noderef for node controller.</span><br><span class=\"line\">            - name: CALICO_K8S_NODE_REF</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                fieldRef:</span><br><span class=\"line\">                  fieldPath: spec.nodeName</span><br><span class=\"line\">            # Set Felix endpoint to host default action to ACCEPT.</span><br><span class=\"line\">            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION</span><br><span class=\"line\">              value: &quot;ACCEPT&quot;</span><br><span class=\"line\">            # The default IPv4 pool to create on startup if none exists. Pod IPs will be</span><br><span class=\"line\">            # chosen from this range. Changing this value after installation will have</span><br><span class=\"line\">            # no effect. This should fall within `--cluster-cidr`.</span><br><span class=\"line\">            - name: CALICO_IPV4POOL_CIDR</span><br><span class=\"line\">              value: &quot;172.16.1.0/16&quot;</span><br><span class=\"line\">            - name: CALICO_IPV4POOL_IPIP</span><br><span class=\"line\">              value: &quot;Always&quot;</span><br><span class=\"line\">            # Disable IPv6 on Kubernetes.</span><br><span class=\"line\">            - name: FELIX_IPV6SUPPORT</span><br><span class=\"line\">              value: &quot;false&quot;</span><br><span class=\"line\">            # Set MTU for tunnel device used if ipip is enabled</span><br><span class=\"line\">            - name: FELIX_IPINIPMTU</span><br><span class=\"line\">              value: &quot;1440&quot;</span><br><span class=\"line\">            # Set Felix logging to &quot;info&quot;</span><br><span class=\"line\">            - name: FELIX_LOGSEVERITYSCREEN</span><br><span class=\"line\">              value: &quot;info&quot;</span><br><span class=\"line\">            # Auto-detect the BGP IP address.</span><br><span class=\"line\">            - name: IP</span><br><span class=\"line\">              value: &quot;autodetect&quot;</span><br><span class=\"line\">            - name: FELIX_HEALTHENABLED</span><br><span class=\"line\">              value: &quot;true&quot;</span><br><span class=\"line\">          securityContext:</span><br><span class=\"line\">            privileged: true</span><br><span class=\"line\">          resources:</span><br><span class=\"line\">            requests:</span><br><span class=\"line\">              cpu: 250m</span><br><span class=\"line\">          livenessProbe:</span><br><span class=\"line\">            httpGet:</span><br><span class=\"line\">              path: /liveness</span><br><span class=\"line\">              port: 9099</span><br><span class=\"line\">            periodSeconds: 10</span><br><span class=\"line\">            initialDelaySeconds: 10</span><br><span class=\"line\">            failureThreshold: 6</span><br><span class=\"line\">          readinessProbe:</span><br><span class=\"line\">            httpGet:</span><br><span class=\"line\">              path: /readiness</span><br><span class=\"line\">              port: 9099</span><br><span class=\"line\">            periodSeconds: 10</span><br><span class=\"line\">          volumeMounts:</span><br><span class=\"line\">            - mountPath: /lib/modules</span><br><span class=\"line\">              name: lib-modules</span><br><span class=\"line\">              readOnly: true</span><br><span class=\"line\">            - mountPath: /var/run/calico</span><br><span class=\"line\">              name: var-run-calico</span><br><span class=\"line\">              readOnly: false</span><br><span class=\"line\">            - mountPath: /var/lib/calico</span><br><span class=\"line\">              name: var-lib-calico</span><br><span class=\"line\">              readOnly: false</span><br><span class=\"line\">        # This container installs the Calico CNI binaries</span><br><span class=\"line\">        # and CNI network config file on each node.</span><br><span class=\"line\">        - name: install-cni</span><br><span class=\"line\">          image: quay.io/calico/cni:v3.1.3</span><br><span class=\"line\">          command: [&quot;/install-cni.sh&quot;]</span><br><span class=\"line\">          env:</span><br><span class=\"line\">            # Name of the CNI config file to create.</span><br><span class=\"line\">            - name: CNI_CONF_NAME</span><br><span class=\"line\">              value: &quot;10-calico.conflist&quot;</span><br><span class=\"line\">            # The location of the Calico etcd cluster.</span><br><span class=\"line\">            - name: ETCD_ENDPOINTS</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                configMapKeyRef:</span><br><span class=\"line\">                  name: calico-config</span><br><span class=\"line\">                  key: etcd_endpoints</span><br><span class=\"line\">            # The CNI network config to install on each node.</span><br><span class=\"line\">            - name: CNI_NETWORK_CONFIG</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                configMapKeyRef:</span><br><span class=\"line\">                  name: calico-config</span><br><span class=\"line\">                  key: cni_network_config</span><br><span class=\"line\">          volumeMounts:</span><br><span class=\"line\">            - mountPath: /host/opt/cni/bin</span><br><span class=\"line\">              name: cni-bin-dir</span><br><span class=\"line\">            - mountPath: /host/etc/cni/net.d</span><br><span class=\"line\">              name: cni-net-dir</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">        # Used by calico/node.</span><br><span class=\"line\">        - name: lib-modules</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /lib/modules</span><br><span class=\"line\">        - name: var-run-calico</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /var/run/calico</span><br><span class=\"line\">        - name: var-lib-calico</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /var/lib/calico</span><br><span class=\"line\">        # Used to install CNI.</span><br><span class=\"line\">        - name: cni-bin-dir</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /opt/cni/bin</span><br><span class=\"line\">        - name: cni-net-dir</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /etc/cni/net.d</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\"># This manifest deploys the Calico Kubernetes controllers.</span><br><span class=\"line\"># See https://github.com/projectcalico/kube-controllers</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: calico-kube-controllers</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  # The controllers can only have a single active instance.</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  strategy:</span><br><span class=\"line\">    type: Recreate</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      name: calico-kube-controllers</span><br><span class=\"line\">      namespace: kube-system</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        k8s-app: calico-kube-controllers</span><br><span class=\"line\">      annotations:</span><br><span class=\"line\">        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler</span><br><span class=\"line\">        # reserves resources for critical add-on pods so that they can be rescheduled after</span><br><span class=\"line\">        # a failure.  This annotation works in tandem with the toleration below.</span><br><span class=\"line\">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      # The controllers must run in the host network namespace so that</span><br><span class=\"line\">      # it isn&apos;t governed by policy that would prevent it from working.</span><br><span class=\"line\">      hostNetwork: true</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">        # Allow this pod to be rescheduled while the node is in &quot;critical add-ons only&quot; mode.</span><br><span class=\"line\">        # This, along with the annotation above marks this pod as a critical add-on.</span><br><span class=\"line\">        - key: CriticalAddonsOnly</span><br><span class=\"line\">          operator: Exists</span><br><span class=\"line\">        - key: node-role.kubernetes.io/master</span><br><span class=\"line\">          effect: NoSchedule</span><br><span class=\"line\">      serviceAccountName: calico-kube-controllers</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">        - name: calico-kube-controllers</span><br><span class=\"line\">          image: quay.io/calico/kube-controllers:v3.1.3</span><br><span class=\"line\">          env:</span><br><span class=\"line\">            # The location of the Calico etcd cluster.</span><br><span class=\"line\">            - name: ETCD_ENDPOINTS</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                configMapKeyRef:</span><br><span class=\"line\">                  name: calico-config</span><br><span class=\"line\">                  key: etcd_endpoints</span><br><span class=\"line\">            # Choose which controllers to run.</span><br><span class=\"line\">            - name: ENABLED_CONTROLLERS</span><br><span class=\"line\">              value: policy,profile,workloadendpoint,node</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-cni-plugin</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: calico-cni-plugin</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: calico-cni-plugin</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">kind: ClusterRole</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-cni-plugin</span><br><span class=\"line\">rules:</span><br><span class=\"line\">  - apiGroups: [&quot;&quot;]</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">      - pods</span><br><span class=\"line\">      - nodes</span><br><span class=\"line\">    verbs:</span><br><span class=\"line\">      - get</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-cni-plugin</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">kind: ClusterRole</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">rules:</span><br><span class=\"line\">  - apiGroups:</span><br><span class=\"line\">    - &quot;&quot;</span><br><span class=\"line\">    - extensions</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">      - pods</span><br><span class=\"line\">      - namespaces</span><br><span class=\"line\">      - networkpolicies</span><br><span class=\"line\">      - nodes</span><br><span class=\"line\">    verbs:</span><br><span class=\"line\">      - watch</span><br><span class=\"line\">      - list</span><br><span class=\"line\">  - apiGroups:</span><br><span class=\"line\">    - networking.k8s.io</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">      - networkpolicies</span><br><span class=\"line\">    verbs:</span><br><span class=\"line\">      - watch</span><br><span class=\"line\">      - list</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>部署calico<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"安装CoreDNS\"><a href=\"#安装CoreDNS\" class=\"headerlink\" title=\"安装CoreDNS\"></a>安装CoreDNS</h2><p>在/etc/kubernetes/manifests/下创建coredns目录,<br>需要把官方提供的两个文件deploy.sh和coredns.yaml.sed放到conredns目录</p>\n<h3 id=\"准备配置文件\"><a href=\"#准备配置文件\" class=\"headerlink\" title=\"准备配置文件\"></a>准备配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -p /etc/kubernetes/manifests/coredns</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh\" target=\"_blank\" rel=\"noopener\">deploy.sh文件</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/coredns/deploy.sh</span><br><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\"># Deploys CoreDNS to a cluster currently running Kube-DNS.</span><br><span class=\"line\"></span><br><span class=\"line\">show_help () &#123;</span><br><span class=\"line\">cat &lt;&lt; USAGE</span><br><span class=\"line\">usage: $0 [ -r REVERSE-CIDR ] [ -i DNS-IP ] [ -d CLUSTER-DOMAIN ] [ -t YAML-TEMPLATE ]</span><br><span class=\"line\">    -r : Define a reverse zone for the given CIDR. You may specifcy this option more</span><br><span class=\"line\">         than once to add multiple reverse zones. If no reverse CIDRs are defined,</span><br><span class=\"line\">         then the default is to handle all reverse zones (i.e. in-addr.arpa and ip6.arpa)</span><br><span class=\"line\">    -i : Specify the cluster DNS IP address. If not specificed, the IP address of</span><br><span class=\"line\">         the existing &quot;kube-dns&quot; service is used, if present.</span><br><span class=\"line\">USAGE</span><br><span class=\"line\">exit 0</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"># Simple Defaults</span><br><span class=\"line\">CLUSTER_DOMAIN=cluster.local</span><br><span class=\"line\">YAML_TEMPLATE=`pwd`/coredns.yaml.sed</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># Get Opts</span><br><span class=\"line\">while getopts &quot;hr:i:d:t:&quot; opt; do</span><br><span class=\"line\">    case &quot;$opt&quot; in</span><br><span class=\"line\">    h)  show_help</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    r)  REVERSE_CIDRS=&quot;$REVERSE_CIDRS $OPTARG&quot;</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    i)  CLUSTER_DNS_IP=$OPTARG</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    d)  CLUSTER_DOMAIN=$OPTARG</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    t)  YAML_TEMPLATE=$OPTARG</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    esac</span><br><span class=\"line\">done</span><br><span class=\"line\"></span><br><span class=\"line\"># Conditional Defaults</span><br><span class=\"line\">if [[ -z $REVERSE_CIDRS ]]; then</span><br><span class=\"line\">  REVERSE_CIDRS=&quot;in-addr.arpa ip6.arpa&quot;</span><br><span class=\"line\">fi</span><br><span class=\"line\">if [[ -z $CLUSTER_DNS_IP ]]; then</span><br><span class=\"line\">  # Default IP to kube-dns IP</span><br><span class=\"line\">  CLUSTER_DNS_IP=$(kubectl get service --namespace kube-system kube-dns -o jsonpath=&quot;&#123;.spec.clusterIP&#125;&quot;)</span><br><span class=\"line\">  if [ $? -ne 0 ]; then</span><br><span class=\"line\">      &gt;&amp;2 echo &quot;Error! The IP address for DNS service couldn&apos;t be determined automatically. Please specify the DNS-IP with the &apos;-i&apos; option.&quot;</span><br><span class=\"line\">      exit 2</span><br><span class=\"line\">  fi</span><br><span class=\"line\">fi</span><br><span class=\"line\"></span><br><span class=\"line\">sed -e s/CLUSTER_DNS_IP/$CLUSTER_DNS_IP/g -e s/CLUSTER_DOMAIN/$CLUSTER_DOMAIN/g -e &quot;s?REVERSE_CIDRS?$REVERSE_CIDRS?g&quot; $YAML_TEMPLATE</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed\" target=\"_blank\" rel=\"noopener\">coredns.yaml.sed文件</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/coredns/coredns.yaml.sed</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: coredns</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRole</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class=\"line\">  name: system:coredns</span><br><span class=\"line\">rules:</span><br><span class=\"line\">- apiGroups:</span><br><span class=\"line\">  - &quot;&quot;</span><br><span class=\"line\">  resources:</span><br><span class=\"line\">  - endpoints</span><br><span class=\"line\">  - services</span><br><span class=\"line\">  - pods</span><br><span class=\"line\">  - namespaces</span><br><span class=\"line\">  verbs:</span><br><span class=\"line\">  - list</span><br><span class=\"line\">  - watch</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  annotations:</span><br><span class=\"line\">    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class=\"line\">  name: system:coredns</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: system:coredns</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: coredns</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: coredns</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">data:</span><br><span class=\"line\">  Corefile: |</span><br><span class=\"line\">    .:53 &#123;</span><br><span class=\"line\">        errors</span><br><span class=\"line\">        health</span><br><span class=\"line\">        kubernetes CLUSTER_DOMAIN REVERSE_CIDRS &#123;</span><br><span class=\"line\">          pods insecure</span><br><span class=\"line\">          upstream</span><br><span class=\"line\">          fallthrough in-addr.arpa ip6.arpa</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        prometheus :9153</span><br><span class=\"line\">        proxy . /etc/resolv.conf</span><br><span class=\"line\">        cache 30</span><br><span class=\"line\">        reload</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: coredns</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kube-dns</span><br><span class=\"line\">    kubernetes.io/name: &quot;CoreDNS&quot;</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 2</span><br><span class=\"line\">  strategy:</span><br><span class=\"line\">    type: RollingUpdate</span><br><span class=\"line\">    rollingUpdate:</span><br><span class=\"line\">      maxUnavailable: 1</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      k8s-app: kube-dns</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        k8s-app: kube-dns</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      serviceAccountName: coredns</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">        - key: &quot;CriticalAddonsOnly&quot;</span><br><span class=\"line\">          operator: &quot;Exists&quot;</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: coredns</span><br><span class=\"line\">        image: coredns/coredns:1.1.3</span><br><span class=\"line\">        imagePullPolicy: IfNotPresent</span><br><span class=\"line\">        args: [ &quot;-conf&quot;, &quot;/etc/coredns/Corefile&quot; ]</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - name: config-volume</span><br><span class=\"line\">          mountPath: /etc/coredns</span><br><span class=\"line\">          readOnly: true</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">        - containerPort: 53</span><br><span class=\"line\">          name: dns</span><br><span class=\"line\">          protocol: UDP</span><br><span class=\"line\">        - containerPort: 53</span><br><span class=\"line\">          name: dns-tcp</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        - containerPort: 9153</span><br><span class=\"line\">          name: metrics</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        securityContext:</span><br><span class=\"line\">          allowPrivilegeEscalation: false</span><br><span class=\"line\">          capabilities:</span><br><span class=\"line\">            add:</span><br><span class=\"line\">            - NET_BIND_SERVICE</span><br><span class=\"line\">            drop:</span><br><span class=\"line\">            - all</span><br><span class=\"line\">          readOnlyRootFilesystem: true</span><br><span class=\"line\">        livenessProbe:</span><br><span class=\"line\">          httpGet:</span><br><span class=\"line\">            path: /health</span><br><span class=\"line\">            port: 8080</span><br><span class=\"line\">            scheme: HTTP</span><br><span class=\"line\">          initialDelaySeconds: 60</span><br><span class=\"line\">          timeoutSeconds: 5</span><br><span class=\"line\">          successThreshold: 1</span><br><span class=\"line\">          failureThreshold: 5</span><br><span class=\"line\">      dnsPolicy: Default</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">        - name: config-volume</span><br><span class=\"line\">          configMap:</span><br><span class=\"line\">            name: coredns</span><br><span class=\"line\">            items:</span><br><span class=\"line\">            - key: Corefile</span><br><span class=\"line\">              path: Corefile</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kube-dns</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  annotations:</span><br><span class=\"line\">    prometheus.io/scrape: &quot;true&quot;</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kube-dns</span><br><span class=\"line\">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class=\"line\">    kubernetes.io/name: &quot;CoreDNS&quot;</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: kube-dns</span><br><span class=\"line\">  clusterIP: CLUSTER_DNS_IP</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">  - name: dns</span><br><span class=\"line\">    port: 53</span><br><span class=\"line\">    protocol: UDP</span><br><span class=\"line\">  - name: dns-tcp</span><br><span class=\"line\">    port: 53</span><br><span class=\"line\">    protocol: TCP</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用CoreDNS替换Kube-DNS\"><a href=\"#使用CoreDNS替换Kube-DNS\" class=\"headerlink\" title=\"使用CoreDNS替换Kube-DNS\"></a>使用CoreDNS替换Kube-DNS</h3><p>最佳的案例场景中，使用CoreDNS替换Kube-DNS只需要使用下面的两个命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ./deploy.sh | kubectl apply -f -</span><br><span class=\"line\">$ kubectl delete --namespace=kube-system deployment kube-dns</span><br></pre></td></tr></table></figure></p>\n<p>查看pod情况<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pods -n kube-system -o wide</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class=\"line\">calico-etcd-2fn6s                          1/1       Running   0          7m        192.168.10.250   k8s-master</span><br><span class=\"line\">calico-kube-controllers-679568f47c-k7q4w   1/1       Running   0          7m        192.168.10.250   k8s-master</span><br><span class=\"line\">calico-node-8l2mf                          2/2       Running   0          7m        192.168.10.250   k8s-master</span><br><span class=\"line\">calico-node-gx4x8                          2/2       Running   0          5m        192.168.10.57    k8s-node1</span><br><span class=\"line\">coredns-6cc44759b8-8p8cb                   1/1       Running   1          6h        172.16.1.193     k8s-master</span><br><span class=\"line\">coredns-6cc44759b8-xzmfr                   1/1       Running   1          6h        172.16.1.194     k8s-master</span><br><span class=\"line\">etcd-k8s-master                            1/1       Running   3          7h        192.168.10.250   k8s-master</span><br><span class=\"line\">heapster-77b9c5bd7b-cdr7d                  1/1       Running   0          4m        172.16.1.68      k8s-node1</span><br><span class=\"line\">kube-apiserver-k8s-master                  1/1       Running   3          7h        192.168.10.250   k8s-master</span><br><span class=\"line\">kube-controller-manager-k8s-master         1/1       Running   3          7h        192.168.10.250   k8s-master</span><br><span class=\"line\">kube-proxy-7nvtx                           1/1       Running   3          7h        192.168.10.250   k8s-master</span><br><span class=\"line\">kube-proxy-nh6jf                           1/1       Running   0          5m        192.168.10.57    k8s-node1</span><br><span class=\"line\">kube-scheduler-k8s-master                  1/1       Running   3          7h        192.168.10.250   k8s-master</span><br></pre></td></tr></table></figure></p>\n<p>至此，node集群已经部署好了。</p>\n<h2 id=\"部署Dashboard\"><a href=\"#部署Dashboard\" class=\"headerlink\" title=\"部署Dashboard\"></a>部署Dashboard</h2><p>node节点load dashboard镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker load -i kube-dashboard-1.8.3.tar</span><br></pre></td></tr></table></figure></p>\n<p>dashboard.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; dashboard.yaml</span><br><span class=\"line\"># Copyright 2017 The Kubernetes Authors.</span><br><span class=\"line\">#</span><br><span class=\"line\"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class=\"line\"># you may not use this file except in compliance with the License.</span><br><span class=\"line\"># You may obtain a copy of the License at</span><br><span class=\"line\">#</span><br><span class=\"line\">#     http://www.apache.org/licenses/LICENSE-2.0</span><br><span class=\"line\">#</span><br><span class=\"line\"># Unless required by applicable law or agreed to in writing, software</span><br><span class=\"line\"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class=\"line\"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class=\"line\"># See the License for the specific language governing permissions and</span><br><span class=\"line\"># limitations under the License.</span><br><span class=\"line\"></span><br><span class=\"line\"># Configuration to deploy release version of the Dashboard UI compatible with</span><br><span class=\"line\"># Kubernetes 1.8.</span><br><span class=\"line\">#</span><br><span class=\"line\"># Example usage: kubectl create -f &lt;this_file&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"># ------------------- Dashboard Secret ------------------- #</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Secret</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\">  name: kubernetes-dashboard-certs</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">type: Opaque</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"># ------------------- Dashboard Service Account ------------------- #</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\">  name: kubernetes-dashboard</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"># ------------------- Dashboard Role &amp; Role Binding ------------------- #</span><br><span class=\"line\"></span><br><span class=\"line\">kind: Role</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kubernetes-dashboard-minimal</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">rules:</span><br><span class=\"line\">  # Allow Dashboard to create &apos;kubernetes-dashboard-key-holder&apos; secret.</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;secrets&quot;]</span><br><span class=\"line\">  verbs: [&quot;create&quot;]</span><br><span class=\"line\">  # Allow Dashboard to create &apos;kubernetes-dashboard-settings&apos; config map.</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;configmaps&quot;]</span><br><span class=\"line\">  verbs: [&quot;create&quot;]</span><br><span class=\"line\">  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;secrets&quot;]</span><br><span class=\"line\">  resourceNames: [&quot;kubernetes-dashboard-key-holder&quot;, &quot;kubernetes-dashboard-certs&quot;]</span><br><span class=\"line\">  verbs: [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;]</span><br><span class=\"line\">  # Allow Dashboard to get and update &apos;kubernetes-dashboard-settings&apos; config map.</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;configmaps&quot;]</span><br><span class=\"line\">  resourceNames: [&quot;kubernetes-dashboard-settings&quot;]</span><br><span class=\"line\">  verbs: [&quot;get&quot;, &quot;update&quot;]</span><br><span class=\"line\">  # Allow Dashboard to get metrics from heapster.</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;services&quot;]</span><br><span class=\"line\">  resourceNames: [&quot;heapster&quot;]</span><br><span class=\"line\">  verbs: [&quot;proxy&quot;]</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;services/proxy&quot;]</span><br><span class=\"line\">  resourceNames: [&quot;heapster&quot;, &quot;http:heapster:&quot;, &quot;https:heapster:&quot;]</span><br><span class=\"line\">  verbs: [&quot;get&quot;]</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class=\"line\">kind: RoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kubernetes-dashboard-minimal</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: Role</span><br><span class=\"line\">  name: kubernetes-dashboard-minimal</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: kubernetes-dashboard</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"># ------------------- Dashboard Deployment ------------------- #</span><br><span class=\"line\"></span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">apiVersion: apps/v1beta2</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\">  name: kubernetes-dashboard</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  revisionHistoryLimit: 10</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      k8s-app: kubernetes-dashboard</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        k8s-app: kubernetes-dashboard</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: kubernetes-dashboard</span><br><span class=\"line\">        image: reg.qiniu.com/k8s/kubernetes-dashboard-amd64:v1.8.3</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">        - containerPort: 8443</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        args:</span><br><span class=\"line\">          - --auto-generate-certificates</span><br><span class=\"line\">          # Uncomment the following line to manually specify Kubernetes API server Host</span><br><span class=\"line\">          # If not specified, Dashboard will attempt to auto discover the API server and connect</span><br><span class=\"line\">          # to it. Uncomment only if the default does not work.</span><br><span class=\"line\">          # - --apiserver-host=http://my-address:port</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - name: kubernetes-dashboard-certs</span><br><span class=\"line\">          mountPath: /certs</span><br><span class=\"line\">          # Create on-disk volume to store exec logs</span><br><span class=\"line\">        - mountPath: /tmp</span><br><span class=\"line\">          name: tmp-volume</span><br><span class=\"line\">        livenessProbe:</span><br><span class=\"line\">          httpGet:</span><br><span class=\"line\">            scheme: HTTPS</span><br><span class=\"line\">            path: /</span><br><span class=\"line\">            port: 8443</span><br><span class=\"line\">          initialDelaySeconds: 30</span><br><span class=\"line\">          timeoutSeconds: 30</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">      - name: kubernetes-dashboard-certs</span><br><span class=\"line\">        secret:</span><br><span class=\"line\">          secretName: kubernetes-dashboard-certs</span><br><span class=\"line\">      - name: tmp-volume</span><br><span class=\"line\">        emptyDir: &#123;&#125;</span><br><span class=\"line\">      serviceAccountName: kubernetes-dashboard</span><br><span class=\"line\">      # Comment the following tolerations if Dashboard must not be deployed on master</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">      - key: node-role.kubernetes.io/master</span><br><span class=\"line\">        effect: NoSchedule</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"># ------------------- Dashboard Service ------------------- #</span><br><span class=\"line\"></span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\">  name: kubernetes-dashboard</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">    - port: 443</span><br><span class=\"line\">      targetPort: 8443</span><br><span class=\"line\">      #以https://ip:nodeport访问dashboard</span><br><span class=\"line\">      nodePort: 32500</span><br><span class=\"line\">  type: NodePort</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\"></span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>部署dashboard<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f dashboard.yaml</span><br></pre></td></tr></table></figure></p>\n<p>部署dashboard后，为了安全默认是不能匿名访问dashboard的，所以要创建一个cluster-admin角色的用户，使用token登录dashboard</p>\n<p>admin-user.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; admin-user.yaml</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: admin-user</span><br><span class=\"line\">  annotations:</span><br><span class=\"line\">    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: cluster-admin</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: admin-user</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: admin-user</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class=\"line\">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class=\"line\"></span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>创建admin-user用户<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f admin-user.yaml</span><br></pre></td></tr></table></figure></p>\n<p>查看admin-user用户token， 使用token去登录dashboard<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &apos;&#123;print $1&#125;&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">Name:         admin-user-token-nfsg4</span><br><span class=\"line\">Namespace:    kube-system</span><br><span class=\"line\">Labels:       &lt;none&gt;</span><br><span class=\"line\">Annotations:  kubernetes.io/service-account.name=admin-user</span><br><span class=\"line\">              kubernetes.io/service-account.uid=6a91db25-704e-11e8-8055-080027c6c772</span><br><span class=\"line\"></span><br><span class=\"line\">Type:  kubernetes.io/service-account-token</span><br><span class=\"line\"></span><br><span class=\"line\">Data</span><br><span class=\"line\">====</span><br><span class=\"line\">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLW5mc2c0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2YTkxZGIyNS03MDRlLTExZTgtODA1NS0wODAwMjdjNmM3NzIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.Ue49ptrBZItvG7f8gavEQSoxtSgbz-bh3jZU9RbjLJh2nkmueTVh5WViauUd5TxsnmefB3DDLgTp_CfwWrV0Uyf7JT6GEDMvjDVhsyHa2qyjLwRfvf1vmnx2P7975yRQniq_a3fgOhdNn0zCL2Er9bDUNabCcR0ubNW6I63kp3-UYGIj9OfAkfqNCTFtjZgvcGvnvkhj2pm_peTJ6H3qmxhcb9WM90Nh77p5qdI8gjk2EdKPAhsOmOxBWSSnHqHPr0gVStsewQEHo0CQOr4MxE3NIg_gwjTbn0KD9vvJeECPAX_zsq7ejD0POoWvSXz8khwvv8yLgJQA-ZkUXJKk7w</span><br><span class=\"line\">ca.crt:     1025 bytes</span><br><span class=\"line\">namespace:  11 bytes</span><br></pre></td></tr></table></figure></p>\n<p>访问<a href=\"https://ip:32500即可打开dashboard页面\" target=\"_blank\" rel=\"noopener\">https://ip:32500即可打开dashboard页面</a></p>\n<h2 id=\"部署heapster、grafana、influxdb\"><a href=\"#部署heapster、grafana、influxdb\" class=\"headerlink\" title=\"部署heapster、grafana、influxdb\"></a>部署heapster、grafana、influxdb</h2><h3 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h3><p>heapster.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; heapster.yaml</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: heapster</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: system:heapster</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: heapster</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---  </span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: heapster</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  </span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: heapster</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        task: monitoring</span><br><span class=\"line\">        k8s-app: heapster</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      serviceAccountName: heapster</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: heapster</span><br><span class=\"line\">        image: mirrorgooglecontainers/heapster-amd64:v1.5.3</span><br><span class=\"line\">        imagePullPolicy: IfNotPresent</span><br><span class=\"line\">        #volumeMounts: </span><br><span class=\"line\">        #- mountPath: /root/.kube</span><br><span class=\"line\">        #  name: config</span><br><span class=\"line\">        command:</span><br><span class=\"line\">        - /heapster</span><br><span class=\"line\">        - --source=kubernetes:https://kubernetes.default</span><br><span class=\"line\">        - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    task: monitoring</span><br><span class=\"line\">    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)</span><br><span class=\"line\">    # If you are NOT using this as an addon, you should comment out this line.</span><br><span class=\"line\">    kubernetes.io/cluster-service: &apos;true&apos;</span><br><span class=\"line\">    kubernetes.io/name: Heapster</span><br><span class=\"line\">  name: heapster</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">  - port: 80</span><br><span class=\"line\">    targetPort: 8082</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: heapster</span><br><span class=\"line\">  </span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>grafana.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; grafana.yaml</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: monitoring-grafana</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        task: monitoring</span><br><span class=\"line\">        k8s-app: grafana</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: grafana</span><br><span class=\"line\">        image: mirrorgooglecontainers/heapster-grafana-amd64:v4.4.3</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">        - containerPort: 3000</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - mountPath: /etc/ssl/certs</span><br><span class=\"line\">          name: ca-certificates</span><br><span class=\"line\">          readOnly: true</span><br><span class=\"line\">        - mountPath: /var</span><br><span class=\"line\">          name: grafana-storage</span><br><span class=\"line\">        env:</span><br><span class=\"line\">        - name: INFLUXDB_HOST</span><br><span class=\"line\">          value: monitoring-influxdb</span><br><span class=\"line\">        - name: GF_SERVER_HTTP_PORT</span><br><span class=\"line\">          value: &quot;3000&quot;</span><br><span class=\"line\">          # The following env variables are required to make Grafana accessible via</span><br><span class=\"line\">          # the kubernetes api-server proxy. On production clusters, we recommend</span><br><span class=\"line\">          # removing these env variables, setup auth for grafana, and expose the grafana</span><br><span class=\"line\">          # service using a LoadBalancer or a public IP.</span><br><span class=\"line\">        - name: GF_AUTH_BASIC_ENABLED</span><br><span class=\"line\">          value: &quot;false&quot;</span><br><span class=\"line\">        - name: GF_AUTH_ANONYMOUS_ENABLED</span><br><span class=\"line\">          value: &quot;true&quot;</span><br><span class=\"line\">        - name: GF_AUTH_ANONYMOUS_ORG_ROLE</span><br><span class=\"line\">          value: Admin</span><br><span class=\"line\">        - name: GF_SERVER_ROOT_URL</span><br><span class=\"line\">          # If you&apos;re only using the API Server proxy, set this value instead:</span><br><span class=\"line\">          # value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</span><br><span class=\"line\">          value: /</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">      - name: ca-certificates</span><br><span class=\"line\">        hostPath:</span><br><span class=\"line\">          path: /etc/ssl/certs</span><br><span class=\"line\">      - name: grafana-storage</span><br><span class=\"line\">        emptyDir: &#123;&#125;</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)</span><br><span class=\"line\">    # If you are NOT using this as an addon, you should comment out this line.</span><br><span class=\"line\">    kubernetes.io/cluster-service: &apos;true&apos;</span><br><span class=\"line\">    kubernetes.io/name: monitoring-grafana</span><br><span class=\"line\">  name: monitoring-grafana</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  # In a production setup, we recommend accessing Grafana through an external Loadbalancer</span><br><span class=\"line\">  # or through a public IP.</span><br><span class=\"line\">  # type: LoadBalancer</span><br><span class=\"line\">  # You could also use NodePort to expose the service at a randomly-generated port</span><br><span class=\"line\">  # type: NodePort</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">  - port: 80</span><br><span class=\"line\">    targetPort: 3000</span><br><span class=\"line\">    nodePort: 32600</span><br><span class=\"line\">  type: NodePort</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: grafana</span><br><span class=\"line\"></span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>influxdb.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; influxdb.yaml</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: monitoring-influxdb</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        task: monitoring</span><br><span class=\"line\">        k8s-app: influxdb</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: influxdb</span><br><span class=\"line\">        image: mirrorgooglecontainers/heapster-influxdb-amd64:v1.3.3</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - mountPath: /data</span><br><span class=\"line\">          name: influxdb-storage</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">      - name: influxdb-storage</span><br><span class=\"line\">        emptyDir: &#123;&#125;</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    task: monitoring</span><br><span class=\"line\">    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)</span><br><span class=\"line\">    # If you are NOT using this as an addon, you should comment out this line.</span><br><span class=\"line\">    kubernetes.io/cluster-service: &apos;true&apos;</span><br><span class=\"line\">    kubernetes.io/name: monitoring-influxdb</span><br><span class=\"line\">  name: monitoring-influxdb</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">  - port: 8086</span><br><span class=\"line\">    targetPort: 8086</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: influxdb</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl create -f grafana.yaml</span><br><span class=\"line\">$ kubectl create -f heapster.yaml</span><br><span class=\"line\">$ kubectl create -f influxdb.yaml</span><br></pre></td></tr></table></figure>\n<p>访问<a href=\"http://ip:32600即可访问到grafana\" target=\"_blank\" rel=\"noopener\">http://ip:32600即可访问到grafana</a> ui了</p>\n<p>参考资料:<br><a href=\"https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/calico\" target=\"_blank\" rel=\"noopener\">https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/calico</a><br><a href=\"https://jimmysong.io/kubernetes-handbook/practice/coredns.html\" target=\"_blank\" rel=\"noopener\">https://jimmysong.io/kubernetes-handbook/practice/coredns.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>本文将介绍如何在Ubuntu server 16.03版本上安装kubeadm，构建一个kubernetes的基础的测试集群，用来做学习和测试用途，当前最新的版本是1.10.4<br>使用CoreDNS替换kubeDNS，使用calico网络，部署dashboard，部署heapster+grafana+influxdb监控</p>\n<p>本次安装2台集群，1个master，1个node，可以根据自己需求增加node机器，每台服务器4G内存，2个CPU核心以上<br>所有服务器使用Ubuntu server 16.03<br>所有节点提前安装好Docker<a href=\"/2018/01/11/docker-install-note/\">安装示例</a></p>\n<h2 id=\"安装kubeadm\"><a href=\"#安装kubeadm\" class=\"headerlink\" title=\"安装kubeadm\"></a>安装kubeadm</h2><p>在所有节点上安装kubeadm</p>\n<p>查看apt安装源如下配置，<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat /etc/apt/sources.list</span><br><span class=\"line\"></span><br><span class=\"line\"># kubeadm及kubernetes组件安装源</span><br><span class=\"line\">deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main</span><br></pre></td></tr></table></figure></p>\n<p>更新源，可以不理会gpg的报错信息。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ apt-get update</span><br><span class=\"line\"></span><br><span class=\"line\">Ign:4 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease</span><br><span class=\"line\">Fetched 8,993 B in 0s (20.7 kB/s)</span><br><span class=\"line\">Reading package lists... Done</span><br><span class=\"line\">W: GPG error: https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease: The following signatures couldn&apos;t be verified because the public key is not available: NO_PUBKEY 6A030B21BA07F4FB</span><br><span class=\"line\">W: The repository &apos;https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease&apos; is not signed.</span><br><span class=\"line\">N: Data from such a repository can&apos;t be authenticated and is therefore potentially dangerous to use.</span><br><span class=\"line\">N: See apt-secure(8) manpage for repository creation and user configuration details.</span><br></pre></td></tr></table></figure></p>\n<p>强制安装kubeadm，kubectl，kubelet软件包。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ apt-get install -y kubelet kubeadm kubectl kubernetes-cni --allow-unauthenticated</span><br></pre></td></tr></table></figure>\n<p>kubeadm安装完以后，就可以使用它来快速安装部署Kubernetes集群了。</p>\n<h2 id=\"下载镜像文件\"><a href=\"#下载镜像文件\" class=\"headerlink\" title=\"下载镜像文件\"></a>下载镜像文件</h2><p>因为在墙内的关系，访问不到gcr站点，所以需要先准备好所需镜像,提供的镜像是1.10.1版本的。<br>地址: <a href=\"https://pan.baidu.com/s/1CmoDA3jQgD8kkF3QfI90qg\" target=\"_blank\" rel=\"noopener\">https://pan.baidu.com/s/1CmoDA3jQgD8kkF3QfI90qg</a> 密码: fkxw<br>下载kube1.10.tar.gz并解压。</p>\n<p>master节点load镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker load -i kube-master.tar</span><br><span class=\"line\">$ docker load -i calico-3.1.3.tar</span><br><span class=\"line\">$ docker load -i coredns-1.1.3.tar</span><br></pre></td></tr></table></figure></p>\n<p>node节点load镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker load -i kube-node.tar</span><br><span class=\"line\">$ docker load -i calico-3.1.3.tar</span><br><span class=\"line\">$ docker load -i coredns-1.1.3.tar</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"初始化master节点\"><a href=\"#初始化master节点\" class=\"headerlink\" title=\"初始化master节点\"></a>初始化master节点</h2><p>–pod-network-cidr这个参数的ip地址可以根据实际情况修改，需要跟calico网络的CALICO_IPV4POOL_CIDR一致<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm init --pod-network-cidr=172.16.1.0/16 --kubernetes-version v1.10.1</span><br><span class=\"line\"></span><br><span class=\"line\">[init] Using Kubernetes version: v1.10.1</span><br><span class=\"line\">[init] Using Authorization modes: [Node RBAC]</span><br><span class=\"line\">[preflight] Running pre-flight checks.</span><br><span class=\"line\">\t[WARNING FileExisting-crictl]: crictl not found in system path</span><br><span class=\"line\">Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl</span><br><span class=\"line\">[preflight] Starting the kubelet service</span><br><span class=\"line\">[certificates] Generated ca certificate and key.</span><br><span class=\"line\">[certificates] Generated apiserver certificate and key.</span><br><span class=\"line\">[certificates] apiserver serving cert is signed for DNS names [ubuntu-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.250]</span><br><span class=\"line\">[certificates] Generated apiserver-kubelet-client certificate and key.</span><br><span class=\"line\">[certificates] Generated etcd/ca certificate and key.</span><br><span class=\"line\">[certificates] Generated etcd/server certificate and key.</span><br><span class=\"line\">[certificates] etcd/server serving cert is signed for DNS names [localhost] and IPs [127.0.0.1]</span><br><span class=\"line\">[certificates] Generated etcd/peer certificate and key.</span><br><span class=\"line\">[certificates] etcd/peer serving cert is signed for DNS names [ubuntu-master] and IPs [192.168.10.250]</span><br><span class=\"line\">[certificates] Generated etcd/healthcheck-client certificate and key.</span><br><span class=\"line\">[certificates] Generated apiserver-etcd-client certificate and key.</span><br><span class=\"line\">[certificates] Generated sa key and public key.</span><br><span class=\"line\">[certificates] Generated front-proxy-ca certificate and key.</span><br><span class=\"line\">[certificates] Generated front-proxy-client certificate and key.</span><br><span class=\"line\">[certificates] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class=\"line\">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class=\"line\">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;</span><br><span class=\"line\">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;</span><br><span class=\"line\">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;</span><br><span class=\"line\">[controlplane] Wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class=\"line\">[controlplane] Wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class=\"line\">[controlplane] Wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class=\"line\">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class=\"line\">[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;.</span><br><span class=\"line\">[init] This might take a minute or longer if the control plane images have to be pulled.</span><br><span class=\"line\">[apiclient] All control plane components are healthy after 28.003828 seconds</span><br><span class=\"line\">[uploadconfig] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class=\"line\">[markmaster] Will mark node ubuntu-master as master by adding a label and a taint</span><br><span class=\"line\">[markmaster] Master ubuntu-master tainted and labelled with key/value: node-role.kubernetes.io/master=&quot;&quot;</span><br><span class=\"line\">[bootstraptoken] Using token: rw4enn.mvk547juq7qi2b5f</span><br><span class=\"line\">[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class=\"line\">[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class=\"line\">[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class=\"line\">[bootstraptoken] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class=\"line\">[addons] Applied essential addon: kube-dns</span><br><span class=\"line\">[addons] Applied essential addon: kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">Your Kubernetes master has initialized successfully!</span><br><span class=\"line\"></span><br><span class=\"line\">To start using your cluster, you need to run the following as a regular user:</span><br><span class=\"line\"></span><br><span class=\"line\">  mkdir -p $HOME/.kube</span><br><span class=\"line\">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class=\"line\"></span><br><span class=\"line\">You should now deploy a pod network to the cluster.</span><br><span class=\"line\">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class=\"line\">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class=\"line\"></span><br><span class=\"line\">You can now join any number of machines by running the following on each node</span><br><span class=\"line\">as root:</span><br><span class=\"line\"></span><br><span class=\"line\">  kubeadm join 192.168.10.250:6443 --token rw4enn.mvk547juq7qi2b5f --discovery-token-ca-cert-hash sha256:ba260d5191213382a806a9a7d92c9e6bb09061847c7914b1ac584d0c69471579</span><br></pre></td></tr></table></figure></p>\n<p>执行如下命令来配置kubectl<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p $HOME/.kube</span><br><span class=\"line\">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure></p>\n<p>这样master的节点就配置好了，并且可以使用kubectl来进行各种操作了，根据上面的提示接着往下做，将slave节点加入到集群。</p>\n<h2 id=\"node节点加入集群\"><a href=\"#node节点加入集群\" class=\"headerlink\" title=\"node节点加入集群\"></a>node节点加入集群</h2><p>在node节点上执行kubeadm join命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm join 192.168.10.250:6443 --token rw4enn.mvk547juq7qi2b5f --discovery-token-ca-cert-hash sha256:ba260d5191213382a806a9a7d92c9e6bb09061847c7914b1ac584d0c69471579</span><br><span class=\"line\"></span><br><span class=\"line\">[preflight] Running pre-flight checks.</span><br><span class=\"line\">\t[WARNING FileExisting-crictl]: crictl not found in system path</span><br><span class=\"line\">Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl</span><br><span class=\"line\">[discovery] Trying to connect to API Server &quot;192.168.10.250:6443&quot;</span><br><span class=\"line\">[discovery] Created cluster-info discovery client, requesting info from &quot;https://192.168.10.250:6443&quot;</span><br><span class=\"line\">[discovery] Requesting info from &quot;https://192.168.10.250:6443&quot; again to validate TLS against the pinned public key</span><br><span class=\"line\">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;192.168.10.250:6443&quot;</span><br><span class=\"line\">[discovery] Successfully established connection with API Server &quot;192.168.10.250:6443&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">This node has joined the cluster:</span><br><span class=\"line\">* Certificate signing request was sent to master and a response</span><br><span class=\"line\">  was received.</span><br><span class=\"line\">* The Kubelet was informed of the new secure connection details.</span><br><span class=\"line\"></span><br><span class=\"line\">Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster.</span><br></pre></td></tr></table></figure></p>\n<p>查看节点状态<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get nodes</span><br><span class=\"line\"></span><br><span class=\"line\">NAME         STATUS    ROLES     AGE       VERSION</span><br><span class=\"line\">k8s-master   Ready     master    1h        v1.10.4</span><br><span class=\"line\">k8s-node1    Ready     &lt;none&gt;    1h        v1.10.4</span><br></pre></td></tr></table></figure></p>\n<p>在master节点查看pod状态<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pods -n kube-system -o wide</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"安装Calico网络\"><a href=\"#安装Calico网络\" class=\"headerlink\" title=\"安装Calico网络\"></a>安装Calico网络</h2><p><a href=\"https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml\" target=\"_blank\" rel=\"noopener\">calico.yaml文件</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br><span class=\"line\">262</span><br><span class=\"line\">263</span><br><span class=\"line\">264</span><br><span class=\"line\">265</span><br><span class=\"line\">266</span><br><span class=\"line\">267</span><br><span class=\"line\">268</span><br><span class=\"line\">269</span><br><span class=\"line\">270</span><br><span class=\"line\">271</span><br><span class=\"line\">272</span><br><span class=\"line\">273</span><br><span class=\"line\">274</span><br><span class=\"line\">275</span><br><span class=\"line\">276</span><br><span class=\"line\">277</span><br><span class=\"line\">278</span><br><span class=\"line\">279</span><br><span class=\"line\">280</span><br><span class=\"line\">281</span><br><span class=\"line\">282</span><br><span class=\"line\">283</span><br><span class=\"line\">284</span><br><span class=\"line\">285</span><br><span class=\"line\">286</span><br><span class=\"line\">287</span><br><span class=\"line\">288</span><br><span class=\"line\">289</span><br><span class=\"line\">290</span><br><span class=\"line\">291</span><br><span class=\"line\">292</span><br><span class=\"line\">293</span><br><span class=\"line\">294</span><br><span class=\"line\">295</span><br><span class=\"line\">296</span><br><span class=\"line\">297</span><br><span class=\"line\">298</span><br><span class=\"line\">299</span><br><span class=\"line\">300</span><br><span class=\"line\">301</span><br><span class=\"line\">302</span><br><span class=\"line\">303</span><br><span class=\"line\">304</span><br><span class=\"line\">305</span><br><span class=\"line\">306</span><br><span class=\"line\">307</span><br><span class=\"line\">308</span><br><span class=\"line\">309</span><br><span class=\"line\">310</span><br><span class=\"line\">311</span><br><span class=\"line\">312</span><br><span class=\"line\">313</span><br><span class=\"line\">314</span><br><span class=\"line\">315</span><br><span class=\"line\">316</span><br><span class=\"line\">317</span><br><span class=\"line\">318</span><br><span class=\"line\">319</span><br><span class=\"line\">320</span><br><span class=\"line\">321</span><br><span class=\"line\">322</span><br><span class=\"line\">323</span><br><span class=\"line\">324</span><br><span class=\"line\">325</span><br><span class=\"line\">326</span><br><span class=\"line\">327</span><br><span class=\"line\">328</span><br><span class=\"line\">329</span><br><span class=\"line\">330</span><br><span class=\"line\">331</span><br><span class=\"line\">332</span><br><span class=\"line\">333</span><br><span class=\"line\">334</span><br><span class=\"line\">335</span><br><span class=\"line\">336</span><br><span class=\"line\">337</span><br><span class=\"line\">338</span><br><span class=\"line\">339</span><br><span class=\"line\">340</span><br><span class=\"line\">341</span><br><span class=\"line\">342</span><br><span class=\"line\">343</span><br><span class=\"line\">344</span><br><span class=\"line\">345</span><br><span class=\"line\">346</span><br><span class=\"line\">347</span><br><span class=\"line\">348</span><br><span class=\"line\">349</span><br><span class=\"line\">350</span><br><span class=\"line\">351</span><br><span class=\"line\">352</span><br><span class=\"line\">353</span><br><span class=\"line\">354</span><br><span class=\"line\">355</span><br><span class=\"line\">356</span><br><span class=\"line\">357</span><br><span class=\"line\">358</span><br><span class=\"line\">359</span><br><span class=\"line\">360</span><br><span class=\"line\">361</span><br><span class=\"line\">362</span><br><span class=\"line\">363</span><br><span class=\"line\">364</span><br><span class=\"line\">365</span><br><span class=\"line\">366</span><br><span class=\"line\">367</span><br><span class=\"line\">368</span><br><span class=\"line\">369</span><br><span class=\"line\">370</span><br><span class=\"line\">371</span><br><span class=\"line\">372</span><br><span class=\"line\">373</span><br><span class=\"line\">374</span><br><span class=\"line\">375</span><br><span class=\"line\">376</span><br><span class=\"line\">377</span><br><span class=\"line\">378</span><br><span class=\"line\">379</span><br><span class=\"line\">380</span><br><span class=\"line\">381</span><br><span class=\"line\">382</span><br><span class=\"line\">383</span><br><span class=\"line\">384</span><br><span class=\"line\">385</span><br><span class=\"line\">386</span><br><span class=\"line\">387</span><br><span class=\"line\">388</span><br><span class=\"line\">389</span><br><span class=\"line\">390</span><br><span class=\"line\">391</span><br><span class=\"line\">392</span><br><span class=\"line\">393</span><br><span class=\"line\">394</span><br><span class=\"line\">395</span><br><span class=\"line\">396</span><br><span class=\"line\">397</span><br><span class=\"line\">398</span><br><span class=\"line\">399</span><br><span class=\"line\">400</span><br><span class=\"line\">401</span><br><span class=\"line\">402</span><br><span class=\"line\">403</span><br><span class=\"line\">404</span><br><span class=\"line\">405</span><br><span class=\"line\">406</span><br><span class=\"line\">407</span><br><span class=\"line\">408</span><br><span class=\"line\">409</span><br><span class=\"line\">410</span><br><span class=\"line\">411</span><br><span class=\"line\">412</span><br><span class=\"line\">413</span><br><span class=\"line\">414</span><br><span class=\"line\">415</span><br><span class=\"line\">416</span><br><span class=\"line\">417</span><br><span class=\"line\">418</span><br><span class=\"line\">419</span><br><span class=\"line\">420</span><br><span class=\"line\">421</span><br><span class=\"line\">422</span><br><span class=\"line\">423</span><br><span class=\"line\">424</span><br><span class=\"line\">425</span><br><span class=\"line\">426</span><br><span class=\"line\">427</span><br><span class=\"line\">428</span><br><span class=\"line\">429</span><br><span class=\"line\">430</span><br><span class=\"line\">431</span><br><span class=\"line\">432</span><br><span class=\"line\">433</span><br><span class=\"line\">434</span><br><span class=\"line\">435</span><br><span class=\"line\">436</span><br><span class=\"line\">437</span><br><span class=\"line\">438</span><br><span class=\"line\">439</span><br><span class=\"line\">440</span><br><span class=\"line\">441</span><br><span class=\"line\">442</span><br><span class=\"line\">443</span><br><span class=\"line\">444</span><br><span class=\"line\">445</span><br><span class=\"line\">446</span><br><span class=\"line\">447</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt;EOF &gt; calico.yaml</span><br><span class=\"line\"># Calico Version v3.1.3</span><br><span class=\"line\"># https://docs.projectcalico.org/v3.1/releases#v3.1.3</span><br><span class=\"line\"># This manifest includes the following component versions:</span><br><span class=\"line\">#   calico/node:v3.1.3</span><br><span class=\"line\">#   calico/cni:v3.1.3</span><br><span class=\"line\">#   calico/kube-controllers:v3.1.3</span><br><span class=\"line\"></span><br><span class=\"line\"># This ConfigMap is used to configure a self-hosted Calico installation.</span><br><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-config</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">data:</span><br><span class=\"line\">  # The location of your etcd cluster.  This uses the Service clusterIP defined below.</span><br><span class=\"line\">  etcd_endpoints: &quot;http://10.96.232.136:6666&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Configure the Calico backend to use.</span><br><span class=\"line\">  calico_backend: &quot;bird&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # The CNI network configuration to install on each node.</span><br><span class=\"line\">  cni_network_config: |-</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;name&quot;: &quot;k8s-pod-network&quot;,</span><br><span class=\"line\">      &quot;cniVersion&quot;: &quot;0.3.0&quot;,</span><br><span class=\"line\">      &quot;plugins&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;calico&quot;,</span><br><span class=\"line\">          &quot;etcd_endpoints&quot;: &quot;__ETCD_ENDPOINTS__&quot;,</span><br><span class=\"line\">          &quot;log_level&quot;: &quot;info&quot;,</span><br><span class=\"line\">          &quot;mtu&quot;: 1500,</span><br><span class=\"line\">          &quot;ipam&quot;: &#123;</span><br><span class=\"line\">              &quot;type&quot;: &quot;calico-ipam&quot;,</span><br><span class=\"line\">              &quot;subnet&quot;: &quot;usePodCidr&quot;</span><br><span class=\"line\">          &#125;,</span><br><span class=\"line\">          &quot;policy&quot;: &#123;</span><br><span class=\"line\">              &quot;type&quot;: &quot;k8s&quot;</span><br><span class=\"line\">          &#125;,</span><br><span class=\"line\">          &quot;kubernetes&quot;: &#123;</span><br><span class=\"line\">              &quot;kubeconfig&quot;: &quot;__KUBECONFIG_FILEPATH__&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;portmap&quot;,</span><br><span class=\"line\">          &quot;snat&quot;: true,</span><br><span class=\"line\">          &quot;capabilities&quot;: &#123;&quot;portMappings&quot;: true&#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\"># This manifest installs the Calico etcd on the kubeadm master.  This uses a DaemonSet</span><br><span class=\"line\"># to force it to run on the master even when the master isn&apos;t schedulable, and uses</span><br><span class=\"line\"># nodeSelector to ensure it only runs on the master.</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: DaemonSet</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-etcd</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: calico-etcd</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        k8s-app: calico-etcd</span><br><span class=\"line\">      annotations:</span><br><span class=\"line\">        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler</span><br><span class=\"line\">        # reserves resources for critical add-on pods so that they can be rescheduled after</span><br><span class=\"line\">        # a failure.  This annotation works in tandem with the toleration below.</span><br><span class=\"line\">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">        # This taint is set by all kubelets running `--cloud-provider=external`</span><br><span class=\"line\">        # so we should tolerate it to schedule the calico pods</span><br><span class=\"line\">        - key: node.cloudprovider.kubernetes.io/uninitialized</span><br><span class=\"line\">          value: &quot;true&quot;</span><br><span class=\"line\">          effect: NoSchedule</span><br><span class=\"line\">        # Allow this pod to run on the master.</span><br><span class=\"line\">        - key: node-role.kubernetes.io/master</span><br><span class=\"line\">          effect: NoSchedule</span><br><span class=\"line\">        # Allow this pod to be rescheduled while the node is in &quot;critical add-ons only&quot; mode.</span><br><span class=\"line\">        # This, along with the annotation above marks this pod as a critical add-on.</span><br><span class=\"line\">        - key: CriticalAddonsOnly</span><br><span class=\"line\">          operator: Exists</span><br><span class=\"line\">      # Only run this pod on the master.</span><br><span class=\"line\">      nodeSelector:</span><br><span class=\"line\">        node-role.kubernetes.io/master: &quot;&quot;</span><br><span class=\"line\">      hostNetwork: true</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">        - name: calico-etcd</span><br><span class=\"line\">          image: quay.io/coreos/etcd:v3.1.10</span><br><span class=\"line\">          env:</span><br><span class=\"line\">            - name: CALICO_ETCD_IP</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                fieldRef:</span><br><span class=\"line\">                  fieldPath: status.podIP</span><br><span class=\"line\">          command:</span><br><span class=\"line\">          - /usr/local/bin/etcd</span><br><span class=\"line\">          args:</span><br><span class=\"line\">          - --name=calico</span><br><span class=\"line\">          - --data-dir=/var/etcd/calico-data</span><br><span class=\"line\">          - --advertise-client-urls=http://$CALICO_ETCD_IP:6666</span><br><span class=\"line\">          - --listen-client-urls=http://0.0.0.0:6666</span><br><span class=\"line\">          - --listen-peer-urls=http://0.0.0.0:6667</span><br><span class=\"line\">          - --auto-compaction-retention=1</span><br><span class=\"line\">          volumeMounts:</span><br><span class=\"line\">            - name: var-etcd</span><br><span class=\"line\">              mountPath: /var/etcd</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">        - name: var-etcd</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /var/etcd</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\"># This manifest installs the Service which gets traffic to the Calico</span><br><span class=\"line\"># etcd.</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: calico-etcd</span><br><span class=\"line\">  name: calico-etcd</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  # Select the calico-etcd pod running on the master.</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: calico-etcd</span><br><span class=\"line\">  # This ClusterIP needs to be known in advance, since we cannot rely</span><br><span class=\"line\">  # on DNS to get access to etcd.</span><br><span class=\"line\">  clusterIP: 10.96.232.136</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">    - port: 6666</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\"># This manifest installs the calico/node container, as well</span><br><span class=\"line\"># as the Calico CNI plugins and network config on</span><br><span class=\"line\"># each master and worker node in a Kubernetes cluster.</span><br><span class=\"line\">kind: DaemonSet</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-node</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: calico-node</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      k8s-app: calico-node</span><br><span class=\"line\">  updateStrategy:</span><br><span class=\"line\">    type: RollingUpdate</span><br><span class=\"line\">    rollingUpdate:</span><br><span class=\"line\">      maxUnavailable: 1</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        k8s-app: calico-node</span><br><span class=\"line\">      annotations:</span><br><span class=\"line\">        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler</span><br><span class=\"line\">        # reserves resources for critical add-on pods so that they can be rescheduled after</span><br><span class=\"line\">        # a failure.  This annotation works in tandem with the toleration below.</span><br><span class=\"line\">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      hostNetwork: true</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">        # Make sure calico/node gets scheduled on all nodes.</span><br><span class=\"line\">        - effect: NoSchedule</span><br><span class=\"line\">          operator: Exists</span><br><span class=\"line\">        # Mark the pod as a critical add-on for rescheduling.</span><br><span class=\"line\">        - key: CriticalAddonsOnly</span><br><span class=\"line\">          operator: Exists</span><br><span class=\"line\">        - effect: NoExecute</span><br><span class=\"line\">          operator: Exists</span><br><span class=\"line\">      serviceAccountName: calico-cni-plugin</span><br><span class=\"line\">      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &quot;force</span><br><span class=\"line\">      # deletion&quot;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.</span><br><span class=\"line\">      terminationGracePeriodSeconds: 0</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">        # Runs calico/node container on each Kubernetes node.  This</span><br><span class=\"line\">        # container programs network policy and routes on each</span><br><span class=\"line\">        # host.</span><br><span class=\"line\">        - name: calico-node</span><br><span class=\"line\">          image: quay.io/calico/node:v3.1.3</span><br><span class=\"line\">          env:</span><br><span class=\"line\">            # The location of the Calico etcd cluster.</span><br><span class=\"line\">            - name: ETCD_ENDPOINTS</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                configMapKeyRef:</span><br><span class=\"line\">                  name: calico-config</span><br><span class=\"line\">                  key: etcd_endpoints</span><br><span class=\"line\">            # Enable BGP.  Disable to enforce policy only.</span><br><span class=\"line\">            - name: CALICO_NETWORKING_BACKEND</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                configMapKeyRef:</span><br><span class=\"line\">                  name: calico-config</span><br><span class=\"line\">                  key: calico_backend</span><br><span class=\"line\">            # Cluster type to identify the deployment type</span><br><span class=\"line\">            - name: CLUSTER_TYPE</span><br><span class=\"line\">              value: &quot;kubeadm,bgp&quot;</span><br><span class=\"line\">            # Disable file logging so `kubectl logs` works.</span><br><span class=\"line\">            - name: CALICO_DISABLE_FILE_LOGGING</span><br><span class=\"line\">              value: &quot;true&quot;</span><br><span class=\"line\">            # Set noderef for node controller.</span><br><span class=\"line\">            - name: CALICO_K8S_NODE_REF</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                fieldRef:</span><br><span class=\"line\">                  fieldPath: spec.nodeName</span><br><span class=\"line\">            # Set Felix endpoint to host default action to ACCEPT.</span><br><span class=\"line\">            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION</span><br><span class=\"line\">              value: &quot;ACCEPT&quot;</span><br><span class=\"line\">            # The default IPv4 pool to create on startup if none exists. Pod IPs will be</span><br><span class=\"line\">            # chosen from this range. Changing this value after installation will have</span><br><span class=\"line\">            # no effect. This should fall within `--cluster-cidr`.</span><br><span class=\"line\">            - name: CALICO_IPV4POOL_CIDR</span><br><span class=\"line\">              value: &quot;172.16.1.0/16&quot;</span><br><span class=\"line\">            - name: CALICO_IPV4POOL_IPIP</span><br><span class=\"line\">              value: &quot;Always&quot;</span><br><span class=\"line\">            # Disable IPv6 on Kubernetes.</span><br><span class=\"line\">            - name: FELIX_IPV6SUPPORT</span><br><span class=\"line\">              value: &quot;false&quot;</span><br><span class=\"line\">            # Set MTU for tunnel device used if ipip is enabled</span><br><span class=\"line\">            - name: FELIX_IPINIPMTU</span><br><span class=\"line\">              value: &quot;1440&quot;</span><br><span class=\"line\">            # Set Felix logging to &quot;info&quot;</span><br><span class=\"line\">            - name: FELIX_LOGSEVERITYSCREEN</span><br><span class=\"line\">              value: &quot;info&quot;</span><br><span class=\"line\">            # Auto-detect the BGP IP address.</span><br><span class=\"line\">            - name: IP</span><br><span class=\"line\">              value: &quot;autodetect&quot;</span><br><span class=\"line\">            - name: FELIX_HEALTHENABLED</span><br><span class=\"line\">              value: &quot;true&quot;</span><br><span class=\"line\">          securityContext:</span><br><span class=\"line\">            privileged: true</span><br><span class=\"line\">          resources:</span><br><span class=\"line\">            requests:</span><br><span class=\"line\">              cpu: 250m</span><br><span class=\"line\">          livenessProbe:</span><br><span class=\"line\">            httpGet:</span><br><span class=\"line\">              path: /liveness</span><br><span class=\"line\">              port: 9099</span><br><span class=\"line\">            periodSeconds: 10</span><br><span class=\"line\">            initialDelaySeconds: 10</span><br><span class=\"line\">            failureThreshold: 6</span><br><span class=\"line\">          readinessProbe:</span><br><span class=\"line\">            httpGet:</span><br><span class=\"line\">              path: /readiness</span><br><span class=\"line\">              port: 9099</span><br><span class=\"line\">            periodSeconds: 10</span><br><span class=\"line\">          volumeMounts:</span><br><span class=\"line\">            - mountPath: /lib/modules</span><br><span class=\"line\">              name: lib-modules</span><br><span class=\"line\">              readOnly: true</span><br><span class=\"line\">            - mountPath: /var/run/calico</span><br><span class=\"line\">              name: var-run-calico</span><br><span class=\"line\">              readOnly: false</span><br><span class=\"line\">            - mountPath: /var/lib/calico</span><br><span class=\"line\">              name: var-lib-calico</span><br><span class=\"line\">              readOnly: false</span><br><span class=\"line\">        # This container installs the Calico CNI binaries</span><br><span class=\"line\">        # and CNI network config file on each node.</span><br><span class=\"line\">        - name: install-cni</span><br><span class=\"line\">          image: quay.io/calico/cni:v3.1.3</span><br><span class=\"line\">          command: [&quot;/install-cni.sh&quot;]</span><br><span class=\"line\">          env:</span><br><span class=\"line\">            # Name of the CNI config file to create.</span><br><span class=\"line\">            - name: CNI_CONF_NAME</span><br><span class=\"line\">              value: &quot;10-calico.conflist&quot;</span><br><span class=\"line\">            # The location of the Calico etcd cluster.</span><br><span class=\"line\">            - name: ETCD_ENDPOINTS</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                configMapKeyRef:</span><br><span class=\"line\">                  name: calico-config</span><br><span class=\"line\">                  key: etcd_endpoints</span><br><span class=\"line\">            # The CNI network config to install on each node.</span><br><span class=\"line\">            - name: CNI_NETWORK_CONFIG</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                configMapKeyRef:</span><br><span class=\"line\">                  name: calico-config</span><br><span class=\"line\">                  key: cni_network_config</span><br><span class=\"line\">          volumeMounts:</span><br><span class=\"line\">            - mountPath: /host/opt/cni/bin</span><br><span class=\"line\">              name: cni-bin-dir</span><br><span class=\"line\">            - mountPath: /host/etc/cni/net.d</span><br><span class=\"line\">              name: cni-net-dir</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">        # Used by calico/node.</span><br><span class=\"line\">        - name: lib-modules</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /lib/modules</span><br><span class=\"line\">        - name: var-run-calico</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /var/run/calico</span><br><span class=\"line\">        - name: var-lib-calico</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /var/lib/calico</span><br><span class=\"line\">        # Used to install CNI.</span><br><span class=\"line\">        - name: cni-bin-dir</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /opt/cni/bin</span><br><span class=\"line\">        - name: cni-net-dir</span><br><span class=\"line\">          hostPath:</span><br><span class=\"line\">            path: /etc/cni/net.d</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\"># This manifest deploys the Calico Kubernetes controllers.</span><br><span class=\"line\"># See https://github.com/projectcalico/kube-controllers</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: calico-kube-controllers</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  # The controllers can only have a single active instance.</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  strategy:</span><br><span class=\"line\">    type: Recreate</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      name: calico-kube-controllers</span><br><span class=\"line\">      namespace: kube-system</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        k8s-app: calico-kube-controllers</span><br><span class=\"line\">      annotations:</span><br><span class=\"line\">        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler</span><br><span class=\"line\">        # reserves resources for critical add-on pods so that they can be rescheduled after</span><br><span class=\"line\">        # a failure.  This annotation works in tandem with the toleration below.</span><br><span class=\"line\">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      # The controllers must run in the host network namespace so that</span><br><span class=\"line\">      # it isn&apos;t governed by policy that would prevent it from working.</span><br><span class=\"line\">      hostNetwork: true</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">        # Allow this pod to be rescheduled while the node is in &quot;critical add-ons only&quot; mode.</span><br><span class=\"line\">        # This, along with the annotation above marks this pod as a critical add-on.</span><br><span class=\"line\">        - key: CriticalAddonsOnly</span><br><span class=\"line\">          operator: Exists</span><br><span class=\"line\">        - key: node-role.kubernetes.io/master</span><br><span class=\"line\">          effect: NoSchedule</span><br><span class=\"line\">      serviceAccountName: calico-kube-controllers</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">        - name: calico-kube-controllers</span><br><span class=\"line\">          image: quay.io/calico/kube-controllers:v3.1.3</span><br><span class=\"line\">          env:</span><br><span class=\"line\">            # The location of the Calico etcd cluster.</span><br><span class=\"line\">            - name: ETCD_ENDPOINTS</span><br><span class=\"line\">              valueFrom:</span><br><span class=\"line\">                configMapKeyRef:</span><br><span class=\"line\">                  name: calico-config</span><br><span class=\"line\">                  key: etcd_endpoints</span><br><span class=\"line\">            # Choose which controllers to run.</span><br><span class=\"line\">            - name: ENABLED_CONTROLLERS</span><br><span class=\"line\">              value: policy,profile,workloadendpoint,node</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-cni-plugin</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: calico-cni-plugin</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: calico-cni-plugin</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">kind: ClusterRole</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-cni-plugin</span><br><span class=\"line\">rules:</span><br><span class=\"line\">  - apiGroups: [&quot;&quot;]</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">      - pods</span><br><span class=\"line\">      - nodes</span><br><span class=\"line\">    verbs:</span><br><span class=\"line\">      - get</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-cni-plugin</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">kind: ClusterRole</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">rules:</span><br><span class=\"line\">  - apiGroups:</span><br><span class=\"line\">    - &quot;&quot;</span><br><span class=\"line\">    - extensions</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">      - pods</span><br><span class=\"line\">      - namespaces</span><br><span class=\"line\">      - networkpolicies</span><br><span class=\"line\">      - nodes</span><br><span class=\"line\">    verbs:</span><br><span class=\"line\">      - watch</span><br><span class=\"line\">      - list</span><br><span class=\"line\">  - apiGroups:</span><br><span class=\"line\">    - networking.k8s.io</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">      - networkpolicies</span><br><span class=\"line\">    verbs:</span><br><span class=\"line\">      - watch</span><br><span class=\"line\">      - list</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: calico-kube-controllers</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>部署calico<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"安装CoreDNS\"><a href=\"#安装CoreDNS\" class=\"headerlink\" title=\"安装CoreDNS\"></a>安装CoreDNS</h2><p>在/etc/kubernetes/manifests/下创建coredns目录,<br>需要把官方提供的两个文件deploy.sh和coredns.yaml.sed放到conredns目录</p>\n<h3 id=\"准备配置文件\"><a href=\"#准备配置文件\" class=\"headerlink\" title=\"准备配置文件\"></a>准备配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -p /etc/kubernetes/manifests/coredns</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh\" target=\"_blank\" rel=\"noopener\">deploy.sh文件</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/coredns/deploy.sh</span><br><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\"># Deploys CoreDNS to a cluster currently running Kube-DNS.</span><br><span class=\"line\"></span><br><span class=\"line\">show_help () &#123;</span><br><span class=\"line\">cat &lt;&lt; USAGE</span><br><span class=\"line\">usage: $0 [ -r REVERSE-CIDR ] [ -i DNS-IP ] [ -d CLUSTER-DOMAIN ] [ -t YAML-TEMPLATE ]</span><br><span class=\"line\">    -r : Define a reverse zone for the given CIDR. You may specifcy this option more</span><br><span class=\"line\">         than once to add multiple reverse zones. If no reverse CIDRs are defined,</span><br><span class=\"line\">         then the default is to handle all reverse zones (i.e. in-addr.arpa and ip6.arpa)</span><br><span class=\"line\">    -i : Specify the cluster DNS IP address. If not specificed, the IP address of</span><br><span class=\"line\">         the existing &quot;kube-dns&quot; service is used, if present.</span><br><span class=\"line\">USAGE</span><br><span class=\"line\">exit 0</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"># Simple Defaults</span><br><span class=\"line\">CLUSTER_DOMAIN=cluster.local</span><br><span class=\"line\">YAML_TEMPLATE=`pwd`/coredns.yaml.sed</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># Get Opts</span><br><span class=\"line\">while getopts &quot;hr:i:d:t:&quot; opt; do</span><br><span class=\"line\">    case &quot;$opt&quot; in</span><br><span class=\"line\">    h)  show_help</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    r)  REVERSE_CIDRS=&quot;$REVERSE_CIDRS $OPTARG&quot;</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    i)  CLUSTER_DNS_IP=$OPTARG</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    d)  CLUSTER_DOMAIN=$OPTARG</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    t)  YAML_TEMPLATE=$OPTARG</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    esac</span><br><span class=\"line\">done</span><br><span class=\"line\"></span><br><span class=\"line\"># Conditional Defaults</span><br><span class=\"line\">if [[ -z $REVERSE_CIDRS ]]; then</span><br><span class=\"line\">  REVERSE_CIDRS=&quot;in-addr.arpa ip6.arpa&quot;</span><br><span class=\"line\">fi</span><br><span class=\"line\">if [[ -z $CLUSTER_DNS_IP ]]; then</span><br><span class=\"line\">  # Default IP to kube-dns IP</span><br><span class=\"line\">  CLUSTER_DNS_IP=$(kubectl get service --namespace kube-system kube-dns -o jsonpath=&quot;&#123;.spec.clusterIP&#125;&quot;)</span><br><span class=\"line\">  if [ $? -ne 0 ]; then</span><br><span class=\"line\">      &gt;&amp;2 echo &quot;Error! The IP address for DNS service couldn&apos;t be determined automatically. Please specify the DNS-IP with the &apos;-i&apos; option.&quot;</span><br><span class=\"line\">      exit 2</span><br><span class=\"line\">  fi</span><br><span class=\"line\">fi</span><br><span class=\"line\"></span><br><span class=\"line\">sed -e s/CLUSTER_DNS_IP/$CLUSTER_DNS_IP/g -e s/CLUSTER_DOMAIN/$CLUSTER_DOMAIN/g -e &quot;s?REVERSE_CIDRS?$REVERSE_CIDRS?g&quot; $YAML_TEMPLATE</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p><a href=\"https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed\" target=\"_blank\" rel=\"noopener\">coredns.yaml.sed文件</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/coredns/coredns.yaml.sed</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: coredns</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRole</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class=\"line\">  name: system:coredns</span><br><span class=\"line\">rules:</span><br><span class=\"line\">- apiGroups:</span><br><span class=\"line\">  - &quot;&quot;</span><br><span class=\"line\">  resources:</span><br><span class=\"line\">  - endpoints</span><br><span class=\"line\">  - services</span><br><span class=\"line\">  - pods</span><br><span class=\"line\">  - namespaces</span><br><span class=\"line\">  verbs:</span><br><span class=\"line\">  - list</span><br><span class=\"line\">  - watch</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  annotations:</span><br><span class=\"line\">    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class=\"line\">  name: system:coredns</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: system:coredns</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: coredns</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ConfigMap</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: coredns</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">data:</span><br><span class=\"line\">  Corefile: |</span><br><span class=\"line\">    .:53 &#123;</span><br><span class=\"line\">        errors</span><br><span class=\"line\">        health</span><br><span class=\"line\">        kubernetes CLUSTER_DOMAIN REVERSE_CIDRS &#123;</span><br><span class=\"line\">          pods insecure</span><br><span class=\"line\">          upstream</span><br><span class=\"line\">          fallthrough in-addr.arpa ip6.arpa</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        prometheus :9153</span><br><span class=\"line\">        proxy . /etc/resolv.conf</span><br><span class=\"line\">        cache 30</span><br><span class=\"line\">        reload</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: coredns</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kube-dns</span><br><span class=\"line\">    kubernetes.io/name: &quot;CoreDNS&quot;</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 2</span><br><span class=\"line\">  strategy:</span><br><span class=\"line\">    type: RollingUpdate</span><br><span class=\"line\">    rollingUpdate:</span><br><span class=\"line\">      maxUnavailable: 1</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      k8s-app: kube-dns</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        k8s-app: kube-dns</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      serviceAccountName: coredns</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">        - key: &quot;CriticalAddonsOnly&quot;</span><br><span class=\"line\">          operator: &quot;Exists&quot;</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: coredns</span><br><span class=\"line\">        image: coredns/coredns:1.1.3</span><br><span class=\"line\">        imagePullPolicy: IfNotPresent</span><br><span class=\"line\">        args: [ &quot;-conf&quot;, &quot;/etc/coredns/Corefile&quot; ]</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - name: config-volume</span><br><span class=\"line\">          mountPath: /etc/coredns</span><br><span class=\"line\">          readOnly: true</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">        - containerPort: 53</span><br><span class=\"line\">          name: dns</span><br><span class=\"line\">          protocol: UDP</span><br><span class=\"line\">        - containerPort: 53</span><br><span class=\"line\">          name: dns-tcp</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        - containerPort: 9153</span><br><span class=\"line\">          name: metrics</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        securityContext:</span><br><span class=\"line\">          allowPrivilegeEscalation: false</span><br><span class=\"line\">          capabilities:</span><br><span class=\"line\">            add:</span><br><span class=\"line\">            - NET_BIND_SERVICE</span><br><span class=\"line\">            drop:</span><br><span class=\"line\">            - all</span><br><span class=\"line\">          readOnlyRootFilesystem: true</span><br><span class=\"line\">        livenessProbe:</span><br><span class=\"line\">          httpGet:</span><br><span class=\"line\">            path: /health</span><br><span class=\"line\">            port: 8080</span><br><span class=\"line\">            scheme: HTTP</span><br><span class=\"line\">          initialDelaySeconds: 60</span><br><span class=\"line\">          timeoutSeconds: 5</span><br><span class=\"line\">          successThreshold: 1</span><br><span class=\"line\">          failureThreshold: 5</span><br><span class=\"line\">      dnsPolicy: Default</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">        - name: config-volume</span><br><span class=\"line\">          configMap:</span><br><span class=\"line\">            name: coredns</span><br><span class=\"line\">            items:</span><br><span class=\"line\">            - key: Corefile</span><br><span class=\"line\">              path: Corefile</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kube-dns</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  annotations:</span><br><span class=\"line\">    prometheus.io/scrape: &quot;true&quot;</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kube-dns</span><br><span class=\"line\">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class=\"line\">    kubernetes.io/name: &quot;CoreDNS&quot;</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: kube-dns</span><br><span class=\"line\">  clusterIP: CLUSTER_DNS_IP</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">  - name: dns</span><br><span class=\"line\">    port: 53</span><br><span class=\"line\">    protocol: UDP</span><br><span class=\"line\">  - name: dns-tcp</span><br><span class=\"line\">    port: 53</span><br><span class=\"line\">    protocol: TCP</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用CoreDNS替换Kube-DNS\"><a href=\"#使用CoreDNS替换Kube-DNS\" class=\"headerlink\" title=\"使用CoreDNS替换Kube-DNS\"></a>使用CoreDNS替换Kube-DNS</h3><p>最佳的案例场景中，使用CoreDNS替换Kube-DNS只需要使用下面的两个命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ./deploy.sh | kubectl apply -f -</span><br><span class=\"line\">$ kubectl delete --namespace=kube-system deployment kube-dns</span><br></pre></td></tr></table></figure></p>\n<p>查看pod情况<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pods -n kube-system -o wide</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE</span><br><span class=\"line\">calico-etcd-2fn6s                          1/1       Running   0          7m        192.168.10.250   k8s-master</span><br><span class=\"line\">calico-kube-controllers-679568f47c-k7q4w   1/1       Running   0          7m        192.168.10.250   k8s-master</span><br><span class=\"line\">calico-node-8l2mf                          2/2       Running   0          7m        192.168.10.250   k8s-master</span><br><span class=\"line\">calico-node-gx4x8                          2/2       Running   0          5m        192.168.10.57    k8s-node1</span><br><span class=\"line\">coredns-6cc44759b8-8p8cb                   1/1       Running   1          6h        172.16.1.193     k8s-master</span><br><span class=\"line\">coredns-6cc44759b8-xzmfr                   1/1       Running   1          6h        172.16.1.194     k8s-master</span><br><span class=\"line\">etcd-k8s-master                            1/1       Running   3          7h        192.168.10.250   k8s-master</span><br><span class=\"line\">heapster-77b9c5bd7b-cdr7d                  1/1       Running   0          4m        172.16.1.68      k8s-node1</span><br><span class=\"line\">kube-apiserver-k8s-master                  1/1       Running   3          7h        192.168.10.250   k8s-master</span><br><span class=\"line\">kube-controller-manager-k8s-master         1/1       Running   3          7h        192.168.10.250   k8s-master</span><br><span class=\"line\">kube-proxy-7nvtx                           1/1       Running   3          7h        192.168.10.250   k8s-master</span><br><span class=\"line\">kube-proxy-nh6jf                           1/1       Running   0          5m        192.168.10.57    k8s-node1</span><br><span class=\"line\">kube-scheduler-k8s-master                  1/1       Running   3          7h        192.168.10.250   k8s-master</span><br></pre></td></tr></table></figure></p>\n<p>至此，node集群已经部署好了。</p>\n<h2 id=\"部署Dashboard\"><a href=\"#部署Dashboard\" class=\"headerlink\" title=\"部署Dashboard\"></a>部署Dashboard</h2><p>node节点load dashboard镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker load -i kube-dashboard-1.8.3.tar</span><br></pre></td></tr></table></figure></p>\n<p>dashboard.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; dashboard.yaml</span><br><span class=\"line\"># Copyright 2017 The Kubernetes Authors.</span><br><span class=\"line\">#</span><br><span class=\"line\"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class=\"line\"># you may not use this file except in compliance with the License.</span><br><span class=\"line\"># You may obtain a copy of the License at</span><br><span class=\"line\">#</span><br><span class=\"line\">#     http://www.apache.org/licenses/LICENSE-2.0</span><br><span class=\"line\">#</span><br><span class=\"line\"># Unless required by applicable law or agreed to in writing, software</span><br><span class=\"line\"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class=\"line\"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class=\"line\"># See the License for the specific language governing permissions and</span><br><span class=\"line\"># limitations under the License.</span><br><span class=\"line\"></span><br><span class=\"line\"># Configuration to deploy release version of the Dashboard UI compatible with</span><br><span class=\"line\"># Kubernetes 1.8.</span><br><span class=\"line\">#</span><br><span class=\"line\"># Example usage: kubectl create -f &lt;this_file&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"># ------------------- Dashboard Secret ------------------- #</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Secret</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\">  name: kubernetes-dashboard-certs</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">type: Opaque</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"># ------------------- Dashboard Service Account ------------------- #</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\">  name: kubernetes-dashboard</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"># ------------------- Dashboard Role &amp; Role Binding ------------------- #</span><br><span class=\"line\"></span><br><span class=\"line\">kind: Role</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kubernetes-dashboard-minimal</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">rules:</span><br><span class=\"line\">  # Allow Dashboard to create &apos;kubernetes-dashboard-key-holder&apos; secret.</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;secrets&quot;]</span><br><span class=\"line\">  verbs: [&quot;create&quot;]</span><br><span class=\"line\">  # Allow Dashboard to create &apos;kubernetes-dashboard-settings&apos; config map.</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;configmaps&quot;]</span><br><span class=\"line\">  verbs: [&quot;create&quot;]</span><br><span class=\"line\">  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;secrets&quot;]</span><br><span class=\"line\">  resourceNames: [&quot;kubernetes-dashboard-key-holder&quot;, &quot;kubernetes-dashboard-certs&quot;]</span><br><span class=\"line\">  verbs: [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;]</span><br><span class=\"line\">  # Allow Dashboard to get and update &apos;kubernetes-dashboard-settings&apos; config map.</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;configmaps&quot;]</span><br><span class=\"line\">  resourceNames: [&quot;kubernetes-dashboard-settings&quot;]</span><br><span class=\"line\">  verbs: [&quot;get&quot;, &quot;update&quot;]</span><br><span class=\"line\">  # Allow Dashboard to get metrics from heapster.</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;services&quot;]</span><br><span class=\"line\">  resourceNames: [&quot;heapster&quot;]</span><br><span class=\"line\">  verbs: [&quot;proxy&quot;]</span><br><span class=\"line\">- apiGroups: [&quot;&quot;]</span><br><span class=\"line\">  resources: [&quot;services/proxy&quot;]</span><br><span class=\"line\">  resourceNames: [&quot;heapster&quot;, &quot;http:heapster:&quot;, &quot;https:heapster:&quot;]</span><br><span class=\"line\">  verbs: [&quot;get&quot;]</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class=\"line\">kind: RoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kubernetes-dashboard-minimal</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: Role</span><br><span class=\"line\">  name: kubernetes-dashboard-minimal</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: kubernetes-dashboard</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"># ------------------- Dashboard Deployment ------------------- #</span><br><span class=\"line\"></span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">apiVersion: apps/v1beta2</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\">  name: kubernetes-dashboard</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  revisionHistoryLimit: 10</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      k8s-app: kubernetes-dashboard</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        k8s-app: kubernetes-dashboard</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: kubernetes-dashboard</span><br><span class=\"line\">        image: reg.qiniu.com/k8s/kubernetes-dashboard-amd64:v1.8.3</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">        - containerPort: 8443</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        args:</span><br><span class=\"line\">          - --auto-generate-certificates</span><br><span class=\"line\">          # Uncomment the following line to manually specify Kubernetes API server Host</span><br><span class=\"line\">          # If not specified, Dashboard will attempt to auto discover the API server and connect</span><br><span class=\"line\">          # to it. Uncomment only if the default does not work.</span><br><span class=\"line\">          # - --apiserver-host=http://my-address:port</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - name: kubernetes-dashboard-certs</span><br><span class=\"line\">          mountPath: /certs</span><br><span class=\"line\">          # Create on-disk volume to store exec logs</span><br><span class=\"line\">        - mountPath: /tmp</span><br><span class=\"line\">          name: tmp-volume</span><br><span class=\"line\">        livenessProbe:</span><br><span class=\"line\">          httpGet:</span><br><span class=\"line\">            scheme: HTTPS</span><br><span class=\"line\">            path: /</span><br><span class=\"line\">            port: 8443</span><br><span class=\"line\">          initialDelaySeconds: 30</span><br><span class=\"line\">          timeoutSeconds: 30</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">      - name: kubernetes-dashboard-certs</span><br><span class=\"line\">        secret:</span><br><span class=\"line\">          secretName: kubernetes-dashboard-certs</span><br><span class=\"line\">      - name: tmp-volume</span><br><span class=\"line\">        emptyDir: &#123;&#125;</span><br><span class=\"line\">      serviceAccountName: kubernetes-dashboard</span><br><span class=\"line\">      # Comment the following tolerations if Dashboard must not be deployed on master</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">      - key: node-role.kubernetes.io/master</span><br><span class=\"line\">        effect: NoSchedule</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\"># ------------------- Dashboard Service ------------------- #</span><br><span class=\"line\"></span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\">  name: kubernetes-dashboard</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">    - port: 443</span><br><span class=\"line\">      targetPort: 8443</span><br><span class=\"line\">      #以https://ip:nodeport访问dashboard</span><br><span class=\"line\">      nodePort: 32500</span><br><span class=\"line\">  type: NodePort</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\"></span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>部署dashboard<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f dashboard.yaml</span><br></pre></td></tr></table></figure></p>\n<p>部署dashboard后，为了安全默认是不能匿名访问dashboard的，所以要创建一个cluster-admin角色的用户，使用token登录dashboard</p>\n<p>admin-user.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; admin-user.yaml</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: admin-user</span><br><span class=\"line\">  annotations:</span><br><span class=\"line\">    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    k8s-app: kubernetes-dashboard</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: cluster-admin</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: admin-user</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: admin-user</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class=\"line\">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class=\"line\"></span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>创建admin-user用户<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f admin-user.yaml</span><br></pre></td></tr></table></figure></p>\n<p>查看admin-user用户token， 使用token去登录dashboard<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &apos;&#123;print $1&#125;&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">Name:         admin-user-token-nfsg4</span><br><span class=\"line\">Namespace:    kube-system</span><br><span class=\"line\">Labels:       &lt;none&gt;</span><br><span class=\"line\">Annotations:  kubernetes.io/service-account.name=admin-user</span><br><span class=\"line\">              kubernetes.io/service-account.uid=6a91db25-704e-11e8-8055-080027c6c772</span><br><span class=\"line\"></span><br><span class=\"line\">Type:  kubernetes.io/service-account-token</span><br><span class=\"line\"></span><br><span class=\"line\">Data</span><br><span class=\"line\">====</span><br><span class=\"line\">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLW5mc2c0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2YTkxZGIyNS03MDRlLTExZTgtODA1NS0wODAwMjdjNmM3NzIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.Ue49ptrBZItvG7f8gavEQSoxtSgbz-bh3jZU9RbjLJh2nkmueTVh5WViauUd5TxsnmefB3DDLgTp_CfwWrV0Uyf7JT6GEDMvjDVhsyHa2qyjLwRfvf1vmnx2P7975yRQniq_a3fgOhdNn0zCL2Er9bDUNabCcR0ubNW6I63kp3-UYGIj9OfAkfqNCTFtjZgvcGvnvkhj2pm_peTJ6H3qmxhcb9WM90Nh77p5qdI8gjk2EdKPAhsOmOxBWSSnHqHPr0gVStsewQEHo0CQOr4MxE3NIg_gwjTbn0KD9vvJeECPAX_zsq7ejD0POoWvSXz8khwvv8yLgJQA-ZkUXJKk7w</span><br><span class=\"line\">ca.crt:     1025 bytes</span><br><span class=\"line\">namespace:  11 bytes</span><br></pre></td></tr></table></figure></p>\n<p>访问<a href=\"https://ip:32500即可打开dashboard页面\" target=\"_blank\" rel=\"noopener\">https://ip:32500即可打开dashboard页面</a></p>\n<h2 id=\"部署heapster、grafana、influxdb\"><a href=\"#部署heapster、grafana、influxdb\" class=\"headerlink\" title=\"部署heapster、grafana、influxdb\"></a>部署heapster、grafana、influxdb</h2><h3 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h3><p>heapster.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; heapster.yaml</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: heapster</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: system:heapster</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">- kind: ServiceAccount</span><br><span class=\"line\">  name: heapster</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">---  </span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: heapster</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  </span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: heapster</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        task: monitoring</span><br><span class=\"line\">        k8s-app: heapster</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      serviceAccountName: heapster</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: heapster</span><br><span class=\"line\">        image: mirrorgooglecontainers/heapster-amd64:v1.5.3</span><br><span class=\"line\">        imagePullPolicy: IfNotPresent</span><br><span class=\"line\">        #volumeMounts: </span><br><span class=\"line\">        #- mountPath: /root/.kube</span><br><span class=\"line\">        #  name: config</span><br><span class=\"line\">        command:</span><br><span class=\"line\">        - /heapster</span><br><span class=\"line\">        - --source=kubernetes:https://kubernetes.default</span><br><span class=\"line\">        - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    task: monitoring</span><br><span class=\"line\">    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)</span><br><span class=\"line\">    # If you are NOT using this as an addon, you should comment out this line.</span><br><span class=\"line\">    kubernetes.io/cluster-service: &apos;true&apos;</span><br><span class=\"line\">    kubernetes.io/name: Heapster</span><br><span class=\"line\">  name: heapster</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">  - port: 80</span><br><span class=\"line\">    targetPort: 8082</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: heapster</span><br><span class=\"line\">  </span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>grafana.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; grafana.yaml</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: monitoring-grafana</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        task: monitoring</span><br><span class=\"line\">        k8s-app: grafana</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: grafana</span><br><span class=\"line\">        image: mirrorgooglecontainers/heapster-grafana-amd64:v4.4.3</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">        - containerPort: 3000</span><br><span class=\"line\">          protocol: TCP</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - mountPath: /etc/ssl/certs</span><br><span class=\"line\">          name: ca-certificates</span><br><span class=\"line\">          readOnly: true</span><br><span class=\"line\">        - mountPath: /var</span><br><span class=\"line\">          name: grafana-storage</span><br><span class=\"line\">        env:</span><br><span class=\"line\">        - name: INFLUXDB_HOST</span><br><span class=\"line\">          value: monitoring-influxdb</span><br><span class=\"line\">        - name: GF_SERVER_HTTP_PORT</span><br><span class=\"line\">          value: &quot;3000&quot;</span><br><span class=\"line\">          # The following env variables are required to make Grafana accessible via</span><br><span class=\"line\">          # the kubernetes api-server proxy. On production clusters, we recommend</span><br><span class=\"line\">          # removing these env variables, setup auth for grafana, and expose the grafana</span><br><span class=\"line\">          # service using a LoadBalancer or a public IP.</span><br><span class=\"line\">        - name: GF_AUTH_BASIC_ENABLED</span><br><span class=\"line\">          value: &quot;false&quot;</span><br><span class=\"line\">        - name: GF_AUTH_ANONYMOUS_ENABLED</span><br><span class=\"line\">          value: &quot;true&quot;</span><br><span class=\"line\">        - name: GF_AUTH_ANONYMOUS_ORG_ROLE</span><br><span class=\"line\">          value: Admin</span><br><span class=\"line\">        - name: GF_SERVER_ROOT_URL</span><br><span class=\"line\">          # If you&apos;re only using the API Server proxy, set this value instead:</span><br><span class=\"line\">          # value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</span><br><span class=\"line\">          value: /</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">      - name: ca-certificates</span><br><span class=\"line\">        hostPath:</span><br><span class=\"line\">          path: /etc/ssl/certs</span><br><span class=\"line\">      - name: grafana-storage</span><br><span class=\"line\">        emptyDir: &#123;&#125;</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)</span><br><span class=\"line\">    # If you are NOT using this as an addon, you should comment out this line.</span><br><span class=\"line\">    kubernetes.io/cluster-service: &apos;true&apos;</span><br><span class=\"line\">    kubernetes.io/name: monitoring-grafana</span><br><span class=\"line\">  name: monitoring-grafana</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  # In a production setup, we recommend accessing Grafana through an external Loadbalancer</span><br><span class=\"line\">  # or through a public IP.</span><br><span class=\"line\">  # type: LoadBalancer</span><br><span class=\"line\">  # You could also use NodePort to expose the service at a randomly-generated port</span><br><span class=\"line\">  # type: NodePort</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">  - port: 80</span><br><span class=\"line\">    targetPort: 3000</span><br><span class=\"line\">    nodePort: 32600</span><br><span class=\"line\">  type: NodePort</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: grafana</span><br><span class=\"line\"></span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>influxdb.yaml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; influxdb.yaml</span><br><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: monitoring-influxdb</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 1</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        task: monitoring</span><br><span class=\"line\">        k8s-app: influxdb</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: influxdb</span><br><span class=\"line\">        image: mirrorgooglecontainers/heapster-influxdb-amd64:v1.3.3</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - mountPath: /data</span><br><span class=\"line\">          name: influxdb-storage</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">      - name: influxdb-storage</span><br><span class=\"line\">        emptyDir: &#123;&#125;</span><br><span class=\"line\">---</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Service</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    task: monitoring</span><br><span class=\"line\">    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)</span><br><span class=\"line\">    # If you are NOT using this as an addon, you should comment out this line.</span><br><span class=\"line\">    kubernetes.io/cluster-service: &apos;true&apos;</span><br><span class=\"line\">    kubernetes.io/name: monitoring-influxdb</span><br><span class=\"line\">  name: monitoring-influxdb</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  ports:</span><br><span class=\"line\">  - port: 8086</span><br><span class=\"line\">    targetPort: 8086</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    k8s-app: influxdb</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl create -f grafana.yaml</span><br><span class=\"line\">$ kubectl create -f heapster.yaml</span><br><span class=\"line\">$ kubectl create -f influxdb.yaml</span><br></pre></td></tr></table></figure>\n<p>访问<a href=\"http://ip:32600即可访问到grafana\" target=\"_blank\" rel=\"noopener\">http://ip:32600即可访问到grafana</a> ui了</p>\n<p>参考资料:<br><a href=\"https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/calico\" target=\"_blank\" rel=\"noopener\">https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/calico</a><br><a href=\"https://jimmysong.io/kubernetes-handbook/practice/coredns.html\" target=\"_blank\" rel=\"noopener\">https://jimmysong.io/kubernetes-handbook/practice/coredns.html</a></p>\n"},{"title":"下载文件，header中Content-Disposition中文文件名乱码","date":"2018-10-15T16:00:00.000Z","_content":"## 前言\n在用Java下在文件时，需要取到下载文件的名称，head中取Content-disposition，显示为乱码，在此记录下这个问题。\n\n\n## 代码\n```\n\tMap<String, List<String>> heads = connection.getHeaderFields();\n\tString description = heads.get(\"Content-disposition\").get(0);\n\t//取到文件名\n\tString fileName = StringUtil.match(description, Pattern.compile(\"(?!\\\")\\\\S+(?<!\\\")\"));\n\t//需要把iso8859-1转换成gbk。问题解决\n\t//转成gbk是因为在Windows环境下。\n\tfileName = new String(fileName.getBytes(\"iso8859-1\"), \"gbk\");\n\tFileOutputStream fileOutputStream = new FileOutputStream(\"C:\\\\down\\\\\" + fileName);\n    FileCopyUtils.copy(connection.getInputStream(), fileOutputStream);\n```\n思路:\n[这篇文章里面写了](https://blog.csdn.net/liuyaqi1993/article/details/78275396)，RFC 822（ Standard for ARPA Internet Text Messages）规定了文本消息只能为ASCII，所以在服务器下载文件时设置head，要把文件名转换成iso8859-1，故需要把head的文件名先转成iso8859-1，再转成目的需要的编码，即可解决乱码问题。","source":"_posts/开发日志/Content-disposition.md","raw":"---\ntitle: 下载文件，header中Content-Disposition中文文件名乱码\ntags: [开发日志]\ndate: 2018-10-16\n---\n## 前言\n在用Java下在文件时，需要取到下载文件的名称，head中取Content-disposition，显示为乱码，在此记录下这个问题。\n\n\n## 代码\n```\n\tMap<String, List<String>> heads = connection.getHeaderFields();\n\tString description = heads.get(\"Content-disposition\").get(0);\n\t//取到文件名\n\tString fileName = StringUtil.match(description, Pattern.compile(\"(?!\\\")\\\\S+(?<!\\\")\"));\n\t//需要把iso8859-1转换成gbk。问题解决\n\t//转成gbk是因为在Windows环境下。\n\tfileName = new String(fileName.getBytes(\"iso8859-1\"), \"gbk\");\n\tFileOutputStream fileOutputStream = new FileOutputStream(\"C:\\\\down\\\\\" + fileName);\n    FileCopyUtils.copy(connection.getInputStream(), fileOutputStream);\n```\n思路:\n[这篇文章里面写了](https://blog.csdn.net/liuyaqi1993/article/details/78275396)，RFC 822（ Standard for ARPA Internet Text Messages）规定了文本消息只能为ASCII，所以在服务器下载文件时设置head，要把文件名转换成iso8859-1，故需要把head的文件名先转成iso8859-1，再转成目的需要的编码，即可解决乱码问题。","slug":"开发日志/Content-disposition","published":1,"updated":"2018-10-16T03:32:23.223Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjncsum5e000y7gvrgn73pgnk","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在用Java下在文件时，需要取到下载文件的名称，head中取Content-disposition，显示为乱码，在此记录下这个问题。</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map&lt;String, List&lt;String&gt;&gt; heads = connection.getHeaderFields();</span><br><span class=\"line\">String description = heads.get(&quot;Content-disposition&quot;).get(0);</span><br><span class=\"line\">//取到文件名</span><br><span class=\"line\">String fileName = StringUtil.match(description, Pattern.compile(&quot;(?!\\&quot;)\\\\S+(?&lt;!\\&quot;)&quot;));</span><br><span class=\"line\">//需要把iso8859-1转换成gbk。问题解决</span><br><span class=\"line\">//转成gbk是因为在Windows环境下。</span><br><span class=\"line\">fileName = new String(fileName.getBytes(&quot;iso8859-1&quot;), &quot;gbk&quot;);</span><br><span class=\"line\">FileOutputStream fileOutputStream = new FileOutputStream(&quot;C:\\\\down\\\\&quot; + fileName);</span><br><span class=\"line\">   FileCopyUtils.copy(connection.getInputStream(), fileOutputStream);</span><br></pre></td></tr></table></figure>\n<p>思路:<br><a href=\"https://blog.csdn.net/liuyaqi1993/article/details/78275396\" target=\"_blank\" rel=\"noopener\">这篇文章里面写了</a>，RFC 822（ Standard for ARPA Internet Text Messages）规定了文本消息只能为ASCII，所以在服务器下载文件时设置head，要把文件名转换成iso8859-1，故需要把head的文件名先转成iso8859-1，再转成目的需要的编码，即可解决乱码问题。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在用Java下在文件时，需要取到下载文件的名称，head中取Content-disposition，显示为乱码，在此记录下这个问题。</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Map&lt;String, List&lt;String&gt;&gt; heads = connection.getHeaderFields();</span><br><span class=\"line\">String description = heads.get(&quot;Content-disposition&quot;).get(0);</span><br><span class=\"line\">//取到文件名</span><br><span class=\"line\">String fileName = StringUtil.match(description, Pattern.compile(&quot;(?!\\&quot;)\\\\S+(?&lt;!\\&quot;)&quot;));</span><br><span class=\"line\">//需要把iso8859-1转换成gbk。问题解决</span><br><span class=\"line\">//转成gbk是因为在Windows环境下。</span><br><span class=\"line\">fileName = new String(fileName.getBytes(&quot;iso8859-1&quot;), &quot;gbk&quot;);</span><br><span class=\"line\">FileOutputStream fileOutputStream = new FileOutputStream(&quot;C:\\\\down\\\\&quot; + fileName);</span><br><span class=\"line\">   FileCopyUtils.copy(connection.getInputStream(), fileOutputStream);</span><br></pre></td></tr></table></figure>\n<p>思路:<br><a href=\"https://blog.csdn.net/liuyaqi1993/article/details/78275396\" target=\"_blank\" rel=\"noopener\">这篇文章里面写了</a>，RFC 822（ Standard for ARPA Internet Text Messages）规定了文本消息只能为ASCII，所以在服务器下载文件时设置head，要把文件名转换成iso8859-1，故需要把head的文件名先转成iso8859-1，再转成目的需要的编码，即可解决乱码问题。</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjncsum4r00047gvrn7om26uv","tag_id":"cjncsum4o00027gvrngmouzl1","_id":"cjncsum4t00077gvr8mla49yh"},{"post_id":"cjncsum4g00007gvr0qxx7m8f","tag_id":"cjncsum4o00027gvrngmouzl1","_id":"cjncsum4u00097gvrpwdstpwl"},{"post_id":"cjncsum4w000c7gvrcsfma5lr","tag_id":"cjncsum4s00067gvr0daa416i","_id":"cjncsum4z000f7gvrjvwl1fcd"},{"post_id":"cjncsum4l00017gvrnhgjrbql","tag_id":"cjncsum4s00067gvr0daa416i","_id":"cjncsum50000g7gvr5kmun0my"},{"post_id":"cjncsum4l00017gvrnhgjrbql","tag_id":"cjncsum4v000b7gvr7br6njf0","_id":"cjncsum51000i7gvr6t3hkjsh"},{"post_id":"cjncsum4x000d7gvriie93tcn","tag_id":"cjncsum4s00067gvr0daa416i","_id":"cjncsum51000j7gvr5a24cxde"},{"post_id":"cjncsum4p00037gvrhovnm5c1","tag_id":"cjncsum4o00027gvrngmouzl1","_id":"cjncsum51000k7gvr6tru6gtt"},{"post_id":"cjncsum4s00057gvrktbm8l0c","tag_id":"cjncsum50000h7gvr4deskb7g","_id":"cjncsum53000p7gvrysbe3xtq"},{"post_id":"cjncsum4s00057gvrktbm8l0c","tag_id":"cjncsum51000l7gvr9gyn266f","_id":"cjncsum53000q7gvr7k1fz3bo"},{"post_id":"cjncsum4s00057gvrktbm8l0c","tag_id":"cjncsum52000m7gvrtvvkparq","_id":"cjncsum54000s7gvrl1o82zec"},{"post_id":"cjncsum4s00057gvrktbm8l0c","tag_id":"cjncsum52000n7gvr0pcgk5wq","_id":"cjncsum54000t7gvrh7k96mvm"},{"post_id":"cjncsum4t00087gvr6pgk9aux","tag_id":"cjncsum4s00067gvr0daa416i","_id":"cjncsum54000u7gvr3rxy6tru"},{"post_id":"cjncsum4t00087gvr6pgk9aux","tag_id":"cjncsum53000o7gvri2v5uqg3","_id":"cjncsum54000v7gvrv0xarqsv"},{"post_id":"cjncsum4v000a7gvrnddhgays","tag_id":"cjncsum4o00027gvrngmouzl1","_id":"cjncsum54000w7gvrfiuhoiz8"},{"post_id":"cjncsum4v000a7gvrnddhgays","tag_id":"cjncsum53000r7gvri1lq5k9o","_id":"cjncsum54000x7gvr79opqx00"},{"post_id":"cjncsum5e000y7gvrgn73pgnk","tag_id":"cjncsum5f000z7gvrg58k72qm","_id":"cjncsum5f00107gvru7vo7br1"}],"Tag":[{"name":"docker","_id":"cjncsum4o00027gvrngmouzl1"},{"name":"kubernetes","_id":"cjncsum4s00067gvr0daa416i"},{"name":"jenkins","_id":"cjncsum4v000b7gvr7br6njf0"},{"name":"Elasticsearch","_id":"cjncsum50000h7gvr4deskb7g"},{"name":"Docker","_id":"cjncsum51000l7gvr9gyn266f"},{"name":"Fluentd","_id":"cjncsum52000m7gvrtvvkparq"},{"name":"Kibana","_id":"cjncsum52000n7gvr0pcgk5wq"},{"name":"configmap","_id":"cjncsum53000o7gvri2v5uqg3"},{"name":"kafka","_id":"cjncsum53000r7gvri1lq5k9o"},{"name":"开发日志","_id":"cjncsum5f000z7gvrg58k72qm"}]}}